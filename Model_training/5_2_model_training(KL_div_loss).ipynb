{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_2_model_training(KL_div_loss).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qAEbLZ7vsTVo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU 설정"
      ],
      "metadata": {
        "id": "X0t7yrUOg6lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NY4iGyKcgjif",
        "outputId": "62949848-5992-40d9-d79a-5baf4b93590b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 가져오기"
      ],
      "metadata": {
        "id": "8CGmvK_xg3uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/drive/MyDrive/고모부_머신러닝/dogncat'\n",
        "path = []\n",
        "dataset_type = []\n",
        "label = []\n",
        "for dir_name, _, file_names in os.walk(dir_path):\n",
        "    for file_name in file_names:\n",
        "        file_path = dir_name + '/' + file_name\n",
        "        path.append(file_path)\n",
        "    \n",
        "        if '/training_set' in file_path:\n",
        "            dataset_type.append('train')\n",
        "        elif '/test_set' in file_path:\n",
        "            dataset_type.append('test')\n",
        "        else:\n",
        "            dataset_type.append('N/A')\n",
        "\n",
        "        if '/cats' in file_path:\n",
        "            label.append('CAT')\n",
        "        elif '/dogs' in file_path:\n",
        "            label.append('DOG')\n",
        "        else:\n",
        "            label.append('N/A')\n",
        "\n",
        "cnd_df = pd.DataFrame({'path' : path, 'type' : dataset_type, 'label' : label})\n",
        "            "
      ],
      "metadata": {
        "id": "LkxUK6rEg-Xq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XozN7BI4g-ui",
        "outputId": "2850a7a4-0a66-4e47-c8f0-271b5590daaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10032 entries, 0 to 10031\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   path    10032 non-null  object\n",
            " 1   type    10032 non-null  object\n",
            " 2   label   10032 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 235.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Wryqn3cYrTyG",
        "outputId": "78d649a9-96af-4f2c-c254-4b0d8e500cfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path  type label\n",
              "0  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "1  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "2  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "3  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "4  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "5  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "6  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "7  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "8  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "9  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16ab4fd3-8825-45b4-becb-9d19f0f3e462\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16ab4fd3-8825-45b4-becb-9d19f0f3e462')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16ab4fd3-8825-45b4-becb-9d19f0f3e462 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16ab4fd3-8825-45b4-becb-9d19f0f3e462');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df = cnd_df[cnd_df['path'].str.contains('.jpg')]"
      ],
      "metadata": {
        "id": "HnbcGm6KlfCy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXSzkKp6lsN2",
        "outputId": "f6ec5f81-3140-4ed3-db7e-e46fa95514b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10028 entries, 0 to 10031\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   path    10028 non-null  object\n",
            " 1   type    10028 non-null  object\n",
            " 2   label   10028 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 313.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2Zh0hAbQrEu3",
        "outputId": "e188dd0c-1b51-4ea5-c6b2-ea2e121744c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path  type label\n",
              "0  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "1  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "2  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "3  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "4  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56b46bcc-c4ab-4278-9f4d-0e21c4ba4642\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56b46bcc-c4ab-4278-9f4d-0e21c4ba4642')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-56b46bcc-c4ab-4278-9f4d-0e21c4ba4642 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-56b46bcc-c4ab-4278-9f4d-0e21c4ba4642');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, validation 구분\n",
        "train_df, valid_df = train_test_split(cnd_df[cnd_df['type']=='train'], test_size = 0.25)\n",
        "\n",
        "# onehotencoding\n",
        "train_onehot = pd.get_dummies(train_df['label'])\n",
        "valid_onehot = pd.get_dummies(valid_df['label'])\n",
        "\n",
        "train_df = pd.concat([train_df, train_onehot], axis=1)\n",
        "valid_df = pd.concat([valid_df, valid_onehot], axis=1)\n",
        "\n",
        "train_df['label_idx'] = train_df.iloc[:, 3:].values.argmax(axis=1)\n",
        "valid_df['label_idx'] = valid_df.iloc[:, 3:].values.argmax(axis=1)"
      ],
      "metadata": {
        "id": "NFAW5uESmr1v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[:, 3:].values.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pikphOpBXJFB",
        "outputId": "d547b1a7-02cd-4ed1-99d2-d6eddf375646"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0sroPlZSsXG0",
        "outputId": "98d5ebc7-c3dc-41ec-8dcd-b2b83ebcd04d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    path   type label  CAT  \\\n",
              "2749   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "6180   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8738   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7957   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "3718   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "...                                                  ...    ...   ...  ...   \n",
              "8248   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "3234   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5076   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "10010  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "5763   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "\n",
              "       DOG  label_idx  \n",
              "2749     0          0  \n",
              "6180     1          1  \n",
              "8738     1          1  \n",
              "7957     1          1  \n",
              "3718     0          0  \n",
              "...    ...        ...  \n",
              "8248     1          1  \n",
              "3234     0          0  \n",
              "5076     0          0  \n",
              "10010    1          1  \n",
              "5763     0          0  \n",
              "\n",
              "[6003 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebd0933b-ca18-4ef0-8cea-b8506374a9fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "      <th>CAT</th>\n",
              "      <th>DOG</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2749</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6180</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8738</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7957</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3718</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8248</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3234</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5076</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10010</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5763</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6003 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebd0933b-ca18-4ef0-8cea-b8506374a9fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebd0933b-ca18-4ef0-8cea-b8506374a9fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebd0933b-ca18-4ef0-8cea-b8506374a9fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(len(valid_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDTPLZhCrxvL",
        "outputId": "4e83b14d-ea33-40fe-e797-56c16409c82a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6003\n",
            "2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample용 데이터\n",
        "\n",
        "sample_dog = train_df[train_df['label'] == 'DOG'].sample(30)\n",
        "sample_cat = train_df[train_df['label'] == 'CAT'].sample(30)\n",
        "\n",
        "sample_data = pd.concat([sample_dog, sample_cat])"
      ],
      "metadata": {
        "id": "GbzO3016sNUc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DJKhhaPZZ2Ss",
        "outputId": "1f45b46e-d4b6-4988-e3be-cc73777e3308"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   path   type label  CAT  \\\n",
              "9912  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7760  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7264  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8552  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9322  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9167  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6215  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6752  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8207  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8855  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9836  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9445  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6143  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9006  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9518  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9444  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8063  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8484  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7923  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8165  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7022  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7208  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9076  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6270  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9543  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8763  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8311  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6345  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7908  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6326  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "3516  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5263  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5184  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3489  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2067  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3617  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4490  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5533  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2953  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4828  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5928  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2421  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2736  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5317  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4776  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3517  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2446  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3284  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2178  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2029  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3065  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3619  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5393  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3486  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5363  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4220  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4582  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3997  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4679  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3779  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "\n",
              "      DOG  label_idx  \n",
              "9912    1          1  \n",
              "7760    1          1  \n",
              "7264    1          1  \n",
              "8552    1          1  \n",
              "9322    1          1  \n",
              "9167    1          1  \n",
              "6215    1          1  \n",
              "6752    1          1  \n",
              "8207    1          1  \n",
              "8855    1          1  \n",
              "9836    1          1  \n",
              "9445    1          1  \n",
              "6143    1          1  \n",
              "9006    1          1  \n",
              "9518    1          1  \n",
              "9444    1          1  \n",
              "8063    1          1  \n",
              "8484    1          1  \n",
              "7923    1          1  \n",
              "8165    1          1  \n",
              "7022    1          1  \n",
              "7208    1          1  \n",
              "9076    1          1  \n",
              "6270    1          1  \n",
              "9543    1          1  \n",
              "8763    1          1  \n",
              "8311    1          1  \n",
              "6345    1          1  \n",
              "7908    1          1  \n",
              "6326    1          1  \n",
              "3516    0          0  \n",
              "5263    0          0  \n",
              "5184    0          0  \n",
              "3489    0          0  \n",
              "2067    0          0  \n",
              "3617    0          0  \n",
              "4490    0          0  \n",
              "5533    0          0  \n",
              "2953    0          0  \n",
              "4828    0          0  \n",
              "5928    0          0  \n",
              "2421    0          0  \n",
              "2736    0          0  \n",
              "5317    0          0  \n",
              "4776    0          0  \n",
              "3517    0          0  \n",
              "2446    0          0  \n",
              "3284    0          0  \n",
              "2178    0          0  \n",
              "2029    0          0  \n",
              "3065    0          0  \n",
              "3619    0          0  \n",
              "5393    0          0  \n",
              "3486    0          0  \n",
              "5363    0          0  \n",
              "4220    0          0  \n",
              "4582    0          0  \n",
              "3997    0          0  \n",
              "4679    0          0  \n",
              "3779    0          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-156f6351-f5d1-4803-8464-f87d76e16c59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "      <th>CAT</th>\n",
              "      <th>DOG</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9912</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7760</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7264</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8552</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9322</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9167</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6215</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6752</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8207</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8855</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9836</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9445</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6143</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9006</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9518</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9444</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8063</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8484</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7923</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8165</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7022</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7208</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9076</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6270</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9543</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8763</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8311</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7908</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6326</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3516</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5263</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5184</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3489</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2067</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3617</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4490</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5533</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2953</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4828</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5928</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2736</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5317</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4776</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3517</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2446</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3284</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2178</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2029</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3065</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3619</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5393</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3486</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5363</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4220</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4582</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4679</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3779</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-156f6351-f5d1-4803-8464-f87d76e16c59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-156f6351-f5d1-4803-8464-f87d76e16c59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-156f6351-f5d1-4803-8464-f87d76e16c59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSs3Dx85PeWp",
        "outputId": "8f66b7b7-a6b6-405c-e867-6766fcbe6683"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 60 entries, 9912 to 3779\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   path       60 non-null     object\n",
            " 1   type       60 non-null     object\n",
            " 2   label      60 non-null     object\n",
            " 3   CAT        60 non-null     uint8 \n",
            " 4   DOG        60 non-null     uint8 \n",
            " 5   label_idx  60 non-null     int64 \n",
            "dtypes: int64(1), object(3), uint8(2)\n",
            "memory usage: 2.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 커스텀 데이터 셋 만들기\n",
        "- KL_div_loss를 사용하려면 onehot label과 인덱스 라벨 둘 다 필요하다.\n",
        "- onehot label은 loss 계산용\n",
        "- index label은 accuracy, recall, precision 계산용"
      ],
      "metadata": {
        "id": "ChOVST4KuQrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset2(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(MyDataset2, self).__init__()\n",
        "        self.path = df['path'].values\n",
        "        self.label = df['label_idx'].values # 클래스 idx가 들어있는 레이블 데이터. Accuracy, recall, precision 계산에 사용.\n",
        "        self.label_onehot = df.iloc[:, 3:5].values # 클래스 idx를 onehot encoding한 레이블 데이터. kl_div loss 계산에 사용.\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.cvtColor(cv2.imread(self.path[idx]), cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (244,244))\n",
        "        image = np.asarray(image, dtype=np.float32).transpose(2,0,1)\n",
        "        norm_image = (image - np.amin(image)) / (np.amax(image) - np.amin(image))\n",
        "\n",
        "        label = np.asarray(self.label[idx], dtype=np.float32)\n",
        "        label_onehot = np.asarray(self.label_onehot[idx], dtype=np.float32)\n",
        "\n",
        "        return norm_image, label, label_onehot"
      ],
      "metadata": {
        "id": "YG8V2wCcxtuC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = MyDataset2(train_df)\n",
        "valid_dataset2 = MyDataset2(valid_df)\n",
        "sample_dataset2 = MyDataset2(sample_data)"
      ],
      "metadata": {
        "id": "wnSMVZpC0Qhz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터로더 만들기"
      ],
      "metadata": {
        "id": "liL-OIS_yrz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader2 = DataLoader(train_dataset2, batch_size=8, shuffle=True)\n",
        "valid_loader2 = DataLoader(valid_dataset2, batch_size=8, shuffle=False)\n",
        "sample_loader2 = DataLoader(sample_dataset2, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "l9GXUm2f0-qI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample_loader2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7FWf-SNGCiC",
        "outputId": "bbc2a44f-944c-435d-c7f8-cacdd57f5193"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 만들기"
      ],
      "metadata": {
        "id": "zR4SShLAzMzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 5, kernel_size=3, padding='same')\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(5*244*244, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('input size :', x.shape)\n",
        "        # print('input max :', torch.amax(x, dim=(1,2,3)))\n",
        "        # print('input min :', torch.amin(x, dim=(1,2,3)))\n",
        "        # print('input l2norm :', torch.linalg.vector_norm(x, dim=(1,2,3)))\n",
        "\n",
        "        conv = self.conv1(x)\n",
        "        # print('conv size :', conv.shape)\n",
        "        # print('conv max :', torch.amax(conv, dim=(1,2,3)))\n",
        "        # print('conv min :', torch.amin(conv, dim=(1,2,3)))\n",
        "        # print('conv l2norm :', torch.linalg.vector_norm(conv, dim=(1,2,3)))\n",
        "\n",
        "        conv_out = self.relu(conv)\n",
        "        # print('conv_out size :', conv_out.shape)\n",
        "        # print('conv_out max :', torch.amax(conv_out, dim=(1,2,3)))\n",
        "        # print('conv_out min :', torch.amin(conv_out, dim=(1,2,3)))\n",
        "        # print('conv_out l2norm :', torch.linalg.vector_norm(conv_out, dim=(1,2,3)))\n",
        "\n",
        "        fc_input = conv_out.view(conv_out.size(0), -1)\n",
        "        # print('fc_input size :', fc_input.shape)\n",
        "        # print('fc_input max :', torch.amax(fc_input, dim=(1)))\n",
        "        # print('fc_input min :', torch.amin(fc_input, dim=(1)))\n",
        "        # print('fc_input l2norm :', torch.linalg.vector_norm(fc_input, dim=(1)))\n",
        "\n",
        "        fc_logit = self.fc1(fc_input)\n",
        "        # print('fc_logit size :', fc_logit.shape)\n",
        "        # print('fc_logit max :', torch.amax(fc_logit, dim=(1)))\n",
        "        # print('fc_logit min :', torch.amin(fc_logit, dim=(1)))\n",
        "        # print('fc_logit l2norm :', torch.linalg.vector_norm(fc_logit, dim=(1)))\n",
        "        # print()\n",
        "\n",
        "        return fc_logit"
      ],
      "metadata": {
        "id": "VSXfN1KDzWZM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd2Swq0pBQe7",
        "outputId": "2cfa49ab-ebb1-48a7-bf82-8697aec9d88e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=297680, out_features=2, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss function"
      ],
      "metadata": {
        "id": "4pJtT1qOBTx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.KLDivLoss(reduction='batchmean').to(device)"
      ],
      "metadata": {
        "id": "ygVVDGjWBZG2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optimizer, lr_scheduler"
      ],
      "metadata": {
        "id": "LeTelJ6oBfTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(model.parameters(), lr=0.001)\n",
        "scheduler = LinearLR(optimizer)"
      ],
      "metadata": {
        "id": "RRnlRjBxBf79"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix, Accuracy, recall, precision"
      ],
      "metadata": {
        "id": "XteaDB4KB-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_conf_matrix(predicted:float, label:float, class_idx:float):\n",
        "    TP_num = 0\n",
        "    TN_num = 0\n",
        "    FP_num = 0\n",
        "    FN_num = 0\n",
        "    for i in range(len(predicted)):\n",
        "        if predicted[i] == class_idx and label[i] == class_idx:\n",
        "            TP_num += 1\n",
        "\n",
        "        elif predicted[i] == class_idx and label[i] != class_idx:\n",
        "            FP_num += 1\n",
        "            \n",
        "        elif predicted[i] != class_idx and label[i] == class_idx:\n",
        "            FN_num += 1\n",
        "\n",
        "        elif predicted[i] != class_idx and label[i] != class_idx:\n",
        "            TN_num += 1\n",
        "\n",
        "    return TP_num, TN_num, FP_num, FN_num\n",
        "\n",
        "def calculate_accuracy(conf_matrix_list):\n",
        "    try:\n",
        "        accuracy = (conf_matrix_list[0] + conf_matrix_list[1]) / (conf_matrix_list[0] + conf_matrix_list[1] + conf_matrix_list[2] + conf_matrix_list[3])\n",
        "    except ZeroDivisionError:\n",
        "        accuracy = 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_recall(conf_matrix_list):\n",
        "    try: \n",
        "        recall = conf_matrix_list[0] / (conf_matrix_list[0] + conf_matrix_list[3])\n",
        "    except ZeroDivisionError:\n",
        "        recall = 0\n",
        "    return recall\n",
        "\n",
        "def calculate_precision(conf_matrix_list):\n",
        "    try:\n",
        "        precision = conf_matrix_list[0] / (conf_matrix_list[0] + conf_matrix_list[2])\n",
        "    except ZeroDivisionError:\n",
        "        precision = 0\n",
        "    return precision\n",
        "\n",
        "def calculate_acc_rec_pre(conf_matrix_dict : dict):\n",
        "    for class_idx in conf_matrix_dict.keys():\n",
        "        accuracy = calculate_accuracy(conf_matrix_dict[class_idx])\n",
        "        recall = calculate_recall(conf_matrix_dict[class_idx])\n",
        "        precision = calculate_precision(conf_matrix_dict[class_idx])\n",
        "\n",
        "        print(f'class{class_idx} >>> accuracy : {accuracy}, recall : {recall}, precision : {precision}')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "mTSZU_8IDGRl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 30\n",
        "running_loss = 0.\n",
        "running_vloss = 0.\n",
        "class_num = 2\n",
        "accuracy = 0.\n",
        "recall = 0.\n",
        "precision = 0.\n",
        "\n",
        "for i in range(epoch):\n",
        "    print()\n",
        "    print(f'========= Epoch{i+1} =========')\n",
        "    print()\n",
        "    # train\n",
        "    model.train()\n",
        "    for batch_idx, data in enumerate(sample_loader2):\n",
        "        print(f'------ train batch{batch_idx + 1} ------')\n",
        "        image_data, label_idx, label_onehot = data\n",
        "        image_data = image_data.to(device)\n",
        "        label_onehot = label_onehot.to(device)\n",
        "        print('Label :', label_onehot)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tr_output= model(image_data)\n",
        "        print('predicted :', tr_output)\n",
        "\n",
        "        lsf = nn.LogSoftmax(dim=1)\n",
        "        tr_output_lsf = lsf(tr_output)\n",
        "        print('predicted(logsoftmax) :', tr_output_lsf)\n",
        "\n",
        "        loss = loss_fn(tr_output_lsf, label_onehot) \n",
        "        print(f'train batch{batch_idx+1} loss(kl_div) :', loss)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx + 1 == len(sample_loader2):\n",
        "            avg_loss = running_loss / len(sample_loader2)\n",
        "            print('Loss/train :', avg_loss)\n",
        "            running_loss = 0.\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # #evaluate\n",
        "    model.eval()\n",
        "    TP_TN_FP_FN = {}\n",
        "    for batch_idx, val_data in enumerate(sample_loader2): # training data로 우선 evaluation해보기\n",
        "        print(f'------ valid batch{batch_idx+1} ------')\n",
        "        v_image, v_label, v_label_onehot = val_data\n",
        "        v_image = v_image.to(device)\n",
        "        v_label_onehot = v_label_onehot.to(device)\n",
        "        val_output = model(v_image)\n",
        "        val_output_lsf = lsf(val_output)\n",
        "        val_output_idx = torch.argmax(val_output, dim=1)\n",
        "        print(f'predicted : {val_output_idx}, actual : {v_label}')\n",
        "\n",
        "        for i, class_idx in enumerate(range(class_num)):\n",
        "                TP, TN, FP, FN = calculate_conf_matrix(val_output_idx, v_label, class_idx) # val_output_idx, valid_label 모두 벡터형태로 입력해야 한다.\n",
        "                if batch_idx == 0:\n",
        "                    TP_TN_FP_FN[class_idx] = np.array([TP, TN, FP, FN])\n",
        "                else:\n",
        "                    TP_TN_FP_FN[class_idx] += np.array([TP, TN, FP, FN])\n",
        "\n",
        "        print(f'TP_TN_FP_FN : {TP_TN_FP_FN}')\n",
        "\n",
        "        vloss = loss_fn(val_output_lsf, v_label_onehot)\n",
        "        print(f'valid loss :', vloss)\n",
        "        running_vloss += vloss.item()\n",
        "        \n",
        "        if batch_idx + 1 == len(sample_loader2):\n",
        "            avg_vloss = running_vloss / len(sample_loader2)\n",
        "            calculate_acc_rec_pre(TP_TN_FP_FN)\n",
        "            print('Loss/valid :', avg_vloss)\n",
        "            # print('Accuracy :', accuracy_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            # print('Recall :', recall_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            # print('Precision :', precision_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            running_vloss = 0.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3W_OrbWDJCl",
        "outputId": "444831eb-ce42-4bb4-acd2-575a81d5c307"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= Epoch1 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.0624,  0.3499],\n",
            "        [-0.1404,  0.3477],\n",
            "        [-0.1525,  0.0782],\n",
            "        [-0.0243,  0.2058]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.9204, -0.5081],\n",
            "        [-0.9667, -0.4786],\n",
            "        [-0.8151, -0.5844],\n",
            "        [-0.8148, -0.5847]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.6610, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5007,  0.6802],\n",
            "        [-0.4147,  0.5378],\n",
            "        [-0.6206,  0.5645],\n",
            "        [-0.5111,  0.6157]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4487, -0.2677],\n",
            "        [-1.2787, -0.3263],\n",
            "        [-1.4518, -0.2668],\n",
            "        [-1.4075, -0.2807]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.8623, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8916, -0.6555],\n",
            "        [ 0.8289, -0.5904],\n",
            "        [ 0.7770, -0.6274],\n",
            "        [ 1.0012, -0.9952]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1930, -1.7401],\n",
            "        [-0.2166, -1.6359],\n",
            "        [-0.2195, -1.6240],\n",
            "        [-0.1274, -2.1238]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.5759, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.4887, -0.4789],\n",
            "        [ 0.3259, -0.3581],\n",
            "        [ 0.8051, -0.5080],\n",
            "        [ 0.2818, -0.3081]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3221, -1.2897],\n",
            "        [-0.4085, -1.0925],\n",
            "        [-0.2382, -1.5513],\n",
            "        [-0.4411, -1.0310]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.9992, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.9252,  3.1463],\n",
            "        [-1.3829,  1.4772],\n",
            "        [-2.7561,  2.9406],\n",
            "        [-2.6608,  2.8221]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-6.0738e+00, -2.3051e-03],\n",
            "        [-2.9158e+00, -5.5681e-02],\n",
            "        [-5.7001e+00, -3.3513e-03],\n",
            "        [-5.4871e+00, -4.1483e-03]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(3.6200, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.4528, -1.2678],\n",
            "        [ 1.1735, -1.0116],\n",
            "        [ 1.4158, -1.1709],\n",
            "        [ 1.6850, -1.5211]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0638, -2.7844],\n",
            "        [-0.1066, -2.2916],\n",
            "        [-0.0726, -2.6593],\n",
            "        [-0.0397, -3.2458]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.7508, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6243, -0.3711],\n",
            "        [ 0.2247, -0.0626],\n",
            "        [ 0.4031, -0.2541],\n",
            "        [ 0.4112, -0.3336]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3145, -1.3099],\n",
            "        [-0.5598, -0.8470],\n",
            "        [-0.4176, -1.0748],\n",
            "        [-0.3885, -1.1334]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.9051, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8323,  1.9861],\n",
            "        [-2.5008,  2.6021],\n",
            "        [-1.9337,  2.0533],\n",
            "        [-2.0403,  2.1499]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.8401, -0.0217],\n",
            "        [-5.1089, -0.0061],\n",
            "        [-4.0054, -0.0184],\n",
            "        [-4.2053, -0.0150]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(1.0629, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0704,  1.0995],\n",
            "        [-0.6268,  0.6092],\n",
            "        [-0.8567,  0.9974],\n",
            "        [-0.7550,  0.8541]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2780, -0.1081],\n",
            "        [-1.4911, -0.2551],\n",
            "        [-1.9996, -0.1455],\n",
            "        [-1.7914, -0.1824]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(1.4265, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.1978, -2.0809],\n",
            "        [ 2.3521, -2.2043],\n",
            "        [ 2.5165, -2.4650],\n",
            "        [ 1.9646, -1.8488]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0138, -4.2925],\n",
            "        [-0.0104, -4.5668],\n",
            "        [-0.0068, -4.9884],\n",
            "        [-0.0218, -3.8352]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(2.2119, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2738,  1.3514],\n",
            "        [-1.1203,  1.3446],\n",
            "        [-0.6391,  0.7900],\n",
            "        [-0.9158,  1.1579]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6951, -0.0699],\n",
            "        [-2.5465, -0.0816],\n",
            "        [-1.6438, -0.2147],\n",
            "        [-2.1922, -0.1184]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(1.2558, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7284, -1.6232],\n",
            "        [ 2.9074, -2.7048],\n",
            "        [ 1.8996, -1.8724],\n",
            "        [ 1.7057, -1.6673]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.4428e-02, -3.3861e+00],\n",
            "        [-3.6465e-03, -5.6158e+00],\n",
            "        [-2.2746e-02, -3.7947e+00],\n",
            "        [-3.3711e-02, -3.4067e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(1.8099, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.0851,  0.0937],\n",
            "        [ 0.0836,  0.1801],\n",
            "        [-0.2471,  0.5758],\n",
            "        [ 0.2634, -0.1611]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.6974, -0.6889],\n",
            "        [-0.7426, -0.6461],\n",
            "        [-1.1870, -0.3640],\n",
            "        [-0.5032, -0.9278]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.6830, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.1667,  0.3342],\n",
            "        [-0.2158,  0.3869],\n",
            "        [ 0.3254, -0.1073],\n",
            "        [-0.0111,  0.0586]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.9746, -0.4737],\n",
            "        [-1.0392, -0.4365],\n",
            "        [-0.5000, -0.9327],\n",
            "        [-0.7286, -0.6589]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.6429, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8023,  0.8626],\n",
            "        [-1.1772,  1.4141],\n",
            "        [-1.9377,  2.1097],\n",
            "        [-1.6202,  1.7610]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8382, -0.1733],\n",
            "        [-2.6635, -0.0723],\n",
            "        [-4.0647, -0.0173],\n",
            "        [-3.4146, -0.0334]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(2.5790, grad_fn=<DivBackward0>)\n",
            "Loss/train : 1.3364133715629578\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([1, 0, 3, 0]), 1: array([0, 1, 0, 3])}\n",
            "valid loss : tensor(3.3323, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 0, 4, 0]), 1: array([0, 4, 0, 4])}\n",
            "valid loss : tensor(1.2960, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 0, 6, 0]), 1: array([0, 6, 0, 6])}\n",
            "valid loss : tensor(2.8215, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 0, 8, 0]), 1: array([0, 8, 0, 8])}\n",
            "valid loss : tensor(2.2561, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9,  0, 11,  0]), 1: array([ 0,  9,  0, 11])}\n",
            "valid loss : tensor(3.7048, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  0, 13,  0]), 1: array([ 0, 11,  0, 13])}\n",
            "valid loss : tensor(2.4298, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([14,  0, 14,  0]), 1: array([ 0, 14,  0, 14])}\n",
            "valid loss : tensor(1.6261, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17,  0, 15,  0]), 1: array([ 0, 17,  0, 15])}\n",
            "valid loss : tensor(1.1136, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([18,  0, 18,  0]), 1: array([ 0, 18,  0, 18])}\n",
            "valid loss : tensor(3.7219, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20,  0, 20,  0]), 1: array([ 0, 20,  0, 20])}\n",
            "valid loss : tensor(2.4247, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22,  0, 22,  0]), 1: array([ 0, 22,  0, 22])}\n",
            "valid loss : tensor(1.8484, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24,  0, 24,  0]), 1: array([ 0, 24,  0, 24])}\n",
            "valid loss : tensor(2.2264, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26,  0, 26,  0]), 1: array([ 0, 26,  0, 26])}\n",
            "valid loss : tensor(2.6075, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([28,  0, 28,  0]), 1: array([ 0, 28,  0, 28])}\n",
            "valid loss : tensor(2.8460, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([30,  0, 30,  0]), 1: array([ 0, 30,  0, 30])}\n",
            "valid loss : tensor(2.0050, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.5, recall : 1.0, precision : 0.5\n",
            "class1 >>> accuracy : 0.5, recall : 0.0, precision : nan\n",
            "Loss/valid : 2.4173457860946654\n",
            "\n",
            "========= Epoch2 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.4142,  0.4097],\n",
            "        [ 1.6122, -1.5312],\n",
            "        [-0.4573,  0.5539],\n",
            "        [ 1.0529, -0.9675]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1876, -0.3638],\n",
            "        [-0.0422, -3.1857],\n",
            "        [-1.3215, -0.3103],\n",
            "        [-0.1245, -2.1448]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.2102, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0434,  1.0887],\n",
            "        [ 0.9320, -0.8150],\n",
            "        [ 0.2014, -0.1446],\n",
            "        [-0.5161,  0.5048]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2441, -0.1121],\n",
            "        [-0.1607, -1.9076],\n",
            "        [-0.5351, -0.8810],\n",
            "        [-1.3286, -0.3077]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2789, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9065,  0.8808],\n",
            "        [ 0.9789, -0.9146],\n",
            "        [ 1.2138, -1.1808],\n",
            "        [ 0.6443, -0.6345]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9421, -0.1548],\n",
            "        [-0.1402, -2.0338],\n",
            "        [-0.0873, -2.4819],\n",
            "        [-0.2456, -1.5244]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1570, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0857, -1.0611],\n",
            "        [-0.0628,  0.1817],\n",
            "        [ 1.2274, -1.2177],\n",
            "        [ 0.1436, -0.0716]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1105, -2.2573],\n",
            "        [-0.8229, -0.5783],\n",
            "        [-0.0832, -2.5282],\n",
            "        [-0.5913, -0.8065]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.3946, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.6407,  0.6270],\n",
            "        [ 0.1918, -0.1332],\n",
            "        [-0.1586,  0.1738],\n",
            "        [-0.7375,  0.6972]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5158, -0.2480],\n",
            "        [-0.5438, -0.8688],\n",
            "        [-0.8731, -0.5407],\n",
            "        [-1.6484, -0.2136]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.4696, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.3746,  0.3449],\n",
            "        [ 0.1438, -0.0734],\n",
            "        [-0.6508,  0.6851],\n",
            "        [ 1.0648, -1.0533]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1162, -0.3968],\n",
            "        [-0.5904, -0.8076],\n",
            "        [-1.5694, -0.2334],\n",
            "        [-0.1136, -2.2317]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.3335, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8153, -0.8350],\n",
            "        [-0.6663,  0.7082],\n",
            "        [ 1.7752, -1.7440],\n",
            "        [-0.7323,  0.8017]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1756, -1.8259],\n",
            "        [-1.6000, -0.2255],\n",
            "        [-0.0292, -3.5485],\n",
            "        [-1.7293, -0.1953]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1564, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6829, -0.6953],\n",
            "        [ 0.6778, -0.6551],\n",
            "        [ 1.0077, -0.8918],\n",
            "        [-0.7005,  0.7585]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2248, -1.6030],\n",
            "        [-0.2340, -1.5670],\n",
            "        [-0.1395, -2.0389],\n",
            "        [-1.6680, -0.2090]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2018, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.0549, -0.1044],\n",
            "        [ 1.4131, -1.3909],\n",
            "        [-1.0115,  1.0612],\n",
            "        [-0.5327,  0.6171]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.6167, -0.7760],\n",
            "        [-0.0588, -2.8628],\n",
            "        [-2.1912, -0.1185],\n",
            "        [-1.4249, -0.2751]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.3071, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2120,  1.3075],\n",
            "        [ 0.0999, -0.0814],\n",
            "        [ 0.6947, -0.7290],\n",
            "        [-0.9451,  0.9946]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5968, -0.0774],\n",
            "        [-0.6066, -0.7879],\n",
            "        [-0.2158, -1.6395],\n",
            "        [-2.0740, -0.1343]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.2585, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8539, -0.7954],\n",
            "        [-0.7999,  0.7530],\n",
            "        [ 0.7017, -0.7417],\n",
            "        [-0.8449,  0.7730]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1758, -1.8251],\n",
            "        [-1.7449, -0.1920],\n",
            "        [-0.2120, -1.6554],\n",
            "        [-1.7988, -0.1809]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1902, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0002,  1.0819],\n",
            "        [-0.8274,  0.8905],\n",
            "        [ 0.6821, -0.6676],\n",
            "        [ 0.8164, -0.7157]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1996, -0.1175],\n",
            "        [-1.8829, -0.1651],\n",
            "        [-0.2306, -1.5803],\n",
            "        [-0.1956, -1.7278]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1772, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2662, -1.1842],\n",
            "        [-0.2791,  0.3301],\n",
            "        [-0.4144,  0.4793],\n",
            "        [-0.4950,  0.5631]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0827, -2.5331],\n",
            "        [-1.0434, -0.4343],\n",
            "        [-1.2366, -0.3430],\n",
            "        [-1.3561, -0.2980]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2895, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4159,  1.4678],\n",
            "        [ 0.0473,  0.0353],\n",
            "        [-0.9516,  1.0331],\n",
            "        [-1.4119,  1.4319]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9381, -0.0544],\n",
            "        [-0.6872, -0.6991],\n",
            "        [-2.1135, -0.1288],\n",
            "        [-2.9003, -0.0566]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2317, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5808, -0.5249],\n",
            "        [-1.3439,  1.3661],\n",
            "        [-0.2481,  0.2284],\n",
            "        [ 1.1527, -1.0733]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2859, -1.3916],\n",
            "        [-2.7744, -0.0644],\n",
            "        [-0.9595, -0.4830],\n",
            "        [-0.1025, -2.3286]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.3531, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2672910133997599\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.2999, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.2670, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.1438, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 7, 0, 0]), 1: array([7, 9, 0, 0])}\n",
            "valid loss : tensor(0.3385, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([12,  8,  0,  0]), 1: array([ 8, 12,  0,  0])}\n",
            "valid loss : tensor(0.2698, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 11,  0,  0]), 1: array([11, 13,  0,  0])}\n",
            "valid loss : tensor(0.2042, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 13,  0,  0]), 1: array([13, 15,  0,  0])}\n",
            "valid loss : tensor(0.1383, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.2729, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 18,  0,  0]), 1: array([18, 18,  0,  0])}\n",
            "valid loss : tensor(0.1996, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.2027, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 22,  0,  0]), 1: array([22, 22,  0,  0])}\n",
            "valid loss : tensor(0.1714, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.2352, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.1851, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.1758, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1232, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.2151547019680341\n",
            "\n",
            "========= Epoch14 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3845, -1.3024],\n",
            "        [ 1.5911, -1.5106],\n",
            "        [ 0.6282, -0.6048],\n",
            "        [ 1.2365, -1.1379]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0659, -2.7528],\n",
            "        [-0.0440, -3.1457],\n",
            "        [-0.2558, -1.4887],\n",
            "        [-0.0890, -2.4635]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1137, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.0594,  0.1281],\n",
            "        [ 1.4096, -1.3321],\n",
            "        [-0.3346,  0.4527],\n",
            "        [-0.0856,  0.2285]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.7913, -0.6038],\n",
            "        [-0.0625, -2.8041],\n",
            "        [-1.1624, -0.3750],\n",
            "        [-0.8625, -0.5484]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.3974, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5203, -0.5054],\n",
            "        [ 0.2778, -0.3205],\n",
            "        [ 0.4968, -0.4814],\n",
            "        [ 1.2680, -1.2333]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3064, -1.3322],\n",
            "        [-0.4381, -1.0364],\n",
            "        [-0.3192, -1.2974],\n",
            "        [-0.0788, -2.5800]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.2856, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8047,  0.9101],\n",
            "        [-1.0838,  1.1404],\n",
            "        [ 1.1179, -1.0536],\n",
            "        [-0.1049,  0.0567]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8803, -0.1655],\n",
            "        [-2.3269, -0.1027],\n",
            "        [-0.1080, -2.2795],\n",
            "        [-0.7772, -0.6156]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2479, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2408,  1.3060],\n",
            "        [-1.3341,  1.4156],\n",
            "        [-1.0371,  1.1166],\n",
            "        [-0.9943,  0.9955]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6222, -0.0754],\n",
            "        [-2.8117, -0.0620],\n",
            "        [-2.2635, -0.1098],\n",
            "        [-2.1179, -0.1282]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0938, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.2711, -0.2413],\n",
            "        [ 0.6393, -0.5095],\n",
            "        [-0.9852,  1.0549],\n",
            "        [-0.7038,  0.7900]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4694, -0.9818],\n",
            "        [-0.2754, -1.4241],\n",
            "        [-2.1624, -0.1222],\n",
            "        [-1.6964, -0.2025]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2674, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8285, -0.6964],\n",
            "        [-1.3790,  1.4046],\n",
            "        [ 1.2643, -1.1598],\n",
            "        [ 0.5844, -0.4761]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1969, -1.7218],\n",
            "        [-2.8435, -0.0600],\n",
            "        [-0.0849, -2.5089],\n",
            "        [-0.2973, -1.3579]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1598, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7436,  0.6694],\n",
            "        [-0.9779,  1.0024],\n",
            "        [-0.2932,  0.2971],\n",
            "        [-0.7887,  0.7410]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6308, -0.2179],\n",
            "        [-2.1096, -0.1293],\n",
            "        [-1.0312, -0.4409],\n",
            "        [-1.7258, -0.1961]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2460, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7121, -0.6861],\n",
            "        [ 0.3983, -0.3794],\n",
            "        [-1.4814,  1.5368],\n",
            "        [ 0.8048, -0.7582]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2208, -1.6190],\n",
            "        [-0.3781, -1.1558],\n",
            "        [-3.0659, -0.0477],\n",
            "        [-0.1902, -1.7532]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.2092, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7242,  0.7143],\n",
            "        [-1.1477,  1.1187],\n",
            "        [-0.3599,  0.4509],\n",
            "        [-0.9525,  0.9998]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6515, -0.2129],\n",
            "        [-2.3651, -0.0987],\n",
            "        [-1.1786, -0.3678],\n",
            "        [-2.0850, -0.1327]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.2030, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8852, -0.8519],\n",
            "        [-1.1301,  1.0819],\n",
            "        [ 0.8957, -0.9322],\n",
            "        [-1.3013,  1.3567]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1621, -1.8993],\n",
            "        [-2.3159, -0.1039],\n",
            "        [-0.1491, -1.9769],\n",
            "        [-2.7257, -0.0677]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1207, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0258,  1.1153],\n",
            "        [ 0.0924, -0.0685],\n",
            "        [-0.3689,  0.4475],\n",
            "        [-0.0154,  0.0891]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2522, -0.1111],\n",
            "        [-0.6159, -0.7768],\n",
            "        [-1.1824, -0.3660],\n",
            "        [-0.7468, -0.6423]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.6641, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.7227, -0.6341],\n",
            "        [ 1.3725, -1.2814],\n",
            "        [ 0.2553, -0.1999],\n",
            "        [ 0.5866, -0.5298]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2291, -1.5860],\n",
            "        [-0.0680, -2.7219],\n",
            "        [-0.4912, -0.9464],\n",
            "        [-0.2833, -1.3996]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.6608, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8630, -0.8729],\n",
            "        [ 0.9477, -0.9651],\n",
            "        [ 0.1102, -0.0861],\n",
            "        [-0.7083,  0.7740]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1623, -1.8982],\n",
            "        [-0.1377, -2.0506],\n",
            "        [-0.5998, -0.7961],\n",
            "        [-1.6869, -0.2047]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2761, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8793,  0.9331],\n",
            "        [ 0.5146, -0.5319],\n",
            "        [-0.1429,  0.1183],\n",
            "        [ 1.1936, -1.1360]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9637, -0.1512],\n",
            "        [-0.3010, -1.3475],\n",
            "        [-0.8323, -0.5710],\n",
            "        [-0.0929, -2.4225]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.2790, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2816410998503367\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([0, 4, 0, 0]), 1: array([4, 0, 0, 0])}\n",
            "valid loss : tensor(0.2733, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 6, 0, 0]), 1: array([6, 2, 0, 0])}\n",
            "valid loss : tensor(0.2644, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([4, 8, 0, 0]), 1: array([8, 4, 0, 0])}\n",
            "valid loss : tensor(0.2656, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.1611, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.1858, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11, 12,  1,  0]), 1: array([12, 11,  0,  1])}\n",
            "valid loss : tensor(0.2828, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 13,  1,  0]), 1: array([13, 14,  0,  1])}\n",
            "valid loss : tensor(0.2131, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 14,  1,  0]), 1: array([14, 17,  0,  1])}\n",
            "valid loss : tensor(0.1376, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 16,  1,  0]), 1: array([16, 19,  0,  1])}\n",
            "valid loss : tensor(0.2286, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 19,  1,  0]), 1: array([19, 20,  0,  1])}\n",
            "valid loss : tensor(0.1817, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 21,  1,  0]), 1: array([21, 22,  0,  1])}\n",
            "valid loss : tensor(0.1605, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 23,  1,  0]), 1: array([23, 24,  0,  1])}\n",
            "valid loss : tensor(0.1912, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 26,  1,  0]), 1: array([26, 25,  0,  1])}\n",
            "valid loss : tensor(0.1986, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 29,  1,  0]), 1: array([29, 26,  0,  1])}\n",
            "valid loss : tensor(0.2688, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 29,  1,  0]), 1: array([29, 30,  0,  1])}\n",
            "valid loss : tensor(0.0455, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "Loss/valid : 0.20391638626654943\n",
            "\n",
            "========= Epoch15 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8381,  0.9466],\n",
            "        [ 1.1416, -1.1756],\n",
            "        [ 1.2996, -1.1680],\n",
            "        [ 0.9502, -0.9593]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9399, -0.1552],\n",
            "        [-0.0940, -2.4112],\n",
            "        [-0.0814, -2.5490],\n",
            "        [-0.1382, -2.0477]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1172, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5596,  0.6494],\n",
            "        [-0.9681,  0.9422],\n",
            "        [ 0.9706, -0.8942],\n",
            "        [ 0.9571, -0.9297]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4702, -0.2612],\n",
            "        [-2.0484, -0.1381],\n",
            "        [-0.1440, -2.0089],\n",
            "        [-0.1411, -2.0279]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1711, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.2941, -1.2240],\n",
            "        [-0.7046,  0.7627],\n",
            "        [-1.0157,  1.0736],\n",
            "        [ 1.1450, -1.1233]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0775, -2.5957],\n",
            "        [-1.6748, -0.2075],\n",
            "        [-2.2060, -0.1167],\n",
            "        [-0.0985, -2.3667]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1250, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7365,  0.8100],\n",
            "        [-0.5411,  0.5319],\n",
            "        [-0.1272,  0.2163],\n",
            "        [-0.8937,  0.9762]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.7396, -0.1931],\n",
            "        [-1.3671, -0.2942],\n",
            "        [-0.8796, -0.5361],\n",
            "        [-2.0133, -0.1434]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2917, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.4951, -0.5148],\n",
            "        [ 0.7055, -0.6424],\n",
            "        [-1.1253,  1.1724],\n",
            "        [ 0.5980, -0.5656]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3106, -1.3205],\n",
            "        [-0.2309, -1.5788],\n",
            "        [-2.3935, -0.0958],\n",
            "        [-0.2718, -1.4354]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2273, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.0485, -0.0183],\n",
            "        [-0.4971,  0.5737],\n",
            "        [-1.0104,  1.0855],\n",
            "        [-0.9861,  1.0462]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.6603, -0.7271],\n",
            "        [-1.3655, -0.2947],\n",
            "        [-2.2118, -0.1160],\n",
            "        [-2.1554, -0.1231]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2985, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8311,  0.7847],\n",
            "        [-0.5678,  0.6276],\n",
            "        [-1.0316,  1.0585],\n",
            "        [ 0.7337, -0.6455]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.7971, -0.1813],\n",
            "        [-1.4598, -0.2643],\n",
            "        [-2.2067, -0.1166],\n",
            "        [-0.2246, -1.6038]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1967, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.1419,  0.2304],\n",
            "        [-0.9154,  0.9972],\n",
            "        [-1.2287,  1.2911],\n",
            "        [-1.0716,  1.0274]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.8966, -0.5242],\n",
            "        [-2.0504, -0.1378],\n",
            "        [-2.5972, -0.0774],\n",
            "        [-2.2147, -0.1156]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.3068, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.2840, -1.2318],\n",
            "        [ 1.0816, -0.9686],\n",
            "        [-0.6490,  0.7451],\n",
            "        [ 1.2938, -1.1952]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0777, -2.5935],\n",
            "        [-0.1211, -2.1712],\n",
            "        [-1.6157, -0.2216],\n",
            "        [-0.0797, -2.5687]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1250, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9817,  1.0443],\n",
            "        [ 0.8894, -0.7720],\n",
            "        [ 1.3512, -1.2628],\n",
            "        [-0.8026,  0.9571]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1498, -0.1239],\n",
            "        [-0.1739, -1.8352],\n",
            "        [-0.0707, -2.6846],\n",
            "        [-1.9185, -0.1588]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1318, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9859, -0.9522],\n",
            "        [ 1.0126, -0.9885],\n",
            "        [-0.9829,  0.9094],\n",
            "        [ 0.3415, -0.3119]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1345, -2.0726],\n",
            "        [-0.1268, -2.1279],\n",
            "        [-2.0327, -0.1404],\n",
            "        [-0.4189, -1.0723]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2051, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4502,  0.4326],\n",
            "        [ 2.2631, -2.2179],\n",
            "        [ 0.4817, -0.4951],\n",
            "        [-0.8173,  0.9518]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.2290, -0.3462],\n",
            "        [-0.0113, -4.4922],\n",
            "        [-0.3196, -1.2963],\n",
            "        [-1.9266, -0.1574]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2086, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9681,  1.0003],\n",
            "        [ 1.1430, -1.1156],\n",
            "        [ 0.6420, -0.5421],\n",
            "        [-0.6163,  0.6231]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0991, -0.1308],\n",
            "        [-0.0994, -2.3581],\n",
            "        [-0.2670, -1.4511],\n",
            "        [-1.4937, -0.2543]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1879, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6098, -0.5317],\n",
            "        [ 1.8749, -1.7550],\n",
            "        [-1.1607,  1.2562],\n",
            "        [ 1.4786, -1.3344]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2771, -1.4187],\n",
            "        [-0.0262, -3.6560],\n",
            "        [-2.5023, -0.0854],\n",
            "        [-0.0583, -2.8713]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1118, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2918e+00, -1.3271e+00],\n",
            "        [ 3.3053e-04, -4.0554e-02],\n",
            "        [ 1.5419e+00, -1.5094e+00],\n",
            "        [-3.8201e-01,  3.9683e-01]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0703, -2.6893],\n",
            "        [-0.6729, -0.7138],\n",
            "        [-0.0462, -3.0975],\n",
            "        [-1.1565, -0.3777]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.3020, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2004356121023496\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 0, 0, 1]), 1: array([0, 3, 1, 0])}\n",
            "valid loss : tensor(0.4181, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 0, 0, 1]), 1: array([0, 7, 1, 0])}\n",
            "valid loss : tensor(0.2859, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  0,  0,  1]), 1: array([ 0, 11,  1,  0])}\n",
            "valid loss : tensor(0.1721, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11,  4,  0,  1]), 1: array([ 4, 11,  1,  0])}\n",
            "valid loss : tensor(0.1238, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([12,  7,  0,  1]), 1: array([ 7, 12,  1,  0])}\n",
            "valid loss : tensor(0.2275, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 10,  0,  1]), 1: array([10, 13,  1,  0])}\n",
            "valid loss : tensor(0.1107, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 12,  0,  1]), 1: array([12, 15,  1,  0])}\n",
            "valid loss : tensor(0.1145, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 15,  0,  1]), 1: array([15, 16,  1,  0])}\n",
            "valid loss : tensor(0.0915, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([18, 17,  0,  1]), 1: array([17, 18,  1,  0])}\n",
            "valid loss : tensor(0.1367, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 18,  0,  1]), 1: array([18, 21,  1,  0])}\n",
            "valid loss : tensor(0.2405, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 19,  0,  1]), 1: array([19, 24,  1,  0])}\n",
            "valid loss : tensor(0.1427, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 21,  0,  1]), 1: array([21, 26,  1,  0])}\n",
            "valid loss : tensor(0.1739, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 25,  0,  1]), 1: array([25, 26,  1,  0])}\n",
            "valid loss : tensor(0.0677, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 28,  0,  1]), 1: array([28, 27,  1,  0])}\n",
            "valid loss : tensor(0.1846, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([29, 30,  0,  1]), 1: array([30, 29,  1,  0])}\n",
            "valid loss : tensor(0.1828, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "Loss/valid : 0.17820524523655573\n",
            "\n",
            "========= Epoch16 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.5397,  1.6075],\n",
            "        [ 0.6542, -0.6262],\n",
            "        [-0.4415,  0.3999],\n",
            "        [-1.4957,  1.4673]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1893, -0.0421],\n",
            "        [-0.2452, -1.5256],\n",
            "        [-1.1999, -0.3584],\n",
            "        [-3.0134, -0.0504]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1740, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.3576, -0.2770],\n",
            "        [ 0.6581, -0.6202],\n",
            "        [-0.7046,  0.8041],\n",
            "        [ 0.8801, -0.8445]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4254, -1.0600],\n",
            "        [-0.2457, -1.5240],\n",
            "        [-1.7085, -0.1998],\n",
            "        [-0.1640, -1.8886]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2587, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7001,  0.6853],\n",
            "        [-0.5303,  0.6102],\n",
            "        [ 0.9694, -0.9864],\n",
            "        [ 1.6118, -1.4648]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6088, -0.2233],\n",
            "        [-1.4179, -0.2774],\n",
            "        [-0.1323, -2.0882],\n",
            "        [-0.0451, -3.1217]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1695, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7748,  0.8818],\n",
            "        [ 1.0945, -0.9719],\n",
            "        [ 1.0487, -0.8961],\n",
            "        [-1.0532,  1.1111]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8312, -0.1746],\n",
            "        [-0.1192, -2.1856],\n",
            "        [-0.1337, -2.0785],\n",
            "        [-2.2730, -0.1087]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1341, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5456, -0.5028],\n",
            "        [ 0.3297, -0.3410],\n",
            "        [-1.1753,  1.2698],\n",
            "        [-0.6595,  0.7281]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3005, -1.3489],\n",
            "        [-0.4130, -1.0837],\n",
            "        [-2.5283, -0.0832],\n",
            "        [-1.6105, -0.2229]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2549, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.3784, -2.3285],\n",
            "        [ 1.2353, -1.1607],\n",
            "        [-0.8276,  0.8644],\n",
            "        [-1.0218,  1.1590]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0090, -4.7159],\n",
            "        [-0.0872, -2.4832],\n",
            "        [-1.8611, -0.1690],\n",
            "        [-2.2878, -0.1070]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0930, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1318, -1.0013],\n",
            "        [-0.7104,  0.7202],\n",
            "        [ 0.9242, -0.9274],\n",
            "        [-1.1557,  1.2569]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1120, -2.2451],\n",
            "        [-1.6450, -0.2144],\n",
            "        [-0.1458, -1.9974],\n",
            "        [-2.4983, -0.0858]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1395, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.6441, -1.5211],\n",
            "        [ 0.5043, -0.4684],\n",
            "        [-1.5191,  1.5935],\n",
            "        [ 1.8585, -1.7261]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0413, -3.2065],\n",
            "        [-0.3207, -1.2934],\n",
            "        [-3.1561, -0.0435],\n",
            "        [-0.0274, -3.6120]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1082, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8015, -0.6895],\n",
            "        [ 1.4046, -1.3679],\n",
            "        [-0.8097,  0.8798],\n",
            "        [ 0.4439, -0.3962]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2031, -1.6941],\n",
            "        [-0.0606, -2.8332],\n",
            "        [-1.8589, -0.1694],\n",
            "        [-0.3588, -1.1990]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1980, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.3242,  0.4036],\n",
            "        [ 1.9155, -1.8764],\n",
            "        [ 1.8389, -1.7378],\n",
            "        [ 1.8684, -1.7743]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1218, -0.3941],\n",
            "        [-0.0223, -3.8141],\n",
            "        [-0.0276, -3.6044],\n",
            "        [-0.0258, -3.6685]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1174, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4296, -1.3958],\n",
            "        [-0.6369,  0.7056],\n",
            "        [-0.9659,  1.0596],\n",
            "        [-0.9039,  0.8490]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0576, -2.8830],\n",
            "        [-1.5745, -0.2321],\n",
            "        [-2.1495, -0.1239],\n",
            "        [-1.9127, -0.1598]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1433, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2140,  1.3040],\n",
            "        [-1.0037,  0.9289],\n",
            "        [ 1.0333, -1.0672],\n",
            "        [ 0.0066,  0.1025]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5956, -0.0775],\n",
            "        [-2.0678, -0.1352],\n",
            "        [-0.1155, -2.2160],\n",
            "        [-0.7423, -0.6463]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2676, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8912, -0.7906],\n",
            "        [-0.8155,  0.7708],\n",
            "        [-0.9951,  1.0346],\n",
            "        [ 1.8815, -1.8119]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1706, -1.8524],\n",
            "        [-1.7725, -0.1862],\n",
            "        [-2.1531, -0.1234],\n",
            "        [-0.0246, -3.7180]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1262, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8836,  1.0390],\n",
            "        [ 1.2716, -1.1615],\n",
            "        [-0.5338,  0.7199],\n",
            "        [-0.4068,  0.4318]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0590, -0.1365],\n",
            "        [-0.0841, -2.5172],\n",
            "        [-1.5048, -0.2511],\n",
            "        [-1.1979, -0.3593]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2078, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3571,  1.3535],\n",
            "        [ 1.1119, -1.1479],\n",
            "        [-1.2863,  1.3975],\n",
            "        [-1.2286,  1.3321]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7750, -0.0644],\n",
            "        [-0.0993, -2.3591],\n",
            "        [-2.7498, -0.0661],\n",
            "        [-2.6352, -0.0744]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0760, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.1645585889617602\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1845, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.1265, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 6, 0, 1]), 1: array([6, 5, 1, 0])}\n",
            "valid loss : tensor(0.3270, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 8, 0, 1]), 1: array([8, 7, 1, 0])}\n",
            "valid loss : tensor(0.3956, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 10,  0,  1]), 1: array([10,  9,  1,  0])}\n",
            "valid loss : tensor(0.2309, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11, 11,  0,  2]), 1: array([11, 11,  2,  0])}\n",
            "valid loss : tensor(0.3577, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 12,  0,  2]), 1: array([12, 14,  2,  0])}\n",
            "valid loss : tensor(0.1485, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 14,  0,  2]), 1: array([14, 16,  2,  0])}\n",
            "valid loss : tensor(0.2318, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 15,  0,  3]), 1: array([15, 18,  3,  0])}\n",
            "valid loss : tensor(0.4085, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 18,  0,  3]), 1: array([18, 19,  3,  0])}\n",
            "valid loss : tensor(0.0916, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 21,  0,  3]), 1: array([21, 20,  3,  0])}\n",
            "valid loss : tensor(0.0776, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 23,  0,  3]), 1: array([23, 22,  3,  0])}\n",
            "valid loss : tensor(0.0910, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 25,  0,  3]), 1: array([25, 24,  3,  0])}\n",
            "valid loss : tensor(0.1283, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 27,  0,  3]), 1: array([27, 26,  3,  0])}\n",
            "valid loss : tensor(0.1673, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 30,  0,  3]), 1: array([30, 27,  3,  0])}\n",
            "valid loss : tensor(0.1540, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.95, recall : 0.9, precision : 1.0\n",
            "class1 >>> accuracy : 0.95, recall : 1.0, precision : 0.9090909090909091\n",
            "Loss/valid : 0.20805195967356363\n",
            "\n",
            "========= Epoch17 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5859,  1.5406],\n",
            "        [-0.7092,  0.6747],\n",
            "        [ 0.5091, -0.5100],\n",
            "        [ 0.8415, -0.7383]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1694, -0.0429],\n",
            "        [-1.6075, -0.2236],\n",
            "        [-0.3082, -1.3272],\n",
            "        [-0.1873, -1.7672]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1905, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3383, -0.2867],\n",
            "        [ 0.1937, -0.1546],\n",
            "        [-1.6486,  1.7418],\n",
            "        [-1.0035,  1.1240]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4287, -1.0537],\n",
            "        [-0.5341, -0.8824],\n",
            "        [-3.4235, -0.0331],\n",
            "        [-2.2401, -0.1126]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2771, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.5230,  0.5363],\n",
            "        [-0.4555,  0.4476],\n",
            "        [ 1.7462, -1.7052],\n",
            "        [-0.8379,  0.9958]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3569, -0.2977],\n",
            "        [-1.2433, -0.3403],\n",
            "        [-0.0312, -3.4826],\n",
            "        [-1.9820, -0.1483]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.2044, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.8694,  1.9465],\n",
            "        [-0.0513,  0.1530],\n",
            "        [-2.1257,  2.2071],\n",
            "        [-1.8140,  1.9212]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.8377, -0.0218],\n",
            "        [-0.8005, -0.5962],\n",
            "        [-4.3459, -0.0130],\n",
            "        [-3.7587, -0.0236]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2147, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.1464,  0.2636],\n",
            "        [ 1.6941, -1.6553],\n",
            "        [-0.9749,  1.1293],\n",
            "        [ 1.5508, -1.4054]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.9190, -0.5090],\n",
            "        [-0.0345, -3.3839],\n",
            "        [-2.2192, -0.1151],\n",
            "        [-0.0507, -3.0069]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1773, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0872,  1.1290],\n",
            "        [ 2.4108, -2.3522],\n",
            "        [-1.1135,  1.1861],\n",
            "        [ 1.2517, -1.2049]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3197, -0.1035],\n",
            "        [-0.0085, -4.7716],\n",
            "        [-2.3952, -0.0956],\n",
            "        [-0.0823, -2.5388]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0725, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1185,  1.0617],\n",
            "        [ 1.0523, -1.0140],\n",
            "        [-1.3290,  1.4086],\n",
            "        [-1.3204,  1.4061]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2873, -0.1071],\n",
            "        [-0.1193, -2.1855],\n",
            "        [-2.8003, -0.0627],\n",
            "        [-2.7899, -0.0634]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0881, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5809, -0.4916],\n",
            "        [ 1.0056, -0.8277],\n",
            "        [-1.4662,  1.5082],\n",
            "        [-1.7459,  1.7209]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2943, -1.3668],\n",
            "        [-0.1483, -1.9817],\n",
            "        [-3.0242, -0.0498],\n",
            "        [-3.4975, -0.0307]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1308, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1230,  1.2274],\n",
            "        [-0.9179,  0.8429],\n",
            "        [ 1.4370, -1.4643],\n",
            "        [-1.1997,  1.3051]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4415, -0.0911],\n",
            "        [-1.9194, -0.1586],\n",
            "        [-0.0535, -2.9548],\n",
            "        [-2.5833, -0.0785]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0954, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 4.8559e-01, -3.6382e-01],\n",
            "        [ 1.8636e+00, -1.7117e+00],\n",
            "        [-1.4072e-03,  1.2037e-01],\n",
            "        [ 1.3526e+00, -1.2080e+00]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3560, -1.2055],\n",
            "        [-0.0276, -3.6030],\n",
            "        [-0.7559, -0.6341],\n",
            "        [-0.0744, -2.6350]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.3035, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.5056,  0.5775],\n",
            "        [ 2.0126, -1.8911],\n",
            "        [ 1.8327, -1.8508],\n",
            "        [ 0.1235, -0.0346]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3747, -0.2916],\n",
            "        [-0.0200, -3.9237],\n",
            "        [-0.0248, -3.7083],\n",
            "        [-0.6172, -0.7753]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2779, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5320,  0.6533],\n",
            "        [ 1.3313, -1.3663],\n",
            "        [ 1.3076, -1.2216],\n",
            "        [ 1.8593, -1.6968]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4520, -0.2667],\n",
            "        [-0.0652, -2.7628],\n",
            "        [-0.0767, -2.6059],\n",
            "        [-0.0282, -3.5842]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1092, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9743, -0.9302],\n",
            "        [ 1.6817, -1.5424],\n",
            "        [-1.1471,  1.3386],\n",
            "        [ 0.1153, -0.0615]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1388, -2.0433],\n",
            "        [-0.0390, -3.2631],\n",
            "        [-2.5656, -0.0800],\n",
            "        [-0.6086, -0.7855]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2166, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.3743,  0.4043],\n",
            "        [-0.7447,  0.7431],\n",
            "        [ 1.6004, -1.5545],\n",
            "        [ 2.0879, -2.0116]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1564, -0.3778],\n",
            "        [-1.6915, -0.2036],\n",
            "        [-0.0418, -3.1967],\n",
            "        [-0.0164, -4.1159]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1599, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9925,  1.0984],\n",
            "        [-0.5356,  0.6156],\n",
            "        [ 1.2706, -1.1693],\n",
            "        [ 0.5375, -0.5445]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2074, -0.1165],\n",
            "        [-1.4260, -0.2748],\n",
            "        [-0.0836, -2.5234],\n",
            "        [-0.2919, -1.3739]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1917, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.18064143260320029\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.0895, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.1162, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 7, 0, 0]), 1: array([7, 5, 0, 0])}\n",
            "valid loss : tensor(0.0884, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 9, 0, 0]), 1: array([9, 7, 0, 0])}\n",
            "valid loss : tensor(0.1593, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([10, 10,  0,  0]), 1: array([10, 10,  0,  0])}\n",
            "valid loss : tensor(0.2074, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 13,  0,  0]), 1: array([13, 11,  0,  0])}\n",
            "valid loss : tensor(0.0715, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 14,  0,  0]), 1: array([14, 14,  0,  0])}\n",
            "valid loss : tensor(0.1328, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 15,  0,  0]), 1: array([15, 17,  0,  0])}\n",
            "valid loss : tensor(0.2086, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 17,  0,  0]), 1: array([17, 19,  0,  0])}\n",
            "valid loss : tensor(0.0780, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.0951, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.1103, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.1632, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 25,  0,  0]), 1: array([25, 27,  0,  0])}\n",
            "valid loss : tensor(0.0567, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.1227, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1849, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.1256416027744611\n",
            "\n",
            "========= Epoch18 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1501,  1.2557],\n",
            "        [ 1.5637, -1.4269],\n",
            "        [ 1.7863, -1.6464],\n",
            "        [-1.2562,  1.3436]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4921, -0.0864],\n",
            "        [-0.0490, -3.0396],\n",
            "        [-0.0318, -3.4645],\n",
            "        [-2.6715, -0.0717]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0597, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5681, -0.4447],\n",
            "        [ 1.7302, -1.5693],\n",
            "        [-1.2397,  1.3373],\n",
            "        [ 1.3710, -1.1967]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3098, -1.3226],\n",
            "        [-0.0362, -3.3357],\n",
            "        [-2.6503, -0.0733],\n",
            "        [-0.0739, -2.6416]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1233, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8432,  0.7669],\n",
            "        [ 1.4527, -1.3052],\n",
            "        [ 1.4203, -1.4521],\n",
            "        [ 2.6135, -2.5569]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.7924, -0.1822],\n",
            "        [-0.0615, -2.8193],\n",
            "        [-0.0550, -2.9275],\n",
            "        [-0.0057, -5.1761]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0761, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8007,  0.9086],\n",
            "        [ 1.4179, -1.3779],\n",
            "        [ 1.8642, -1.7869],\n",
            "        [-1.3106,  1.2820]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8757, -0.1664],\n",
            "        [-0.0593, -2.8551],\n",
            "        [-0.0256, -3.6768],\n",
            "        [-2.6648, -0.0722]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0809, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8243,  0.8555],\n",
            "        [-0.8289,  0.9535],\n",
            "        [-1.2273,  1.2993],\n",
            "        [ 1.4003, -1.4363]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8507, -0.1709],\n",
            "        [-1.9379, -0.1555],\n",
            "        [-2.6035, -0.0769],\n",
            "        [-0.0570, -2.8936]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1151, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.2796, -0.2864],\n",
            "        [-1.4785,  1.5525],\n",
            "        [-1.7526,  1.7957],\n",
            "        [ 0.8732, -0.7854]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4497, -1.0157],\n",
            "        [-3.0781, -0.0471],\n",
            "        [-3.5767, -0.0284],\n",
            "        [-0.1743, -1.8329]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1749, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2708,  1.4325],\n",
            "        [ 0.9068, -0.9224],\n",
            "        [-0.7636,  0.9660],\n",
            "        [-1.0484,  1.0031]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7681, -0.0648],\n",
            "        [-0.1489, -1.9780],\n",
            "        [-1.8929, -0.1633],\n",
            "        [-2.1724, -0.1209]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1245, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.4719,  0.4418],\n",
            "        [ 1.8590, -1.7065],\n",
            "        [ 0.7855, -0.7828],\n",
            "        [ 1.1084, -1.0655]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.2510, -0.3372],\n",
            "        [-0.0279, -3.5934],\n",
            "        [-0.1893, -1.7576],\n",
            "        [-0.1077, -2.2816]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1655, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7900, -0.6680],\n",
            "        [-1.2042,  1.2203],\n",
            "        [-0.7478,  0.8337],\n",
            "        [ 0.7454, -0.6489]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2092, -1.6672],\n",
            "        [-2.5094, -0.0848],\n",
            "        [-1.7686, -0.1870],\n",
            "        [-0.2215, -1.6158]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1756, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6955, -0.6510],\n",
            "        [ 0.4115, -0.3498],\n",
            "        [ 1.1403, -1.0908],\n",
            "        [ 1.2161, -1.1648]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2312, -1.5777],\n",
            "        [-0.3833, -1.1445],\n",
            "        [-0.1020, -2.3331],\n",
            "        [-0.0884, -2.4694]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.2012, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4388,  0.5581],\n",
            "        [-0.2535,  0.2530],\n",
            "        [ 1.8140, -1.7613],\n",
            "        [-0.6406,  0.7352]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3110, -0.3141],\n",
            "        [-0.9781, -0.4716],\n",
            "        [-0.0276, -3.6029],\n",
            "        [-1.6010, -0.2253]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2597, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.5849,  1.6968],\n",
            "        [ 1.1425, -1.0307],\n",
            "        [-1.0335,  1.1253],\n",
            "        [-1.3721,  1.4187]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3185, -0.0369],\n",
            "        [-0.1078, -2.2810],\n",
            "        [-2.2680, -0.1093],\n",
            "        [-2.8504, -0.0596]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0784, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8613,  0.9906],\n",
            "        [-1.6947,  1.7767],\n",
            "        [-2.0165,  2.1020],\n",
            "        [ 0.1445, -0.0295]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9977, -0.1458],\n",
            "        [-3.5020, -0.0306],\n",
            "        [-4.1347, -0.0161],\n",
            "        [-0.6100, -0.7839]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2006, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5693, -0.4396],\n",
            "        [-0.9041,  0.9060],\n",
            "        [-0.2018,  0.3410],\n",
            "        [ 1.7891, -1.7412]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3109, -1.3198],\n",
            "        [-1.9617, -0.1515],\n",
            "        [-1.0009, -0.4581],\n",
            "        [-0.0289, -3.5593]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2374, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7028, -0.6380],\n",
            "        [-1.2480,  1.1866],\n",
            "        [-1.3598,  1.5339],\n",
            "        [ 1.3010, -1.1876]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2324, -1.5732],\n",
            "        [-2.5186, -0.0840],\n",
            "        [-2.9475, -0.0539],\n",
            "        [-0.0798, -2.5683]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1125, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.1456882769862811\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1344, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.0476, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.1852, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.1239, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.1305, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11, 13,  0,  0]), 1: array([13, 11,  0,  0])}\n",
            "valid loss : tensor(0.2136, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 16,  0,  0]), 1: array([16, 12,  0,  0])}\n",
            "valid loss : tensor(0.1802, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 18,  0,  0]), 1: array([18, 14,  0,  0])}\n",
            "valid loss : tensor(0.0694, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 20,  0,  0]), 1: array([20, 16,  0,  0])}\n",
            "valid loss : tensor(0.1703, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.0651, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.1033, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 24,  0,  0]), 1: array([24, 24,  0,  0])}\n",
            "valid loss : tensor(0.1635, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 27,  0,  0]), 1: array([27, 25,  0,  0])}\n",
            "valid loss : tensor(0.0837, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.0913, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0599, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.12144706547260284\n",
            "\n",
            "========= Epoch19 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5764, -1.5301],\n",
            "        [ 2.4825, -2.3211],\n",
            "        [ 1.0196, -1.0214],\n",
            "        [ 2.1324, -2.0464]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0438, -3.1503],\n",
            "        [-0.0082, -4.8118],\n",
            "        [-0.1221, -2.1631],\n",
            "        [-0.0152, -4.1941]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0473, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 3.0222, -2.9584],\n",
            "        [-0.2197,  0.4318],\n",
            "        [ 2.0409, -1.8489],\n",
            "        [ 1.9569, -1.8399]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5242e-03, -5.9831e+00],\n",
            "        [-1.0711e+00, -4.1953e-01],\n",
            "        [-2.0244e-02, -3.9100e+00],\n",
            "        [-2.2193e-02, -3.8190e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1161, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3607,  1.2982],\n",
            "        [ 1.3650, -1.3160],\n",
            "        [-0.9456,  1.0396],\n",
            "        [ 1.0465, -0.9919]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7266, -0.0677],\n",
            "        [-0.0663, -2.7473],\n",
            "        [-2.1139, -0.1287],\n",
            "        [-0.1224, -2.1609]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0963, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6479, -1.4940],\n",
            "        [ 0.6392, -0.5896],\n",
            "        [-1.6528,  1.7407],\n",
            "        [-1.6948,  1.6676]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0423, -3.1842],\n",
            "        [-0.2567, -1.4855],\n",
            "        [-3.4266, -0.0330],\n",
            "        [-3.3964, -0.0341]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0915, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1390,  1.3188],\n",
            "        [-1.0348,  1.0375],\n",
            "        [-0.5700,  0.7140],\n",
            "        [ 2.3027, -2.1295]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5400, -0.0822],\n",
            "        [-2.1909, -0.1186],\n",
            "        [-1.5285, -0.2444],\n",
            "        [-0.0118, -4.4440]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1142, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6631, -0.4998],\n",
            "        [ 0.0317,  0.1003],\n",
            "        [ 0.9525, -0.9797],\n",
            "        [-1.4789,  1.5932]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2720, -1.4349],\n",
            "        [-0.7281, -0.6594],\n",
            "        [-0.1353, -2.0674],\n",
            "        [-3.1173, -0.0453]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2952, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.0544, -1.9376],\n",
            "        [-0.8307,  0.9129],\n",
            "        [-0.7377,  0.7399],\n",
            "        [ 0.1887, -0.2144]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0183, -4.0102],\n",
            "        [-1.9048, -0.1612],\n",
            "        [-1.6831, -0.2055],\n",
            "        [-0.5118, -0.9149]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.3250, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4198,  1.3426],\n",
            "        [-1.2385,  1.3164],\n",
            "        [-0.7854,  0.9191],\n",
            "        [-0.8284,  0.9176]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8236, -0.0612],\n",
            "        [-2.6297, -0.0748],\n",
            "        [-1.8716, -0.1671],\n",
            "        [-1.9069, -0.1608]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1160, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.3405, -0.2877],\n",
            "        [-1.9924,  2.0427],\n",
            "        [ 0.7782, -0.6844],\n",
            "        [ 0.0806,  0.0463]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4276, -1.0558],\n",
            "        [-4.0526, -0.0175],\n",
            "        [-0.2083, -1.6709],\n",
            "        [-0.6762, -0.7104]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.3324, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.6508, -1.5941],\n",
            "        [-0.8514,  0.9649],\n",
            "        [ 1.4217, -1.4159],\n",
            "        [ 1.6745, -1.5497]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0382, -3.2832],\n",
            "        [-1.9670, -0.1507],\n",
            "        [-0.0569, -2.8945],\n",
            "        [-0.0390, -3.2632]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0712, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9427,  1.0673],\n",
            "        [ 1.0375, -1.0485],\n",
            "        [ 1.5655, -1.4603],\n",
            "        [-0.3805,  0.4906]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1358, -0.1257],\n",
            "        [-0.1171, -2.2031],\n",
            "        [-0.0474, -3.0732],\n",
            "        [-1.2207, -0.3496]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1599, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2704,  1.3545],\n",
            "        [ 2.1137, -1.9505],\n",
            "        [ 1.3409, -1.2952],\n",
            "        [ 1.5905, -1.6248]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6949, -0.0699],\n",
            "        [-0.0170, -4.0813],\n",
            "        [-0.0692, -2.7053],\n",
            "        [-0.0394, -3.2546]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0489, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3285,  1.5071],\n",
            "        [-1.2505,  1.3547],\n",
            "        [-0.4026,  0.5462],\n",
            "        [-0.6512,  0.6912]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8926, -0.0570],\n",
            "        [-2.6765, -0.0713],\n",
            "        [-1.2761, -0.3273],\n",
            "        [-1.5745, -0.2321]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1719, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.0411,  0.1114],\n",
            "        [-2.5866,  2.6829],\n",
            "        [-1.9303,  1.9493],\n",
            "        [-1.8944,  1.8519]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-7.7231e-01, -6.1980e-01],\n",
            "        [-5.2747e+00, -5.1328e-03],\n",
            "        [-3.9001e+00, -2.0447e-02],\n",
            "        [-3.7696e+00, -2.3331e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2053, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2632,  1.3142],\n",
            "        [ 0.7021, -0.5635],\n",
            "        [ 0.4846, -0.4148],\n",
            "        [-0.9066,  1.0196]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6506, -0.0732],\n",
            "        [-0.2485, -1.5141],\n",
            "        [-0.3413, -1.2407],\n",
            "        [-2.0622, -0.1360]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1998, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.1594008115430673\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.3212, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.2595, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.1047, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 7, 0, 0]), 1: array([7, 9, 0, 0])}\n",
            "valid loss : tensor(0.0650, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  9,  0,  0]), 1: array([ 9, 11,  0,  0])}\n",
            "valid loss : tensor(0.1112, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 12,  0,  0]), 1: array([12, 12,  0,  0])}\n",
            "valid loss : tensor(0.1593, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 15,  0,  0]), 1: array([15, 13,  0,  0])}\n",
            "valid loss : tensor(0.2189, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.0809, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 19,  0,  0]), 1: array([19, 17,  0,  0])}\n",
            "valid loss : tensor(0.0714, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 21,  0,  0]), 1: array([21, 19,  0,  0])}\n",
            "valid loss : tensor(0.0635, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.1017, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 24,  0,  0]), 1: array([24, 24,  0,  0])}\n",
            "valid loss : tensor(0.1294, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0647, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.1912, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0814, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.13493645340204238\n",
            "\n",
            "========= Epoch20 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8091,  0.9258],\n",
            "        [ 1.9000, -1.8476],\n",
            "        [-0.5451,  0.5843],\n",
            "        [ 2.1731, -1.9785]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8973, -0.1625],\n",
            "        [-0.0233, -3.7709],\n",
            "        [-1.4094, -0.2801],\n",
            "        [-0.0156, -4.1673]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1204, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5925,  1.6822],\n",
            "        [ 2.3054, -2.1397],\n",
            "        [-1.2409,  1.4214],\n",
            "        [ 1.4967, -1.4008]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3118, -0.0371],\n",
            "        [-0.0117, -4.4568],\n",
            "        [-2.7297, -0.0675],\n",
            "        [-0.0537, -2.9511]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0425, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.8401, -1.6872],\n",
            "        [-1.3777,  1.4285],\n",
            "        [ 1.0722, -1.0665],\n",
            "        [-1.5283,  1.7058]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0290, -3.5562],\n",
            "        [-2.8649, -0.0587],\n",
            "        [-0.1114, -2.2500],\n",
            "        [-3.2728, -0.0386]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0594, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1848,  1.1420],\n",
            "        [-1.5458,  1.6718],\n",
            "        [ 2.7300, -2.6648],\n",
            "        [-1.3525,  1.3546]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4199e+00, -9.3134e-02],\n",
            "        [-3.2569e+00, -3.9267e-02],\n",
            "        [-4.5298e-03, -5.3993e+00],\n",
            "        [-2.7717e+00, -6.4600e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0504, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5044, -0.3655],\n",
            "        [ 0.5835, -0.5341],\n",
            "        [ 0.3247, -0.1893],\n",
            "        [ 1.9088, -1.7437]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3500, -1.2198],\n",
            "        [-0.2830, -1.4006],\n",
            "        [-0.4688, -0.9828],\n",
            "        [-0.0256, -3.6780]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2818, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3434, -0.3681],\n",
            "        [ 2.2525, -2.1761],\n",
            "        [ 0.3461, -0.1202],\n",
            "        [-0.7175,  0.8029]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3994, -1.1109],\n",
            "        [-0.0119, -4.4405],\n",
            "        [-0.4869, -0.9533],\n",
            "        [-1.7181, -0.1977]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.5684, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3787, -0.2920],\n",
            "        [-2.8520,  2.9514],\n",
            "        [-2.4951,  2.5454],\n",
            "        [-1.9564,  2.0507]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1301e-01, -1.0837e+00],\n",
            "        [-5.8064e+00, -3.0129e-03],\n",
            "        [-5.0470e+00, -6.4495e-03],\n",
            "        [-4.0251e+00, -1.8023e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1101, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5380, -0.3794],\n",
            "        [-2.3013,  2.4140],\n",
            "        [ 0.9396, -0.8968],\n",
            "        [-1.2479,  1.3829]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3361, -1.2536],\n",
            "        [-4.7242, -0.0089],\n",
            "        [-0.1479, -1.9843],\n",
            "        [-2.7003, -0.0695]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1406, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0717,  1.2129],\n",
            "        [-1.2482,  1.1692],\n",
            "        [-1.6042,  1.5408],\n",
            "        [ 1.0852, -1.0300]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3816, -0.0970],\n",
            "        [-2.5028, -0.0854],\n",
            "        [-3.1871, -0.0422],\n",
            "        [-0.1139, -2.2291]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0846, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3549,  1.4330],\n",
            "        [-1.8548,  1.8243],\n",
            "        [ 1.0163, -0.9642],\n",
            "        [ 0.9619, -0.9068]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8477, -0.0597],\n",
            "        [-3.7040, -0.0249],\n",
            "        [-0.1293, -2.1098],\n",
            "        [-0.1435, -2.0123]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0894, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6783, -1.5587],\n",
            "        [ 0.8438, -0.8458],\n",
            "        [-1.2079,  1.2270],\n",
            "        [-1.3074,  1.4177]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0385, -3.2755],\n",
            "        [-0.1694, -1.8590],\n",
            "        [-2.5189, -0.0840],\n",
            "        [-2.7886, -0.0635]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0888, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9435,  1.0559],\n",
            "        [ 1.6246, -1.6604],\n",
            "        [ 0.5832, -0.4577],\n",
            "        [ 1.3247, -1.1936]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1264, -0.1270],\n",
            "        [-0.0368, -3.3218],\n",
            "        [-0.3024, -1.3433],\n",
            "        [-0.0775, -2.5958]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1359, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1446,  1.2461],\n",
            "        [ 1.9977, -2.0272],\n",
            "        [-0.7155,  0.7204],\n",
            "        [ 0.9361, -0.8655]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4783, -0.0876],\n",
            "        [-0.0177, -4.0425],\n",
            "        [-1.6493, -0.2134],\n",
            "        [-0.1528, -1.9543]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1179, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.5663, -2.3895],\n",
            "        [ 1.3155, -1.2106],\n",
            "        [ 0.8231, -0.8356],\n",
            "        [-0.2979,  0.4445]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0070, -4.9628],\n",
            "        [-0.0769, -2.6030],\n",
            "        [-0.1743, -1.8330],\n",
            "        [-1.1317, -0.3893]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1619, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2682, -1.2225],\n",
            "        [-1.0793,  1.1656],\n",
            "        [ 1.4420, -1.3239],\n",
            "        [-0.4231,  0.5186]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0796, -2.5703],\n",
            "        [-2.3456, -0.1007],\n",
            "        [-0.0610, -2.8269],\n",
            "        [-1.2709, -0.3293]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1427, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.14632095222671826\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1007, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.0676, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0998, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  5,  0,  0]), 1: array([ 5, 11,  0,  0])}\n",
            "valid loss : tensor(0.0771, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([12,  8,  0,  0]), 1: array([ 8, 12,  0,  0])}\n",
            "valid loss : tensor(0.0459, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([15,  9,  0,  0]), 1: array([ 9, 15,  0,  0])}\n",
            "valid loss : tensor(0.1726, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 11,  0,  0]), 1: array([11, 17,  0,  0])}\n",
            "valid loss : tensor(0.1338, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 13,  0,  0]), 1: array([13, 19,  0,  0])}\n",
            "valid loss : tensor(0.1104, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 15,  0,  0]), 1: array([15, 21,  0,  0])}\n",
            "valid loss : tensor(0.1264, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 17,  0,  0]), 1: array([17, 23,  0,  0])}\n",
            "valid loss : tensor(0.0580, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 21,  0,  0]), 1: array([21, 23,  0,  0])}\n",
            "valid loss : tensor(0.0330, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.0942, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 25,  0,  0]), 1: array([25, 27,  0,  0])}\n",
            "valid loss : tensor(0.1457, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.0881, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1763, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.10196693936983744\n",
            "\n",
            "========= Epoch21 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1660, -1.0481],\n",
            "        [ 1.3791, -1.2793],\n",
            "        [-1.4419,  1.5596],\n",
            "        [-1.0384,  1.1365]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1037, -2.3178],\n",
            "        [-0.0677, -2.7261],\n",
            "        [-3.0501, -0.0485],\n",
            "        [-2.2825, -0.1076]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0819, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6747,  1.8968],\n",
            "        [-1.5941,  1.6830],\n",
            "        [ 1.1195, -1.0615],\n",
            "        [ 0.9539, -0.9463]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5993, -0.0277],\n",
            "        [-3.3141, -0.0370],\n",
            "        [-0.1070, -2.2880],\n",
            "        [-0.1394, -2.0396]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0778, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8470, -0.7187],\n",
            "        [ 0.7706, -0.7696],\n",
            "        [-1.7702,  1.8606],\n",
            "        [-1.2433,  1.3262]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1897, -1.7555],\n",
            "        [-0.1942, -1.7344],\n",
            "        [-3.6569, -0.0262],\n",
            "        [-2.6433, -0.0738]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1210, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.4082,  1.5152],\n",
            "        [-0.5961,  0.6435],\n",
            "        [-0.9582,  0.9655],\n",
            "        [ 1.7959, -1.7320]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9757, -0.0524],\n",
            "        [-1.4939, -0.2542],\n",
            "        [-2.0600, -0.1363],\n",
            "        [-0.0289, -3.5569]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1180, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5048, -0.3639],\n",
            "        [-1.1669,  1.2872],\n",
            "        [ 1.1920, -1.1463],\n",
            "        [-1.3612,  1.3197]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3503, -1.2190],\n",
            "        [-2.5366, -0.0824],\n",
            "        [-0.0921, -2.4304],\n",
            "        [-2.7471, -0.0663]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1478, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0465,  1.1654],\n",
            "        [-1.5197,  1.6529],\n",
            "        [ 2.4546, -2.2720],\n",
            "        [-1.2516,  1.2559]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3158, -0.1039],\n",
            "        [-3.2137, -0.0410],\n",
            "        [-0.0088, -4.7355],\n",
            "        [-2.5858, -0.0783]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0580, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6746, -0.6893],\n",
            "        [ 0.5911, -0.5361],\n",
            "        [ 1.6174, -1.4539],\n",
            "        [ 1.0674, -0.9260]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2277, -1.5915],\n",
            "        [-0.2806, -1.4078],\n",
            "        [-0.0453, -3.1166],\n",
            "        [-0.1277, -2.1211]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1703, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9637, -1.8565],\n",
            "        [-1.0165,  0.9507],\n",
            "        [ 3.6289, -3.5574],\n",
            "        [ 0.0362,  0.1102]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1685e-02, -3.8420e+00],\n",
            "        [-2.0981e+00, -1.3090e-01],\n",
            "        [-7.5657e-04, -7.1870e+00],\n",
            "        [-7.3083e-01, -6.5684e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2025, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0664,  1.0901],\n",
            "        [-0.6198,  0.7217],\n",
            "        [ 1.5895, -1.6145],\n",
            "        [-1.3537,  1.4651]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2660, -0.1095],\n",
            "        [-1.5738, -0.2323],\n",
            "        [-0.0398, -3.2438],\n",
            "        [-2.8768, -0.0580]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1099, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.1435,  2.3345],\n",
            "        [-1.7064,  1.7933],\n",
            "        [-1.7752,  1.8303],\n",
            "        [-2.1954,  2.1697]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.4893, -0.0113],\n",
            "        [-3.5295, -0.0298],\n",
            "        [-3.6322, -0.0268],\n",
            "        [-4.3777, -0.0126]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0201, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9331, -0.8038],\n",
            "        [ 1.2185, -1.0378],\n",
            "        [-1.9637,  2.0187],\n",
            "        [ 0.4531, -0.3762]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1622, -1.8991],\n",
            "        [-0.0996, -2.3559],\n",
            "        [-4.0009, -0.0185],\n",
            "        [-0.3621, -1.1914]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1606, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.2630,  0.4274],\n",
            "        [ 1.8347, -1.8667],\n",
            "        [ 2.0123, -1.9574],\n",
            "        [ 1.1303, -0.9826]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0968, -0.4064],\n",
            "        [-0.0244, -3.7259],\n",
            "        [-0.0187, -3.9884],\n",
            "        [-0.1141, -2.2271]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1409, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.5076, -1.2980],\n",
            "        [-1.7734,  1.9018],\n",
            "        [ 1.4491, -1.3951],\n",
            "        [-0.9756,  1.1308]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0587, -2.8643],\n",
            "        [-3.7002, -0.0250],\n",
            "        [-0.0565, -2.9008],\n",
            "        [-2.2212, -0.1148]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0638, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5884,  0.5659],\n",
            "        [ 1.1429, -0.9648],\n",
            "        [ 2.1892, -2.0110],\n",
            "        [ 1.3148, -1.2539]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4283, -0.2741],\n",
            "        [-0.1147, -2.2223],\n",
            "        [-0.0149, -4.2151],\n",
            "        [-0.0738, -2.6426]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1194, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2561,  1.1777],\n",
            "        [ 2.1079, -2.0089],\n",
            "        [-1.6117,  1.8060],\n",
            "        [ 0.2995, -0.2242]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5179, -0.0841],\n",
            "        [-0.0162, -4.1330],\n",
            "        [-3.4499, -0.0323],\n",
            "        [-0.4652, -0.9889]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1494, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.11608952619135379\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0918, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.1456, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.0562, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([10,  6,  0,  0]), 1: array([ 6, 10,  0,  0])}\n",
            "valid loss : tensor(0.0366, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([13,  7,  0,  0]), 1: array([ 7, 13,  0,  0])}\n",
            "valid loss : tensor(0.0600, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 10,  0,  0]), 1: array([10, 14,  0,  0])}\n",
            "valid loss : tensor(0.0899, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 12,  0,  0]), 1: array([12, 16,  0,  0])}\n",
            "valid loss : tensor(0.1505, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 15,  0,  0]), 1: array([15, 17,  0,  0])}\n",
            "valid loss : tensor(0.0927, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 19,  0,  0]), 1: array([19, 17,  0,  0])}\n",
            "valid loss : tensor(0.1539, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.1379, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.0529, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 24,  0,  0]), 1: array([24, 24,  0,  0])}\n",
            "valid loss : tensor(0.0529, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0859, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 30,  0,  0]), 1: array([30, 26,  0,  0])}\n",
            "valid loss : tensor(0.0817, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0327, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.08808320512374242\n",
            "\n",
            "========= Epoch22 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9559, -1.8244],\n",
            "        [-1.3463,  1.4023],\n",
            "        [ 1.3829, -1.3348],\n",
            "        [ 1.9817, -1.7721]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0226, -3.8029],\n",
            "        [-2.8107, -0.0620],\n",
            "        [-0.0639, -2.7816],\n",
            "        [-0.0232, -3.7769]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0429, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7571, -1.7011],\n",
            "        [-0.3334,  0.3117],\n",
            "        [ 3.1818, -3.1112],\n",
            "        [-0.7901,  0.9116]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1003e-02, -3.4891e+00],\n",
            "        [-1.0668e+00, -4.2174e-01],\n",
            "        [-1.8475e-03, -6.2949e+00],\n",
            "        [-1.8693e+00, -1.6752e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1555, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6164,  1.6386],\n",
            "        [ 1.1332, -1.0718],\n",
            "        [ 0.5472, -0.3990],\n",
            "        [ 1.1395, -1.1668]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2929, -0.0379],\n",
            "        [-0.1046, -2.3097],\n",
            "        [-0.3280, -1.2742],\n",
            "        [-0.0950, -2.4013]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1414, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6227,  1.7325],\n",
            "        [-1.3385,  1.4285],\n",
            "        [ 1.3010, -1.2983],\n",
            "        [ 1.7039, -1.5800]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3895, -0.0343],\n",
            "        [-2.8280, -0.0609],\n",
            "        [-0.0717, -2.6710],\n",
            "        [-0.0368, -3.3208]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0509, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.6425, -2.5417],\n",
            "        [-0.6280,  0.7774],\n",
            "        [ 1.3212, -1.1851],\n",
            "        [ 2.6743, -2.4915]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0056, -5.1899],\n",
            "        [-1.6247, -0.2194],\n",
            "        [-0.0784, -2.5848],\n",
            "        [-0.0057, -5.1714]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0773, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1166,  1.1266],\n",
            "        [ 1.1075, -1.0502],\n",
            "        [-1.5528,  1.6845],\n",
            "        [-0.4453,  0.5502]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3440, -0.1009],\n",
            "        [-0.1094, -2.2671],\n",
            "        [-3.2758, -0.0385],\n",
            "        [-1.3100, -0.3145]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1408, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.1245,  2.2194],\n",
            "        [ 1.0925, -1.0243],\n",
            "        [ 0.8550, -0.7450],\n",
            "        [ 0.8220, -0.8368]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.3568, -0.0129],\n",
            "        [-0.1137, -2.2305],\n",
            "        [-0.1839, -1.7839],\n",
            "        [-0.1743, -1.8331]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1212, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.4624, -2.2911],\n",
            "        [ 2.6422, -2.4535],\n",
            "        [-0.5980,  0.7680],\n",
            "        [-0.7281,  0.9663]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0086, -4.7621],\n",
            "        [-0.0061, -5.1019],\n",
            "        [-1.5932, -0.2272],\n",
            "        [-1.8630, -0.1687]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1026, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4101,  1.4637],\n",
            "        [-1.5310,  1.4520],\n",
            "        [ 1.7706, -1.5854],\n",
            "        [-1.7920,  1.7502]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9288, -0.0549],\n",
            "        [-3.0324, -0.0494],\n",
            "        [-0.0343, -3.3903],\n",
            "        [-3.5707, -0.0285]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0418, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0590, -0.9494],\n",
            "        [-1.5025,  1.6631],\n",
            "        [-2.0391,  2.1641],\n",
            "        [-2.1282,  2.1013]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1259, -2.1343],\n",
            "        [-3.2069, -0.0413],\n",
            "        [-4.2181, -0.0148],\n",
            "        [-4.2440, -0.0145]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0491, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.5335, -1.5649],\n",
            "        [ 0.1711, -0.0217],\n",
            "        [-1.9582,  1.9648],\n",
            "        [-1.8225,  1.7534]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0441, -3.1425],\n",
            "        [-0.6014, -0.7942],\n",
            "        [-3.9426, -0.0196],\n",
            "        [-3.6035, -0.0276]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1732, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.6348,  0.7395],\n",
            "        [ 1.6674, -1.6108],\n",
            "        [ 1.5675, -1.3828],\n",
            "        [-1.1122,  1.2002]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5998, -0.2256],\n",
            "        [-0.0370, -3.3153],\n",
            "        [-0.0510, -3.0013],\n",
            "        [-2.4068, -0.0944]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1020, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0054, -0.9222],\n",
            "        [-1.6409,  1.8412],\n",
            "        [ 1.0233, -0.8738],\n",
            "        [-1.4339,  1.5598]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1358, -2.0635],\n",
            "        [-3.5123, -0.0303],\n",
            "        [-0.1398, -2.0368],\n",
            "        [-3.0426, -0.0489]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0887, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3104, -1.2977],\n",
            "        [-1.2178,  1.3369],\n",
            "        [-1.4290,  1.5703],\n",
            "        [ 1.6886, -1.6210]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0711, -2.6792],\n",
            "        [-2.6295, -0.0748],\n",
            "        [-3.0478, -0.0486],\n",
            "        [-0.0359, -3.3455]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0576, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2797,  1.3783],\n",
            "        [-1.5650,  1.7682],\n",
            "        [-1.2038,  1.2630],\n",
            "        [ 0.6517, -0.5709]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7257, -0.0677],\n",
            "        [-3.3682, -0.0351],\n",
            "        [-2.5483, -0.0814],\n",
            "        [-0.2581, -1.4806]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1106, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.09704300910234451\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0980, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 2, 0, 0]), 1: array([2, 6, 0, 0])}\n",
            "valid loss : tensor(0.1331, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.0571, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 7, 0, 0]), 1: array([7, 9, 0, 0])}\n",
            "valid loss : tensor(0.1118, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([12,  8,  0,  0]), 1: array([ 8, 12,  0,  0])}\n",
            "valid loss : tensor(0.1536, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 10,  0,  0]), 1: array([10, 14,  0,  0])}\n",
            "valid loss : tensor(0.0559, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 11,  0,  0]), 1: array([11, 17,  0,  0])}\n",
            "valid loss : tensor(0.0233, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 13,  0,  0]), 1: array([13, 19,  0,  0])}\n",
            "valid loss : tensor(0.0436, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 16,  0,  0]), 1: array([16, 20,  0,  0])}\n",
            "valid loss : tensor(0.0683, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.1066, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 22,  0,  0]), 1: array([22, 22,  0,  0])}\n",
            "valid loss : tensor(0.0347, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.0995, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 27,  0,  0]), 1: array([27, 25,  0,  0])}\n",
            "valid loss : tensor(0.0404, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0708, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0442, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.0760579360028108\n",
            "\n",
            "========= Epoch23 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.5017, -2.3947],\n",
            "        [ 1.2160, -1.2102],\n",
            "        [-1.2647,  1.4689],\n",
            "        [ 1.4270, -1.3769]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0074, -4.9039],\n",
            "        [-0.0847, -2.5108],\n",
            "        [-2.7965, -0.0630],\n",
            "        [-0.0588, -2.8626]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0535, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4888, -1.5016],\n",
            "        [-0.7503,  0.9067],\n",
            "        [-1.7131,  1.8304],\n",
            "        [-0.8240,  0.8807]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0490, -3.0394],\n",
            "        [-1.8316, -0.1745],\n",
            "        [-3.5720, -0.0285],\n",
            "        [-1.8717, -0.1671]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1048, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4355, -1.3230],\n",
            "        [ 1.7983, -1.6211],\n",
            "        [-1.6144,  1.7140],\n",
            "        [-1.5001,  1.6773]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0615, -2.8199],\n",
            "        [-0.0322, -3.4517],\n",
            "        [-3.3637, -0.0352],\n",
            "        [-3.2182, -0.0409]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0424, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.5239,  1.5846],\n",
            "        [-1.4838,  1.5743],\n",
            "        [ 1.1044, -1.0168],\n",
            "        [-1.6110,  1.7414]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1522, -0.0437],\n",
            "        [-3.1041, -0.0459],\n",
            "        [-0.1132, -2.2344],\n",
            "        [-3.3868, -0.0344]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0593, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0910, -1.0756],\n",
            "        [ 0.7305, -0.5866],\n",
            "        [-1.7106,  1.7194],\n",
            "        [-1.5386,  1.6682]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1085, -2.2751],\n",
            "        [-0.2374, -1.5545],\n",
            "        [-3.4618, -0.0319],\n",
            "        [-3.2466, -0.0397]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1043, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7189,  0.7481],\n",
            "        [ 1.9376, -1.8647],\n",
            "        [-1.1189,  1.2155],\n",
            "        [ 1.2992, -1.1390]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6745, -0.2075],\n",
            "        [-0.0221, -3.8244],\n",
            "        [-2.4268, -0.0925],\n",
            "        [-0.0837, -2.5219]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1014, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0796,  1.2137],\n",
            "        [-0.4584,  0.4428],\n",
            "        [ 1.8102, -1.5822],\n",
            "        [ 0.7731, -0.6150]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3895, -0.0962],\n",
            "        [-1.2420, -0.3408],\n",
            "        [-0.0331, -3.4255],\n",
            "        [-0.2228, -1.6109]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1732, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6518,  1.8999],\n",
            "        [-1.9425,  2.0406],\n",
            "        [ 1.7150, -1.7443],\n",
            "        [ 1.5661, -1.4988]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5800, -0.0283],\n",
            "        [-4.0016, -0.0185],\n",
            "        [-0.0310, -3.4902],\n",
            "        [-0.0456, -3.1106]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0308, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.5856,  1.5466],\n",
            "        [-0.9297,  1.0441],\n",
            "        [-1.3673,  1.2881],\n",
            "        [-1.9534,  2.1646]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1749, -0.0427],\n",
            "        [-2.1040, -0.1301],\n",
            "        [-2.7234, -0.0679],\n",
            "        [-4.1342, -0.0161]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0642, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.4469, -2.3658],\n",
            "        [-1.6006,  1.7734],\n",
            "        [ 1.1868, -1.1262],\n",
            "        [-2.2963,  2.2718]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0081, -4.8207],\n",
            "        [-3.4077, -0.0337],\n",
            "        [-0.0944, -2.4074],\n",
            "        [-4.5784, -0.0103]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0366, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8423, -0.7288],\n",
            "        [ 0.2796, -0.2126],\n",
            "        [ 0.7136, -0.5516],\n",
            "        [-1.8311,  1.8457]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1888, -1.7599],\n",
            "        [-0.4770, -0.9692],\n",
            "        [-0.2486, -1.5138],\n",
            "        [-3.7018, -0.0250]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2349, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4599, -1.3732],\n",
            "        [ 0.0395,  0.0710],\n",
            "        [ 2.3812, -2.3014],\n",
            "        [-0.8885,  1.0315]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0572, -2.8903],\n",
            "        [-0.7091, -0.6775],\n",
            "        [-0.0092, -4.6918],\n",
            "        [-2.0568, -0.1368]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2202, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6565,  1.5850],\n",
            "        [-1.6611,  1.8067],\n",
            "        [ 1.6711, -1.6110],\n",
            "        [-1.6038,  1.6643]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2799, -0.0384],\n",
            "        [-3.4984, -0.0307],\n",
            "        [-0.0369, -3.3190],\n",
            "        [-3.3055, -0.0374]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0358, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.2806, -2.0864],\n",
            "        [ 1.5058, -1.3758],\n",
            "        [ 1.5634, -1.5854],\n",
            "        [-1.6110,  1.7354]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0126, -4.3797],\n",
            "        [-0.0545, -2.9361],\n",
            "        [-0.0420, -3.1908],\n",
            "        [-3.3810, -0.0346]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0359, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7627, -1.6182],\n",
            "        [ 2.6303, -2.4329],\n",
            "        [ 1.5756, -1.3813],\n",
            "        [ 2.5049, -2.3059]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0335, -3.4143],\n",
            "        [-0.0063, -5.0695],\n",
            "        [-0.0507, -3.0076],\n",
            "        [-0.0081, -4.8188]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0246, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.08813843131065369\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0460, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.1319, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0634, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 8, 0, 0]), 1: array([8, 8, 0, 0])}\n",
            "valid loss : tensor(0.1189, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  9,  0,  0]), 1: array([ 9, 11,  0,  0])}\n",
            "valid loss : tensor(0.0731, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 12,  0,  0]), 1: array([12, 12,  0,  0])}\n",
            "valid loss : tensor(0.0730, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 14,  0,  0]), 1: array([14, 14,  0,  0])}\n",
            "valid loss : tensor(0.0490, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.0546, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 19,  0,  0]), 1: array([19, 17,  0,  0])}\n",
            "valid loss : tensor(0.0521, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([18, 22,  0,  0]), 1: array([22, 18,  0,  0])}\n",
            "valid loss : tensor(0.1303, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.0895, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.0492, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0466, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0649, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0573, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.07332033887505532\n",
            "\n",
            "========= Epoch24 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7960, -1.6803],\n",
            "        [-1.4723,  1.6831],\n",
            "        [ 2.5758, -2.4642],\n",
            "        [-1.1506,  1.3616]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0305, -3.5068],\n",
            "        [-3.1972, -0.0417],\n",
            "        [-0.0065, -5.0465],\n",
            "        [-2.5902, -0.0780]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0392, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8522,  1.9528],\n",
            "        [-1.2109,  1.3447],\n",
            "        [ 1.7970, -1.7356],\n",
            "        [ 2.5718, -2.3711]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.8271, -0.0220],\n",
            "        [-2.6304, -0.0748],\n",
            "        [-0.0288, -3.5614],\n",
            "        [-0.0071, -4.9500]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0332, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5031,  1.4628],\n",
            "        [-1.8032,  1.7773],\n",
            "        [-0.7759,  0.8900],\n",
            "        [ 2.4954, -2.2965]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.0161, -0.0502],\n",
            "        [-3.6080, -0.0275],\n",
            "        [-1.8391, -0.1731],\n",
            "        [-0.0083, -4.8001]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0648, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8562,  1.9183],\n",
            "        [-1.5449,  1.6069],\n",
            "        [ 1.6102, -1.6399],\n",
            "        [ 1.2755, -1.1294]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7971, -0.0227],\n",
            "        [-3.1938, -0.0419],\n",
            "        [-0.0380, -3.2881],\n",
            "        [-0.0864, -2.4913]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0473, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.4951, -0.3481],\n",
            "        [ 0.6663, -0.6582],\n",
            "        [-1.4159,  1.5259],\n",
            "        [ 1.2019, -1.1499]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3579, -1.2011],\n",
            "        [-0.2358, -1.5603],\n",
            "        [-2.9933, -0.0514],\n",
            "        [-0.0909, -2.4427]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1840, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.5300, -2.4346],\n",
            "        [-0.4085,  0.5430],\n",
            "        [-0.7616,  0.8256],\n",
            "        [ 3.4183, -3.2186]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-6.9563e-03, -4.9716e+00],\n",
            "        [-1.2781e+00, -3.2652e-01],\n",
            "        [-1.7732e+00, -1.8607e-01],\n",
            "        [-1.3102e-03, -6.6382e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1302, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 3.2848, -3.2053],\n",
            "        [ 1.8308, -1.8521],\n",
            "        [-1.0817,  1.2680],\n",
            "        [ 1.3024, -1.2868]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5172e-03, -6.4916e+00],\n",
            "        [-2.4839e-02, -3.7077e+00],\n",
            "        [-2.4408e+00, -9.1118e-02],\n",
            "        [-7.2393e-02, -2.6616e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0475, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5597, -1.4279],\n",
            "        [-1.4241,  1.6792],\n",
            "        [ 2.2852, -2.1010],\n",
            "        [ 1.9995, -1.7699]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0492, -3.0367],\n",
            "        [-3.1472, -0.0439],\n",
            "        [-0.0124, -4.3985],\n",
            "        [-0.0228, -3.7922]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0321, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6471, -1.5776],\n",
            "        [-0.9649,  1.1280],\n",
            "        [-1.4877,  1.5918],\n",
            "        [-1.1948,  1.2236]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0390, -3.2637],\n",
            "        [-2.2092, -0.1163],\n",
            "        [-3.1245, -0.0450],\n",
            "        [-2.5038, -0.0853]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0714, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8396, -0.8558],\n",
            "        [-1.7255,  1.8186],\n",
            "        [-1.6214,  1.6369],\n",
            "        [-2.4681,  2.5909]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1685, -1.8640],\n",
            "        [-3.5726, -0.0285],\n",
            "        [-3.2961, -0.0377],\n",
            "        [-5.0654, -0.0063]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0603, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7478, -0.6799],\n",
            "        [ 1.7475, -1.6848],\n",
            "        [ 0.6547, -0.4923],\n",
            "        [ 1.4166, -1.3355]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2150, -1.6428],\n",
            "        [-0.0318, -3.4641],\n",
            "        [-0.2758, -1.4228],\n",
            "        [-0.0618, -2.8139]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1461, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9448,  1.0980],\n",
            "        [ 2.3954, -2.3187],\n",
            "        [-0.0723,  0.2503],\n",
            "        [-0.8587,  0.9937]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1648, -0.1219],\n",
            "        [-0.0089, -4.7230],\n",
            "        [-0.8674, -0.5448],\n",
            "        [-1.9981, -0.1457]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2053, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.7238, -0.5606],\n",
            "        [ 0.7366, -0.5366],\n",
            "        [ 0.0335,  0.0528],\n",
            "        [-2.0691,  2.0787]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2444, -1.5288],\n",
            "        [-0.2468, -1.5200],\n",
            "        [-0.7029, -0.6835],\n",
            "        [-4.1635, -0.0157]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.3024, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7514,  0.8945],\n",
            "        [-1.0369,  0.9650],\n",
            "        [ 0.1117,  0.0198],\n",
            "        [ 0.4181, -0.4327]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8222, -0.1763],\n",
            "        [-2.1286, -0.1267],\n",
            "        [-0.6483, -0.7401],\n",
            "        [-0.3556, -1.2064]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.5624, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.7306,  2.8277],\n",
            "        [-2.4939,  2.4140],\n",
            "        [ 0.0287,  0.0917],\n",
            "        [-0.2333,  0.3936]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.5621e+00, -3.8479e-03],\n",
            "        [-4.9153e+00, -7.3607e-03],\n",
            "        [-7.2516e-01, -6.6213e-01],\n",
            "        [-1.0550e+00, -4.2802e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.4478, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.15825926909844082\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0883, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 2, 0, 0]), 1: array([2, 6, 0, 0])}\n",
            "valid loss : tensor(0.0602, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.1397, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([10,  6,  0,  0]), 1: array([ 6, 10,  0,  0])}\n",
            "valid loss : tensor(0.0873, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  9,  0,  0]), 1: array([ 9, 11,  0,  0])}\n",
            "valid loss : tensor(0.0781, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 11,  0,  0]), 1: array([11, 13,  0,  0])}\n",
            "valid loss : tensor(0.0888, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 14,  0,  0]), 1: array([14, 14,  0,  0])}\n",
            "valid loss : tensor(0.1444, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 15,  0,  0]), 1: array([15, 17,  0,  0])}\n",
            "valid loss : tensor(0.1581, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 18,  0,  0]), 1: array([18, 18,  0,  0])}\n",
            "valid loss : tensor(0.1839, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.0643, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 21,  0,  0]), 1: array([21, 23,  0,  0])}\n",
            "valid loss : tensor(0.0623, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.2141, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0556, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 30,  0,  0]), 1: array([30, 26,  0,  0])}\n",
            "valid loss : tensor(0.1410, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0036, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.10464081172831356\n",
            "\n",
            "========= Epoch25 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4886,  1.4186],\n",
            "        [ 1.6678, -1.5100],\n",
            "        [-0.5297,  0.4487],\n",
            "        [-0.7003,  0.7624]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9603, -0.0532],\n",
            "        [-0.0408, -3.2186],\n",
            "        [-1.2976, -0.3191],\n",
            "        [-1.6710, -0.2083]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1554, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.8595, -1.6421],\n",
            "        [-1.6256,  1.7520],\n",
            "        [-1.4495,  1.5364],\n",
            "        [-1.3260,  1.3758]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0297, -3.5313],\n",
            "        [-3.4111, -0.0336],\n",
            "        [-3.0351, -0.0493],\n",
            "        [-2.7668, -0.0649]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0444, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2530, -1.0681],\n",
            "        [ 0.8481, -0.8445],\n",
            "        [-1.3626,  1.4682],\n",
            "        [-2.0242,  2.1190]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0936, -2.4147],\n",
            "        [-0.1689, -1.8616],\n",
            "        [-2.8881, -0.0573],\n",
            "        [-4.1589, -0.0157]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0839, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.7329, -2.5419],\n",
            "        [ 1.7525, -1.6124],\n",
            "        [-1.5223,  1.6161],\n",
            "        [-1.3391,  1.4685]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.1059e-03, -5.2799e+00],\n",
            "        [-3.3982e-02, -3.3989e+00],\n",
            "        [-3.1808e+00, -4.2439e-02],\n",
            "        [-2.8662e+00, -5.8601e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0350, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.4386,  1.6447],\n",
            "        [ 1.5099, -1.4544],\n",
            "        [-1.8694,  1.9922],\n",
            "        [ 1.6834, -1.6236]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1281, -0.0448],\n",
            "        [-0.0503, -3.0146],\n",
            "        [-3.8824, -0.0208],\n",
            "        [-0.0360, -3.3430]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0380, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8161, -0.6714],\n",
            "        [ 0.8656, -0.7824],\n",
            "        [ 2.0266, -1.9179],\n",
            "        [-1.6715,  1.6411]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2037, -1.6912],\n",
            "        [-0.1760, -1.8240],\n",
            "        [-0.0192, -3.9637],\n",
            "        [-3.3483, -0.0358]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1087, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 3.1065, -2.9344],\n",
            "        [ 1.8425, -1.8512],\n",
            "        [-0.4217,  0.4480],\n",
            "        [ 2.1532, -2.1870]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3766e-03, -6.0433e+00],\n",
            "        [-2.4575e-02, -3.7183e+00],\n",
            "        [-1.2197e+00, -3.5001e-01],\n",
            "        [-1.2950e-02, -4.3531e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0975, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2971,  1.3097],\n",
            "        [ 1.3029, -1.2514],\n",
            "        [-0.6912,  0.6707],\n",
            "        [-1.3947,  1.3506]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6779, -0.0712],\n",
            "        [-0.0749, -2.6292],\n",
            "        [-1.5899, -0.2281],\n",
            "        [-2.8075, -0.0623]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1091, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.8899,  2.0972],\n",
            "        [-2.5574,  2.6958],\n",
            "        [ 1.1780, -1.0620],\n",
            "        [-2.3513,  2.4680]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.0056e+00, -1.8382e-02],\n",
            "        [-5.2585e+00, -5.2169e-03],\n",
            "        [-1.0117e-01, -2.3411e+00],\n",
            "        [-4.8273e+00, -8.0401e-03]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0332, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.7212, -0.5643],\n",
            "        [-1.8177,  1.9970],\n",
            "        [ 1.1051, -1.0321],\n",
            "        [-2.0776,  2.0842]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2441, -1.5297],\n",
            "        [-3.8366, -0.0218],\n",
            "        [-0.1115, -2.2487],\n",
            "        [-4.1772, -0.0155]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0982, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.3662, -1.2746],\n",
            "        [ 1.7658, -1.7914],\n",
            "        [-1.8111,  1.8727],\n",
            "        [-1.2418,  1.3685]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0689, -2.7097],\n",
            "        [-0.0281, -3.5853],\n",
            "        [-3.7086, -0.0248],\n",
            "        [-2.6813, -0.0709]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0482, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.9064,  2.0075],\n",
            "        [ 2.5578, -2.3632],\n",
            "        [ 1.6892, -1.6213],\n",
            "        [ 3.3418, -3.2659]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.9337e+00, -1.9766e-02],\n",
            "        [-7.2653e-03, -4.9283e+00],\n",
            "        [-3.5847e-02, -3.3464e+00],\n",
            "        [-1.3491e-03, -6.6090e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0161, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7354,  0.9056],\n",
            "        [ 1.8462, -1.6909],\n",
            "        [ 2.8400, -2.6401],\n",
            "        [ 1.8255, -1.6942]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8181e+00, -1.7714e-01],\n",
            "        [-2.8680e-02, -3.5658e+00],\n",
            "        [-4.1602e-03, -5.4843e+00],\n",
            "        [-2.9178e-02, -3.5489e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0598, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0034,  1.1647],\n",
            "        [ 1.5758, -1.5055],\n",
            "        [-1.5633,  1.7099],\n",
            "        [ 1.0683, -1.0571]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2765, -0.1083],\n",
            "        [-0.0449, -3.1262],\n",
            "        [-3.3104, -0.0372],\n",
            "        [-0.1128, -2.2382]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0758, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0934, -1.0318],\n",
            "        [-0.6823,  0.7952],\n",
            "        [ 1.4083, -1.2935],\n",
            "        [-1.2759,  1.5336]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1128, -2.2380],\n",
            "        [-1.6830, -0.2056],\n",
            "        [-0.0649, -2.7668],\n",
            "        [-2.8680, -0.0585]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1104, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.07423717814187208\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.0748, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 6, 0, 0]), 1: array([6, 2, 0, 0])}\n",
            "valid loss : tensor(0.0388, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 9, 0, 0]), 1: array([9, 3, 0, 0])}\n",
            "valid loss : tensor(0.0429, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.0280, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 8, 12,  0,  0]), 1: array([12,  8,  0,  0])}\n",
            "valid loss : tensor(0.0446, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([10, 14,  0,  0]), 1: array([14, 10,  0,  0])}\n",
            "valid loss : tensor(0.0458, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11, 17,  0,  0]), 1: array([17, 11,  0,  0])}\n",
            "valid loss : tensor(0.0515, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.0787, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 18,  0,  0]), 1: array([18, 18,  0,  0])}\n",
            "valid loss : tensor(0.0567, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([18, 22,  0,  0]), 1: array([22, 18,  0,  0])}\n",
            "valid loss : tensor(0.0253, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 24,  0,  0]), 1: array([24, 20,  0,  0])}\n",
            "valid loss : tensor(0.0420, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 26,  0,  0]), 1: array([26, 22,  0,  0])}\n",
            "valid loss : tensor(0.0850, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 28,  0,  0]), 1: array([28, 24,  0,  0])}\n",
            "valid loss : tensor(0.0627, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0954, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1206, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.05950921786328157\n",
            "\n",
            "========= Epoch26 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.3292,  2.4305],\n",
            "        [-1.1960,  1.3062],\n",
            "        [ 1.3223, -1.2066],\n",
            "        [-1.3258,  1.4981]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.7682, -0.0085],\n",
            "        [-2.5810, -0.0787],\n",
            "        [-0.0767, -2.6056],\n",
            "        [-2.8815, -0.0577]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0554, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0440,  1.0262],\n",
            "        [ 1.9543, -1.7570],\n",
            "        [ 1.7055, -1.7380],\n",
            "        [ 2.3590, -2.1564]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1890, -0.1188],\n",
            "        [-0.0242, -3.7355],\n",
            "        [-0.0315, -3.4750],\n",
            "        [-0.0109, -4.5263]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0463, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4429, -1.3815],\n",
            "        [ 0.7832, -0.6189],\n",
            "        [-1.8952,  2.1089],\n",
            "        [-1.8268,  1.8897]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0576, -2.8821],\n",
            "        [-0.2200, -1.6221],\n",
            "        [-4.0222, -0.0181],\n",
            "        [-3.7405, -0.0240]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0799, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5688, -1.5919],\n",
            "        [-1.2734,  1.3334],\n",
            "        [ 3.0996, -3.0210],\n",
            "        [ 1.2177, -1.2291]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1522e-02, -3.2022e+00],\n",
            "        [-2.6780e+00, -7.1177e-02],\n",
            "        [-2.1947e-03, -6.1228e+00],\n",
            "        [-8.3030e-02, -2.5298e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0495, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6847,  1.6569],\n",
            "        [-1.3958,  1.4120],\n",
            "        [-1.9038,  1.9670],\n",
            "        [-1.7853,  1.7945]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3764, -0.0348],\n",
            "        [-2.8664, -0.0586],\n",
            "        [-3.8914, -0.0206],\n",
            "        [-3.6073, -0.0275]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0354, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0041,  1.1195],\n",
            "        [ 2.3436, -2.1421],\n",
            "        [-2.0535,  2.1825],\n",
            "        [-1.6937,  1.8297]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2365, -0.1130],\n",
            "        [-0.0112, -4.4969],\n",
            "        [-4.2503, -0.0144],\n",
            "        [-3.5525, -0.0291]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0419, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.6114,  2.7575],\n",
            "        [ 0.4723, -0.3855],\n",
            "        [ 0.7755, -0.7611],\n",
            "        [-2.0602,  2.1602]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.3736e+00, -4.6484e-03],\n",
            "        [-3.5353e-01, -1.2113e+00],\n",
            "        [-1.9484e-01, -1.7314e+00],\n",
            "        [-4.2350e+00, -1.4586e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1419, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9276, -1.8640],\n",
            "        [ 1.8790, -1.7456],\n",
            "        [ 1.8128, -1.7424],\n",
            "        [ 2.3231, -2.0902]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0223, -3.8139],\n",
            "        [-0.0263, -3.6510],\n",
            "        [-0.0282, -3.5834],\n",
            "        [-0.0120, -4.4253]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0222, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.2943, -2.0942],\n",
            "        [-1.2597,  1.3632],\n",
            "        [-1.0209,  0.9393],\n",
            "        [-1.6255,  1.5525]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0123, -4.4009],\n",
            "        [-2.6930, -0.0701],\n",
            "        [-2.0920, -0.1318],\n",
            "        [-3.2189, -0.0408]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0637, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.0175,  2.1436],\n",
            "        [ 1.3308, -1.2777],\n",
            "        [-1.2318,  1.3255],\n",
            "        [ 1.2234, -1.1287]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1766, -0.0155],\n",
            "        [-0.0711, -2.6795],\n",
            "        [-2.6320, -0.0747],\n",
            "        [-0.0909, -2.4430]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0630, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2692,  1.4635],\n",
            "        [-1.3876,  1.3466],\n",
            "        [-1.1679,  1.4298],\n",
            "        [ 1.1985, -1.1316]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7957, -0.0630],\n",
            "        [-2.7971, -0.0629],\n",
            "        [-2.6695, -0.0718],\n",
            "        [-0.0928, -2.4229]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0726, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.6104, -1.4580],\n",
            "        [-1.5926,  1.7286],\n",
            "        [-1.5053,  1.6406],\n",
            "        [ 1.4810, -1.3623]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0454, -3.1139],\n",
            "        [-3.3566, -0.0355],\n",
            "        [-3.1880, -0.0421],\n",
            "        [-0.0566, -2.8999]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0449, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.8349,  1.9889],\n",
            "        [ 1.4411, -1.2752],\n",
            "        [-1.5994,  1.6293],\n",
            "        [-1.8146,  2.0364]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.8454, -0.0216],\n",
            "        [-0.0640, -2.7803],\n",
            "        [-3.2676, -0.0388],\n",
            "        [-3.8720, -0.0210]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0364, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3691, -1.2859],\n",
            "        [-1.2364,  1.4047],\n",
            "        [ 2.1655, -2.0463],\n",
            "        [ 1.1637, -1.1543]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0679, -2.7229],\n",
            "        [-2.7100, -0.0689],\n",
            "        [-0.0147, -4.2265],\n",
            "        [-0.0939, -2.4119]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0614, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0688, -0.9003],\n",
            "        [ 1.2510, -1.0941],\n",
            "        [ 2.4369, -2.2480],\n",
            "        [ 1.6865, -1.6084]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1307, -2.0997],\n",
            "        [-0.0915, -2.4366],\n",
            "        [-0.0092, -4.6941],\n",
            "        [-0.0364, -3.3313]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0669, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.05877038165926933\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0528, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.0964, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.0973, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([10,  6,  0,  0]), 1: array([ 6, 10,  0,  0])}\n",
            "valid loss : tensor(0.1688, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  9,  0,  0]), 1: array([ 9, 11,  0,  0])}\n",
            "valid loss : tensor(0.1567, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 13,  0,  0]), 1: array([13, 11,  0,  0])}\n",
            "valid loss : tensor(0.1319, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 15,  0,  0]), 1: array([15, 13,  0,  0])}\n",
            "valid loss : tensor(0.1377, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 15,  0,  0]), 1: array([15, 17,  0,  0])}\n",
            "valid loss : tensor(0.0338, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 16,  0,  0]), 1: array([16, 20,  0,  0])}\n",
            "valid loss : tensor(0.0209, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 19,  0,  0]), 1: array([19, 21,  0,  0])}\n",
            "valid loss : tensor(0.0593, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.1513, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.0847, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 28,  0,  0]), 1: array([28, 24,  0,  0])}\n",
            "valid loss : tensor(0.1312, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0770, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0323, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.09547545375923315\n",
            "\n",
            "========= Epoch27 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.4897, -2.3323],\n",
            "        [ 1.7644, -1.7461],\n",
            "        [ 3.1501, -2.9438],\n",
            "        [-1.0629,  1.2212]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-8.0186e-03, -4.8300e+00],\n",
            "        [-2.9444e-02, -3.5399e+00],\n",
            "        [-2.2541e-03, -6.0961e+00],\n",
            "        [-2.3811e+00, -9.7004e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0342, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8297,  0.9258],\n",
            "        [ 2.1929, -2.0561],\n",
            "        [ 2.2700, -2.1489],\n",
            "        [ 2.4441, -2.2360]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9149, -0.1594],\n",
            "        [-0.0142, -4.2632],\n",
            "        [-0.0120, -4.4308],\n",
            "        [-0.0092, -4.6893]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0487, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0068,  1.2747],\n",
            "        [-0.7861,  0.9706],\n",
            "        [ 2.0152, -1.9418],\n",
            "        [ 3.6406, -3.5550]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3788e+00, -9.7244e-02],\n",
            "        [-1.9159e+00, -1.5925e-01],\n",
            "        [-1.8939e-02, -3.9760e+00],\n",
            "        [-7.4966e-04, -7.1963e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0690, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7916, -1.7258],\n",
            "        [ 1.4821, -1.5001],\n",
            "        [ 1.8298, -1.8596],\n",
            "        [ 1.0062, -0.8341]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0292, -3.5467],\n",
            "        [-0.0494, -3.0316],\n",
            "        [-0.0247, -3.7141],\n",
            "        [-0.1474, -1.9877]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0627, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6235, -1.6366],\n",
            "        [ 2.2208, -1.9771],\n",
            "        [-1.7473,  1.8145],\n",
            "        [-1.6939,  1.9225]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0377, -3.2977],\n",
            "        [-0.0149, -4.2129],\n",
            "        [-3.5897, -0.0280],\n",
            "        [-3.6430, -0.0265]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0268, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.9794, -2.7644],\n",
            "        [-1.0766,  1.1469],\n",
            "        [-1.9008,  2.0051],\n",
            "        [-1.3049,  1.4507]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1976e-03, -5.7470e+00],\n",
            "        [-2.3262e+00, -1.0277e-01],\n",
            "        [-3.9258e+00, -1.9923e-02],\n",
            "        [-2.8172e+00, -6.1636e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0469, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.3402,  2.4933],\n",
            "        [-1.8085,  1.9139],\n",
            "        [-1.9111,  1.8363],\n",
            "        [ 1.3049, -1.2506]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.8414, -0.0079],\n",
            "        [-3.7463, -0.0239],\n",
            "        [-3.7706, -0.0233],\n",
            "        [-0.0748, -2.6302]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0325, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1534,  1.2697],\n",
            "        [-1.7853,  1.8171],\n",
            "        [ 0.8707, -0.6993],\n",
            "        [-1.7618,  1.9664]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5080, -0.0849],\n",
            "        [-3.6293, -0.0269],\n",
            "        [-0.1890, -1.7590],\n",
            "        [-3.7519, -0.0238]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0811, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3888,  1.5618],\n",
            "        [ 1.4959, -1.3703],\n",
            "        [ 0.8752, -0.8039],\n",
            "        [-1.0111,  1.1349]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.0016, -0.0510],\n",
            "        [-0.0554, -2.9216],\n",
            "        [-0.1710, -1.8502],\n",
            "        [-2.2566, -0.1106]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0970, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.0215,  2.1608],\n",
            "        [-1.6695,  1.6304],\n",
            "        [ 2.7364, -2.5157],\n",
            "        [ 0.8636, -0.7685]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1974e+00, -1.5148e-02],\n",
            "        [-3.3361e+00, -3.6223e-02],\n",
            "        [-5.2228e-03, -5.2573e+00],\n",
            "        [-1.7858e-01, -1.8107e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0588, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3469,  1.5751],\n",
            "        [-2.0532,  2.1877],\n",
            "        [ 1.6483, -1.5597],\n",
            "        [ 1.7155, -1.6334]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9745, -0.0524],\n",
            "        [-4.2552, -0.0143],\n",
            "        [-0.0396, -3.2477],\n",
            "        [-0.0345, -3.3835]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0352, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3782,  1.4009],\n",
            "        [ 1.4409, -1.3372],\n",
            "        [ 2.4761, -2.2810],\n",
            "        [ 1.8218, -1.7512]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8394, -0.0602],\n",
            "        [-0.0603, -2.8385],\n",
            "        [-0.0086, -4.7656],\n",
            "        [-0.0277, -3.6007]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0392, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5670,  1.6790],\n",
            "        [-1.1981,  1.3407],\n",
            "        [ 1.5426, -1.3787],\n",
            "        [ 1.8417, -1.6662]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2841, -0.0382],\n",
            "        [-2.6148, -0.0760],\n",
            "        [-0.0525, -2.9738],\n",
            "        [-0.0295, -3.5374]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0490, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6288,  1.6037],\n",
            "        [-1.3601,  1.5024],\n",
            "        [-1.6137,  1.6281],\n",
            "        [-1.2737,  1.1926]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2712, -0.0387],\n",
            "        [-2.9181, -0.0556],\n",
            "        [-3.2801, -0.0384],\n",
            "        [-2.5478, -0.0815]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0535, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8373,  0.8271],\n",
            "        [ 1.1294, -1.1156],\n",
            "        [ 2.2240, -2.0957],\n",
            "        [-1.5195,  1.5889]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8378, -0.1734],\n",
            "        [-0.1007, -2.3457],\n",
            "        [-0.0132, -4.3329],\n",
            "        [-3.1521, -0.0437]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0827, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.05449334805210431\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 0, 0, 0]), 1: array([0, 4, 0, 0])}\n",
            "valid loss : tensor(0.0848, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 2, 0, 0]), 1: array([2, 6, 0, 0])}\n",
            "valid loss : tensor(0.0367, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.0147, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 8, 0, 0]), 1: array([8, 8, 0, 0])}\n",
            "valid loss : tensor(0.0195, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.0200, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([10, 14,  0,  0]), 1: array([14, 10,  0,  0])}\n",
            "valid loss : tensor(0.0392, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([12, 16,  0,  0]), 1: array([16, 12,  0,  0])}\n",
            "valid loss : tensor(0.0615, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 19,  0,  0]), 1: array([19, 13,  0,  0])}\n",
            "valid loss : tensor(0.0567, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 19,  0,  0]), 1: array([19, 17,  0,  0])}\n",
            "valid loss : tensor(0.0941, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 22,  0,  0]), 1: array([22, 18,  0,  0])}\n",
            "valid loss : tensor(0.0122, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 24,  0,  0]), 1: array([24, 20,  0,  0])}\n",
            "valid loss : tensor(0.0391, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 27,  0,  0]), 1: array([27, 21,  0,  0])}\n",
            "valid loss : tensor(0.0512, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 28,  0,  0]), 1: array([28, 24,  0,  0])}\n",
            "valid loss : tensor(0.0927, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0522, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0870, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.05076772024234136\n",
            "\n",
            "========= Epoch28 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.9033, -0.7277],\n",
            "        [ 2.2871, -2.0672],\n",
            "        [ 1.4992, -1.3742],\n",
            "        [-1.7686,  1.8674]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1788, -1.8097],\n",
            "        [-0.0128, -4.3671],\n",
            "        [-0.0550, -2.9284],\n",
            "        [-3.6621, -0.0260]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0681, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5355, -1.5471],\n",
            "        [-0.9516,  1.0708],\n",
            "        [ 1.9487, -1.7331],\n",
            "        [ 1.9635, -1.9798]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0448, -3.1274],\n",
            "        [-2.1467, -0.1243],\n",
            "        [-0.0249, -3.7067],\n",
            "        [-0.0192, -3.9624]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0533, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.7441,  1.8577],\n",
            "        [-1.7975,  1.8674],\n",
            "        [-1.6606,  1.7711],\n",
            "        [-1.1859,  1.2595]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.6288, -0.0269],\n",
            "        [-3.6902, -0.0253],\n",
            "        [-3.4635, -0.0318],\n",
            "        [-2.5286, -0.0831]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0418, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.1226,  2.2654],\n",
            "        [-2.1029,  2.3377],\n",
            "        [ 1.4743, -1.3892],\n",
            "        [ 1.5706, -1.4421]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.4003, -0.0123],\n",
            "        [-4.4523, -0.0117],\n",
            "        [-0.0555, -2.9190],\n",
            "        [-0.0480, -3.0607]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0319, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.5306, -1.4387],\n",
            "        [ 2.3083, -2.1084],\n",
            "        [-2.1956,  2.3555],\n",
            "        [-1.6146,  1.7653]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0501, -3.0193],\n",
            "        [-0.0120, -4.4287],\n",
            "        [-4.5616, -0.0105],\n",
            "        [-3.4134, -0.0335]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0265, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0872, -1.0119],\n",
            "        [ 2.4679, -2.2550],\n",
            "        [-2.2058,  2.3142],\n",
            "        [-1.8767,  2.0412]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1156, -2.2148],\n",
            "        [-0.0088, -4.7318],\n",
            "        [-4.5309, -0.0108],\n",
            "        [-3.9376, -0.0197]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0387, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.5508, -2.4193],\n",
            "        [ 1.9941, -1.8274],\n",
            "        [ 1.8546, -1.7764],\n",
            "        [ 1.2302, -1.1298]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0069, -4.9771],\n",
            "        [-0.0217, -3.8432],\n",
            "        [-0.0261, -3.6572],\n",
            "        [-0.0902, -2.4502]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0362, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9254, -1.7569],\n",
            "        [-1.2209,  1.3661],\n",
            "        [-1.6527,  1.7922],\n",
            "        [-1.4878,  1.4629]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0249, -3.7072],\n",
            "        [-2.6595, -0.0726],\n",
            "        [-3.4763, -0.0314],\n",
            "        [-3.0017, -0.0510]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0449, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.5170, -1.3348],\n",
            "        [ 1.9997, -1.8568],\n",
            "        [-1.2325,  1.3789],\n",
            "        [-0.5582,  0.6886]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0561, -2.9080],\n",
            "        [-0.0209, -3.8775],\n",
            "        [-2.6823, -0.0709],\n",
            "        [-1.4995, -0.2526]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1001, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9955, -0.8881],\n",
            "        [ 1.1455, -1.1217],\n",
            "        [-2.0265,  1.9497],\n",
            "        [ 1.4827, -1.4093]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1415, -2.0252],\n",
            "        [-0.0986, -2.3658],\n",
            "        [-3.9948, -0.0186],\n",
            "        [-0.0540, -2.9460]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0782, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0636,  1.1358],\n",
            "        [-0.9298,  1.1107],\n",
            "        [ 2.0220, -1.8385],\n",
            "        [-1.5416,  1.5581]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3046, -0.1051],\n",
            "        [-2.1626, -0.1222],\n",
            "        [-0.0208, -3.8813],\n",
            "        [-3.1438, -0.0441]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0731, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6673,  1.8872],\n",
            "        [ 2.7788, -2.5460],\n",
            "        [-1.6466,  1.6728],\n",
            "        [-1.7258,  1.9625]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5827e+00, -2.8195e-02],\n",
            "        [-4.8576e-03, -5.3296e+00],\n",
            "        [-3.3549e+00, -3.5535e-02],\n",
            "        [-3.7130e+00, -2.4707e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0233, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.0378,  2.0729],\n",
            "        [-2.0986,  2.3799],\n",
            "        [-1.9826,  1.9457],\n",
            "        [-1.0449,  1.0376]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1270, -0.0163],\n",
            "        [-4.4899, -0.0113],\n",
            "        [-3.9477, -0.0195],\n",
            "        [-2.2000, -0.1174]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0411, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4414, -1.3703],\n",
            "        [ 2.7342, -2.6416],\n",
            "        [ 1.7042, -1.7310],\n",
            "        [-2.2827,  2.2013]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.8364e-02, -2.8701e+00],\n",
            "        [-4.6164e-03, -5.3805e+00],\n",
            "        [-3.1710e-02, -3.4669e+00],\n",
            "        [-4.4953e+00, -1.1224e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0265, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7518, -0.7352],\n",
            "        [ 1.1941, -1.1372],\n",
            "        [-1.7946,  1.9922],\n",
            "        [ 1.5287, -1.2716]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2038, -1.6908],\n",
            "        [-0.0927, -2.4241],\n",
            "        [-3.8092, -0.0224],\n",
            "        [-0.0590, -2.8593]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0945, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.05188818909227848\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.0400, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.0198, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.1225, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 8, 0, 0]), 1: array([8, 8, 0, 0])}\n",
            "valid loss : tensor(0.0311, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 8, 12,  0,  0]), 1: array([12,  8,  0,  0])}\n",
            "valid loss : tensor(0.0838, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 15,  0,  0]), 1: array([15,  9,  0,  0])}\n",
            "valid loss : tensor(0.0235, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 15,  0,  0]), 1: array([15, 13,  0,  0])}\n",
            "valid loss : tensor(0.0297, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.0414, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 21,  0,  0]), 1: array([21, 15,  0,  0])}\n",
            "valid loss : tensor(0.0254, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 23,  0,  0]), 1: array([23, 17,  0,  0])}\n",
            "valid loss : tensor(0.0456, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.0455, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 26,  0,  0]), 1: array([26, 22,  0,  0])}\n",
            "valid loss : tensor(0.0433, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 27,  0,  0]), 1: array([27, 25,  0,  0])}\n",
            "valid loss : tensor(0.0404, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.0264, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0149, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.04221765492111444\n",
            "\n",
            "========= Epoch29 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.2880, -2.0299],\n",
            "        [-2.1414,  2.2524],\n",
            "        [-1.8394,  2.0083],\n",
            "        [-2.2069,  2.3492]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0132, -4.3312],\n",
            "        [-4.4061, -0.0123],\n",
            "        [-3.8688, -0.0211],\n",
            "        [-4.5665, -0.0104]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0143, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5100, -1.4932],\n",
            "        [ 1.0576, -0.9538],\n",
            "        [-1.1600,  1.3586],\n",
            "        [ 1.9131, -1.9272]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0484, -3.0517],\n",
            "        [-0.1256, -2.1369],\n",
            "        [-2.5961, -0.0775],\n",
            "        [-0.0213, -3.8616]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0682, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.7171,  1.9606],\n",
            "        [ 1.5549, -1.3825],\n",
            "        [-0.6238,  0.7568],\n",
            "        [ 1.6362, -1.5773]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7026, -0.0250],\n",
            "        [-0.0516, -2.9890],\n",
            "        [-1.6048, -0.2243],\n",
            "        [-0.0394, -3.2529]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0851, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.0253,  1.9473],\n",
            "        [-0.8888,  0.8831],\n",
            "        [ 1.6625, -1.4381],\n",
            "        [ 1.4781, -1.2927]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.9912, -0.0187],\n",
            "        [-1.9290, -0.1570],\n",
            "        [-0.0440, -3.1446],\n",
            "        [-0.0607, -2.8316]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0701, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7253, -1.6494],\n",
            "        [-1.7266,  1.9678],\n",
            "        [-1.5944,  1.6698],\n",
            "        [-1.7596,  1.9163]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0337, -3.4084],\n",
            "        [-3.7190, -0.0246],\n",
            "        [-3.3018, -0.0375],\n",
            "        [-3.7009, -0.0250]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0302, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.2801, -2.0621],\n",
            "        [-1.6217,  1.7705],\n",
            "        [ 1.5057, -1.4142],\n",
            "        [ 2.0279, -2.0551]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0129, -4.3552],\n",
            "        [-3.4253, -0.0331],\n",
            "        [-0.0525, -2.9725],\n",
            "        [-0.0167, -4.0997]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0288, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8051,  1.7686],\n",
            "        [-1.2119,  1.3360],\n",
            "        [ 1.2865, -1.2612],\n",
            "        [ 1.7923, -1.6188]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.6014, -0.0277],\n",
            "        [-2.6232, -0.0753],\n",
            "        [-0.0753, -2.6231],\n",
            "        [-0.0325, -3.4436]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0527, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5367, -1.3895],\n",
            "        [ 2.3220, -2.1130],\n",
            "        [ 1.0656, -0.9860],\n",
            "        [ 1.6644, -1.5817]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0522, -2.9785],\n",
            "        [-0.0118, -4.4468],\n",
            "        [-0.1209, -2.1725],\n",
            "        [-0.0382, -3.2843]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0558, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.9521,  2.1209],\n",
            "        [ 2.1761, -2.0432],\n",
            "        [-1.1022,  1.3943],\n",
            "        [-1.3769,  1.5271]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.0899, -0.0169],\n",
            "        [-0.0146, -4.2339],\n",
            "        [-2.5757, -0.0792],\n",
            "        [-2.9574, -0.0534]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0410, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3024, -1.1121],\n",
            "        [-1.9021,  2.0200],\n",
            "        [-1.2145,  1.2957],\n",
            "        [ 1.6499, -1.5505]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0856, -2.5001],\n",
            "        [-3.9417, -0.0196],\n",
            "        [-2.5883, -0.0781],\n",
            "        [-0.0399, -3.2404]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0558, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.5625, -2.4218],\n",
            "        [-1.8678,  1.9880],\n",
            "        [ 2.9625, -2.7288],\n",
            "        [-1.9487,  1.9253]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-6.8212e-03, -4.9911e+00],\n",
            "        [-3.8767e+00, -2.0937e-02],\n",
            "        [-3.3695e-03, -5.6947e+00],\n",
            "        [-3.8946e+00, -2.0562e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0129, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.9966,  2.1497],\n",
            "        [ 1.8519, -1.7157],\n",
            "        [-1.6018,  1.6390],\n",
            "        [-2.1140,  2.1882]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1620, -0.0157],\n",
            "        [-0.0278, -3.5954],\n",
            "        [-3.2791, -0.0384],\n",
            "        [-4.3157, -0.0134]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0238, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2696, -1.2810],\n",
            "        [ 3.4163, -3.3210],\n",
            "        [ 1.0271, -0.8398],\n",
            "        [-1.6461,  1.7505]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-7.5138e-02, -2.6258e+00],\n",
            "        [-1.1852e-03, -6.7385e+00],\n",
            "        [-1.4376e-01, -2.0107e+00],\n",
            "        [-3.4295e+00, -3.2939e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0633, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3504,  1.3813],\n",
            "        [-1.3956,  1.3151],\n",
            "        [ 2.3813, -2.3050],\n",
            "        [-1.1233,  1.3119]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7948, -0.0631],\n",
            "        [-2.7751, -0.0644],\n",
            "        [-0.0092, -4.6954],\n",
            "        [-2.5192, -0.0840]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0551, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8564,  2.0884],\n",
            "        [ 1.1647, -1.0483],\n",
            "        [-2.1377,  2.1570],\n",
            "        [ 2.8772, -2.6323]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.9639e+00, -1.9172e-02],\n",
            "        [-1.0380e-01, -2.3168e+00],\n",
            "        [-4.3082e+00, -1.3550e-02],\n",
            "        [-4.0399e-03, -5.5135e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0351, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.04615026786923408\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.0578, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.0448, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0173, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 7, 0, 0]), 1: array([7, 9, 0, 0])}\n",
            "valid loss : tensor(0.0399, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([13,  7,  0,  0]), 1: array([ 7, 13,  0,  0])}\n",
            "valid loss : tensor(0.0316, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 10,  0,  0]), 1: array([10, 14,  0,  0])}\n",
            "valid loss : tensor(0.0532, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 12,  0,  0]), 1: array([12, 16,  0,  0])}\n",
            "valid loss : tensor(0.0178, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 13,  0,  0]), 1: array([13, 19,  0,  0])}\n",
            "valid loss : tensor(0.0371, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 16,  0,  0]), 1: array([16, 20,  0,  0])}\n",
            "valid loss : tensor(0.0303, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 19,  0,  0]), 1: array([19, 21,  0,  0])}\n",
            "valid loss : tensor(0.0564, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 22,  0,  0]), 1: array([22, 22,  0,  0])}\n",
            "valid loss : tensor(0.0298, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.0720, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0432, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 29,  0,  0]), 1: array([29, 27,  0,  0])}\n",
            "valid loss : tensor(0.0163, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0255, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.038203761105736096\n",
            "\n",
            "========= Epoch30 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2317,  1.4380],\n",
            "        [-1.2787,  1.3630],\n",
            "        [ 1.8502, -1.7171],\n",
            "        [ 1.3516, -1.1591]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7367, -0.0670],\n",
            "        [-2.7105, -0.0688],\n",
            "        [-0.0278, -3.5951],\n",
            "        [-0.0781, -2.5888]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0604, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.1078,  2.0854],\n",
            "        [ 1.4942, -1.5061],\n",
            "        [-1.7979,  1.9499],\n",
            "        [ 2.4874, -2.2645]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.2082, -0.0150],\n",
            "        [-0.0486, -3.0488],\n",
            "        [-3.7711, -0.0233],\n",
            "        [-0.0086, -4.7605]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0239, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.0096,  2.1851],\n",
            "        [-2.0103,  2.1347],\n",
            "        [-1.5190,  1.6740],\n",
            "        [-1.7296,  1.8926]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.2097, -0.0150],\n",
            "        [-4.1608, -0.0157],\n",
            "        [-3.2332, -0.0402],\n",
            "        [-3.6485, -0.0264]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0243, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.9033,  1.8224],\n",
            "        [-2.7619,  2.9103],\n",
            "        [ 1.9939, -1.9173],\n",
            "        [-1.3144,  1.4417]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7495e+00, -2.3812e-02],\n",
            "        [-5.6757e+00, -3.4342e-03],\n",
            "        [-1.9819e-02, -3.9310e+00],\n",
            "        [-2.8177e+00, -6.1602e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0272, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.2759,  2.5702],\n",
            "        [ 1.9221, -1.9473],\n",
            "        [ 1.3628, -1.2668],\n",
            "        [-2.6229,  2.7946]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.8539e+00, -7.8282e-03],\n",
            "        [-2.0657e-02, -3.8900e+00],\n",
            "        [-6.9625e-02, -2.6992e+00],\n",
            "        [-5.4219e+00, -4.4287e-03]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0256, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.9070,  2.1553],\n",
            "        [-1.0178,  1.0154],\n",
            "        [-1.9883,  2.0268],\n",
            "        [ 1.2947, -1.1009]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.0794, -0.0171],\n",
            "        [-2.1563, -0.1230],\n",
            "        [-4.0329, -0.0179],\n",
            "        [-0.0872, -2.4828]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0613, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.4128,  2.4885],\n",
            "        [-2.0706,  2.0357],\n",
            "        [-2.4628,  2.6198],\n",
            "        [-2.0511,  2.1709]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.9086, -0.0074],\n",
            "        [-4.1226, -0.0163],\n",
            "        [-5.0888, -0.0062],\n",
            "        [-4.2366, -0.0146]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0111, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4276,  1.5679],\n",
            "        [ 1.4166, -1.3362],\n",
            "        [ 3.1232, -3.0246],\n",
            "        [-1.8194,  2.0101]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.0443e+00, -4.8802e-02],\n",
            "        [-6.1797e-02, -2.8146e+00],\n",
            "        [-2.1360e-03, -6.1499e+00],\n",
            "        [-3.8511e+00, -2.1487e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0336, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9707, -0.9500],\n",
            "        [ 1.5440, -1.4034],\n",
            "        [ 1.5538, -1.4689],\n",
            "        [ 0.5916, -0.4820]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1367, -2.0573],\n",
            "        [-0.0511, -2.9985],\n",
            "        [-0.0475, -3.0702],\n",
            "        [-0.2940, -1.3677]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1323, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.2276, -2.0453],\n",
            "        [ 2.9549, -2.6842],\n",
            "        [ 2.6809, -2.4463],\n",
            "        [ 1.8709, -1.7867]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3845e-02, -4.2867e+00],\n",
            "        [-3.5498e-03, -5.6426e+00],\n",
            "        [-5.9158e-03, -5.1331e+00],\n",
            "        [-2.5468e-02, -3.6830e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0122, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3319,  1.5882],\n",
            "        [-1.5149,  1.6323],\n",
            "        [ 1.8894, -1.6957],\n",
            "        [ 3.7526, -3.5017]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9726e+00, -5.2527e-02],\n",
            "        [-3.1893e+00, -4.2075e-02],\n",
            "        [-2.7355e-02, -3.6125e+00],\n",
            "        [-7.0690e-04, -7.2550e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0307, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8818,  0.9893],\n",
            "        [-0.8640,  0.9446],\n",
            "        [ 2.6174, -2.6287],\n",
            "        [ 1.8990, -1.8699]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0143, -0.1432],\n",
            "        [-1.9604, -0.1518],\n",
            "        [-0.0053, -5.2513],\n",
            "        [-0.0228, -3.7918]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0808, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.0758,  2.0968],\n",
            "        [ 2.4818, -2.2649],\n",
            "        [ 1.4231, -1.3024],\n",
            "        [ 1.6184, -1.5134]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.1879, -0.0153],\n",
            "        [-0.0086, -4.7554],\n",
            "        [-0.0635, -2.7889],\n",
            "        [-0.0427, -3.1746]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0325, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.3712,  1.4056],\n",
            "        [ 1.9836, -1.8313],\n",
            "        [-1.9238,  1.8433],\n",
            "        [ 3.2394, -2.9963]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8372e+00, -6.0376e-02],\n",
            "        [-2.1801e-02, -3.8367e+00],\n",
            "        [-3.7900e+00, -2.2856e-02],\n",
            "        [-1.9562e-03, -6.2377e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0267, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.0769, -1.8926],\n",
            "        [ 2.7366, -2.5884],\n",
            "        [-1.3860,  1.6261],\n",
            "        [ 1.6299, -1.5682]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8706e-02, -3.9882e+00],\n",
            "        [-4.8565e-03, -5.3299e+00],\n",
            "        [-3.0601e+00, -4.8016e-02],\n",
            "        [-4.0031e-02, -3.2380e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0279, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.04070151423414548\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.0263, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.0177, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 7, 0, 0]), 1: array([7, 5, 0, 0])}\n",
            "valid loss : tensor(0.0572, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 9, 0, 0]), 1: array([9, 7, 0, 0])}\n",
            "valid loss : tensor(0.0617, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([10, 10,  0,  0]), 1: array([10, 10,  0,  0])}\n",
            "valid loss : tensor(0.0317, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 12,  0,  0]), 1: array([12, 12,  0,  0])}\n",
            "valid loss : tensor(0.0318, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 14,  0,  0]), 1: array([14, 14,  0,  0])}\n",
            "valid loss : tensor(0.0434, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 16,  0,  0]), 1: array([16, 16,  0,  0])}\n",
            "valid loss : tensor(0.0269, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 17,  0,  0]), 1: array([17, 19,  0,  0])}\n",
            "valid loss : tensor(0.0269, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 18,  0,  0]), 1: array([18, 22,  0,  0])}\n",
            "valid loss : tensor(0.0395, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 21,  0,  0]), 1: array([21, 23,  0,  0])}\n",
            "valid loss : tensor(0.0376, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.0264, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0547, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.0262, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0332, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.03608286194503307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rqE6e_NyNsaR"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}