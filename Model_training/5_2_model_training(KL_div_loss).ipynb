{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_2_model_training(KL_div_loss).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qAEbLZ7vsTVo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU 설정"
      ],
      "metadata": {
        "id": "X0t7yrUOg6lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NY4iGyKcgjif",
        "outputId": "3dee5cb7-c7a2-4fc9-ae86-e470c3ea3377"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 가져오기"
      ],
      "metadata": {
        "id": "8CGmvK_xg3uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/drive/MyDrive/고모부_머신러닝/dogncat'\n",
        "path = []\n",
        "dataset_type = []\n",
        "label = []\n",
        "for dir_name, _, file_names in os.walk(dir_path):\n",
        "    for file_name in file_names:\n",
        "        file_path = dir_name + '/' + file_name\n",
        "        path.append(file_path)\n",
        "    \n",
        "        if '/training_set' in file_path:\n",
        "            dataset_type.append('train')\n",
        "        elif '/test_set' in file_path:\n",
        "            dataset_type.append('test')\n",
        "        else:\n",
        "            dataset_type.append('N/A')\n",
        "\n",
        "        if '/cats' in file_path:\n",
        "            label.append('CAT')\n",
        "        elif '/dogs' in file_path:\n",
        "            label.append('DOG')\n",
        "        else:\n",
        "            label.append('N/A')\n",
        "\n",
        "cnd_df = pd.DataFrame({'path' : path, 'type' : dataset_type, 'label' : label})\n",
        "            "
      ],
      "metadata": {
        "id": "LkxUK6rEg-Xq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XozN7BI4g-ui",
        "outputId": "f9cc7fff-b5ff-4a3e-fa49-668af83857e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10032 entries, 0 to 10031\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   path    10032 non-null  object\n",
            " 1   type    10032 non-null  object\n",
            " 2   label   10032 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 235.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "Wryqn3cYrTyG",
        "outputId": "6a020164-17a0-4b14-e396-01ecee6314f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path  type label\n",
              "0  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "1  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "2  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "3  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "4  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "5  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "6  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "7  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "8  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "9  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff8fc6da-e34a-4989-a7c7-387b9a173319\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff8fc6da-e34a-4989-a7c7-387b9a173319')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff8fc6da-e34a-4989-a7c7-387b9a173319 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff8fc6da-e34a-4989-a7c7-387b9a173319');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df = cnd_df[cnd_df['path'].str.contains('.jpg')]"
      ],
      "metadata": {
        "id": "HnbcGm6KlfCy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXSzkKp6lsN2",
        "outputId": "b7c82456-b8a0-455a-c35a-9e3517ae1615"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10028 entries, 0 to 10031\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   path    10028 non-null  object\n",
            " 1   type    10028 non-null  object\n",
            " 2   label   10028 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 313.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnd_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "2Zh0hAbQrEu3",
        "outputId": "6bde2f6e-13b9-4e69-89e3-f009f7e0f86e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path  type label\n",
              "0  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "1  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "2  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "3  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT\n",
              "4  /content/drive/MyDrive/고모부_머신러닝/dognc...  test   CAT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-112b267f-76db-4e6d-a951-5eb46f28fb00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>test</td>\n",
              "      <td>CAT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-112b267f-76db-4e6d-a951-5eb46f28fb00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-112b267f-76db-4e6d-a951-5eb46f28fb00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-112b267f-76db-4e6d-a951-5eb46f28fb00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, validation 구분\n",
        "train_df, valid_df = train_test_split(cnd_df[cnd_df['type']=='train'], test_size = 0.25)\n",
        "\n",
        "# onehotencoding\n",
        "train_onehot = pd.get_dummies(train_df['label'])\n",
        "valid_onehot = pd.get_dummies(valid_df['label'])\n",
        "\n",
        "train_df = pd.concat([train_df, train_onehot], axis=1)\n",
        "valid_df = pd.concat([valid_df, valid_onehot], axis=1)\n",
        "\n",
        "train_df['label_idx'] = train_df.iloc[:, 3:].values.argmax(axis=1)\n",
        "valid_df['label_idx'] = valid_df.iloc[:, 3:].values.argmax(axis=1)"
      ],
      "metadata": {
        "id": "NFAW5uESmr1v"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[:, 3:].values.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pikphOpBXJFB",
        "outputId": "edd1bd26-f040-4a04-bbf6-b3b87af7e7f0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0sroPlZSsXG0",
        "outputId": "abd41a25-27cf-490a-a681-77cc20ae6ed4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   path   type label  CAT  \\\n",
              "5276  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3027  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4270  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3873  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2318  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "...                                                 ...    ...   ...  ...   \n",
              "6671  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "3462  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2999  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3577  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2454  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "\n",
              "      DOG  label_idx  \n",
              "5276    0          0  \n",
              "3027    0          0  \n",
              "4270    0          0  \n",
              "3873    0          0  \n",
              "2318    0          0  \n",
              "...   ...        ...  \n",
              "6671    1          1  \n",
              "3462    0          0  \n",
              "2999    0          0  \n",
              "3577    0          0  \n",
              "2454    0          0  \n",
              "\n",
              "[6003 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36f8d99e-a842-4fdb-9e05-9109985e14a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "      <th>CAT</th>\n",
              "      <th>DOG</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5276</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3027</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4270</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3873</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2318</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6671</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3577</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2454</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6003 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36f8d99e-a842-4fdb-9e05-9109985e14a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36f8d99e-a842-4fdb-9e05-9109985e14a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36f8d99e-a842-4fdb-9e05-9109985e14a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(len(valid_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDTPLZhCrxvL",
        "outputId": "3d9cfa78-06f9-4ab0-fd99-baf658c03266"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6003\n",
            "2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample용 데이터\n",
        "\n",
        "sample_dog = train_df[train_df['label'] == 'DOG'].sample(30)\n",
        "sample_cat = train_df[train_df['label'] == 'CAT'].sample(30)\n",
        "\n",
        "sample_data = pd.concat([sample_dog, sample_cat])"
      ],
      "metadata": {
        "id": "GbzO3016sNUc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DJKhhaPZZ2Ss",
        "outputId": "38deb1b7-7e38-4838-9d64-9b7ebf935729"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    path   type label  CAT  \\\n",
              "9820   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8532   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6702   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7618   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9684   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8554   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7706   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8855   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7682   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8607   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9534   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8419   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7227   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9372   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9607   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "10024  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7082   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6318   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6403   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8946   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "7436   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6858   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6438   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "8312   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6440   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6115   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6775   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "10021  /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "6847   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "9630   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   DOG    0   \n",
              "4921   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3924   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4342   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4409   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3493   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4667   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2495   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4251   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2396   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3847   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3751   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2559   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5620   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2069   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3841   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5796   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5034   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3229   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4418   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3758   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3402   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "6002   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "5265   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3804   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3996   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4840   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3110   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "3703   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "4268   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "2201   /content/drive/MyDrive/고모부_머신러닝/dognc...  train   CAT    1   \n",
              "\n",
              "       DOG  label_idx  \n",
              "9820     1          1  \n",
              "8532     1          1  \n",
              "6702     1          1  \n",
              "7618     1          1  \n",
              "9684     1          1  \n",
              "8554     1          1  \n",
              "7706     1          1  \n",
              "8855     1          1  \n",
              "7682     1          1  \n",
              "8607     1          1  \n",
              "9534     1          1  \n",
              "8419     1          1  \n",
              "7227     1          1  \n",
              "9372     1          1  \n",
              "9607     1          1  \n",
              "10024    1          1  \n",
              "7082     1          1  \n",
              "6318     1          1  \n",
              "6403     1          1  \n",
              "8946     1          1  \n",
              "7436     1          1  \n",
              "6858     1          1  \n",
              "6438     1          1  \n",
              "8312     1          1  \n",
              "6440     1          1  \n",
              "6115     1          1  \n",
              "6775     1          1  \n",
              "10021    1          1  \n",
              "6847     1          1  \n",
              "9630     1          1  \n",
              "4921     0          0  \n",
              "3924     0          0  \n",
              "4342     0          0  \n",
              "4409     0          0  \n",
              "3493     0          0  \n",
              "4667     0          0  \n",
              "2495     0          0  \n",
              "4251     0          0  \n",
              "2396     0          0  \n",
              "3847     0          0  \n",
              "3751     0          0  \n",
              "2559     0          0  \n",
              "5620     0          0  \n",
              "2069     0          0  \n",
              "3841     0          0  \n",
              "5796     0          0  \n",
              "5034     0          0  \n",
              "3229     0          0  \n",
              "4418     0          0  \n",
              "3758     0          0  \n",
              "3402     0          0  \n",
              "6002     0          0  \n",
              "5265     0          0  \n",
              "3804     0          0  \n",
              "3996     0          0  \n",
              "4840     0          0  \n",
              "3110     0          0  \n",
              "3703     0          0  \n",
              "4268     0          0  \n",
              "2201     0          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-408c4833-3ba1-42e4-83ad-9b8ef2ea0cef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>type</th>\n",
              "      <th>label</th>\n",
              "      <th>CAT</th>\n",
              "      <th>DOG</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9820</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8532</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6702</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7618</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9684</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8554</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7706</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8855</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7682</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8607</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9534</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8419</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7227</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9372</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9607</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10024</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7082</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6318</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6403</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8946</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7436</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6858</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6438</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8312</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6440</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6115</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6775</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10021</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6847</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9630</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>DOG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4921</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3924</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4342</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4409</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3493</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4667</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4251</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3847</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3751</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2559</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5620</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2069</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3841</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5796</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5034</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3229</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4418</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3758</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3402</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6002</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3804</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4840</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3703</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4268</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2201</th>\n",
              "      <td>/content/drive/MyDrive/고모부_머신러닝/dognc...</td>\n",
              "      <td>train</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-408c4833-3ba1-42e4-83ad-9b8ef2ea0cef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-408c4833-3ba1-42e4-83ad-9b8ef2ea0cef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-408c4833-3ba1-42e4-83ad-9b8ef2ea0cef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSs3Dx85PeWp",
        "outputId": "b364e868-266f-42e1-d8bc-d160c33c4f21"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 60 entries, 9820 to 2201\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   path       60 non-null     object\n",
            " 1   type       60 non-null     object\n",
            " 2   label      60 non-null     object\n",
            " 3   CAT        60 non-null     uint8 \n",
            " 4   DOG        60 non-null     uint8 \n",
            " 5   label_idx  60 non-null     int64 \n",
            "dtypes: int64(1), object(3), uint8(2)\n",
            "memory usage: 2.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 커스텀 데이터 셋 만들기\n",
        "- KL_div_loss를 사용하려면 onehot label과 인덱스 라벨 둘 다 필요하다.\n",
        "- onehot label은 loss 계산용\n",
        "- index label은 accuracy, recall, precision 계산용"
      ],
      "metadata": {
        "id": "ChOVST4KuQrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset2(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(MyDataset2, self).__init__()\n",
        "        self.path = df['path'].values\n",
        "        self.label = df['label_idx'].values # 클래스 idx가 들어있는 레이블 데이터. Accuracy, recall, precision 계산에 사용.\n",
        "        self.label_onehot = df.iloc[:, 3:5].values # 클래스 idx를 onehot encoding한 레이블 데이터. kl_div loss 계산에 사용.\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.cvtColor(cv2.imread(self.path[idx]), cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (244,244))\n",
        "        image = np.asarray(image, dtype=np.float32).transpose(2,0,1)\n",
        "        norm_image = (image - np.amin(image)) / (np.amax(image) - np.amin(image))\n",
        "\n",
        "        label = np.asarray(self.label[idx], dtype=np.float32)\n",
        "        label_onehot = np.asarray(self.label_onehot[idx], dtype=np.float32)\n",
        "\n",
        "        return norm_image, label, label_onehot"
      ],
      "metadata": {
        "id": "YG8V2wCcxtuC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = MyDataset2(train_df)\n",
        "valid_dataset2 = MyDataset2(valid_df)\n",
        "sample_dataset2 = MyDataset2(sample_data)"
      ],
      "metadata": {
        "id": "wnSMVZpC0Qhz"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터로더 만들기"
      ],
      "metadata": {
        "id": "liL-OIS_yrz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader2 = DataLoader(train_dataset2, batch_size=8, shuffle=True)\n",
        "valid_loader2 = DataLoader(valid_dataset2, batch_size=8, shuffle=False)\n",
        "sample_loader2 = DataLoader(sample_dataset2, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "l9GXUm2f0-qI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample_loader2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7FWf-SNGCiC",
        "outputId": "1b32f4f7-f816-47e9-8320-03dc7a4aa276"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 만들기"
      ],
      "metadata": {
        "id": "zR4SShLAzMzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 5, kernel_size=3, padding='same')\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(5*244*244, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('input size :', x.shape)\n",
        "        # print('input max :', torch.amax(x, dim=(1,2,3)))\n",
        "        # print('input min :', torch.amin(x, dim=(1,2,3)))\n",
        "        # print('input l2norm :', torch.linalg.vector_norm(x, dim=(1,2,3)))\n",
        "\n",
        "        conv = self.conv1(x)\n",
        "        # print('conv size :', conv.shape)\n",
        "        # print('conv max :', torch.amax(conv, dim=(1,2,3)))\n",
        "        # print('conv min :', torch.amin(conv, dim=(1,2,3)))\n",
        "        # print('conv l2norm :', torch.linalg.vector_norm(conv, dim=(1,2,3)))\n",
        "\n",
        "        conv_out = self.relu(conv)\n",
        "        # print('conv_out size :', conv_out.shape)\n",
        "        # print('conv_out max :', torch.amax(conv_out, dim=(1,2,3)))\n",
        "        # print('conv_out min :', torch.amin(conv_out, dim=(1,2,3)))\n",
        "        # print('conv_out l2norm :', torch.linalg.vector_norm(conv_out, dim=(1,2,3)))\n",
        "\n",
        "        fc_input = conv_out.view(conv_out.size(0), -1)\n",
        "        # print('fc_input size :', fc_input.shape)\n",
        "        # print('fc_input max :', torch.amax(fc_input, dim=(1)))\n",
        "        # print('fc_input min :', torch.amin(fc_input, dim=(1)))\n",
        "        # print('fc_input l2norm :', torch.linalg.vector_norm(fc_input, dim=(1)))\n",
        "\n",
        "        fc_logit = self.fc1(fc_input)\n",
        "        # print('fc_logit size :', fc_logit.shape)\n",
        "        # print('fc_logit max :', torch.amax(fc_logit, dim=(1)))\n",
        "        # print('fc_logit min :', torch.amin(fc_logit, dim=(1)))\n",
        "        # print('fc_logit l2norm :', torch.linalg.vector_norm(fc_logit, dim=(1)))\n",
        "        # print()\n",
        "\n",
        "        return fc_logit"
      ],
      "metadata": {
        "id": "VSXfN1KDzWZM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd2Swq0pBQe7",
        "outputId": "e81ed8ae-fa43-4d82-eaf9-b9c8de534eca"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=297680, out_features=2, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss function"
      ],
      "metadata": {
        "id": "4pJtT1qOBTx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.KLDivLoss(reduction='batchmean').to(device)"
      ],
      "metadata": {
        "id": "ygVVDGjWBZG2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optimizer, lr_scheduler"
      ],
      "metadata": {
        "id": "LeTelJ6oBfTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(model.parameters(), lr=0.001)\n",
        "scheduler = LinearLR(optimizer)"
      ],
      "metadata": {
        "id": "RRnlRjBxBf79"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix, Accuracy, recall, precision"
      ],
      "metadata": {
        "id": "XteaDB4KB-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_conf_matrix(predicted:float, label:float, class_idx:float):\n",
        "    TP_num = 0\n",
        "    TN_num = 0\n",
        "    FP_num = 0\n",
        "    FN_num = 0\n",
        "    for i in range(len(predicted)):\n",
        "        if predicted[i] == class_idx and label[i] == class_idx:\n",
        "            TP_num += 1\n",
        "\n",
        "        elif predicted[i] == class_idx and label[i] != class_idx:\n",
        "            FP_num += 1\n",
        "            \n",
        "        elif predicted[i] != class_idx and label[i] == class_idx:\n",
        "            FN_num += 1\n",
        "\n",
        "        elif predicted[i] != class_idx and label[i] != class_idx:\n",
        "            TN_num += 1\n",
        "\n",
        "    return TP_num, TN_num, FP_num, FN_num\n",
        "\n",
        "def calculate_accuracy(conf_matrix_list):\n",
        "    try:\n",
        "        accuracy = (conf_matrix_list[0] + conf_matrix_list[1]) / (conf_matrix_list[0] + conf_matrix_list[1] + conf_matrix_list[2] + conf_matrix_list[3])\n",
        "    except ZeroDivisionError:\n",
        "        accuracy = 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_recall(conf_matrix_list):\n",
        "    try: \n",
        "        recall = conf_matrix_list[0] / (conf_matrix_list[0] + conf_matrix_list[3])\n",
        "    except ZeroDivisionError:\n",
        "        recall = 0\n",
        "    return recall\n",
        "\n",
        "def calculate_precision(conf_matrix_list):\n",
        "    try:\n",
        "        precision = conf_matrix_list[0] / (conf_matrix_list[0] + conf_matrix_list[2])\n",
        "    except ZeroDivisionError:\n",
        "        precision = 0\n",
        "    return precision\n",
        "\n",
        "def calculate_acc_rec_pre(conf_matrix_dict : dict):\n",
        "    for class_idx in conf_matrix_dict.keys():\n",
        "        accuracy = calculate_accuracy(conf_matrix_dict[class_idx])\n",
        "        recall = calculate_recall(conf_matrix_dict[class_idx])\n",
        "        precision = calculate_precision(conf_matrix_dict[class_idx])\n",
        "\n",
        "        print(f'class{class_idx} >>> accuracy : {accuracy}, recall : {recall}, precision : {precision}')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "mTSZU_8IDGRl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 30\n",
        "running_loss = 0.\n",
        "running_vloss = 0.\n",
        "class_num = 2\n",
        "accuracy = 0.\n",
        "recall = 0.\n",
        "precision = 0.\n",
        "\n",
        "for i in range(epoch):\n",
        "    print()\n",
        "    print(f'========= Epoch{i+1} =========')\n",
        "    print()\n",
        "    # train\n",
        "    model.train()\n",
        "    for batch_idx, data in enumerate(sample_loader2):\n",
        "        print(f'------ train batch{batch_idx + 1} ------')\n",
        "        image_data, label_idx, label_onehot = data\n",
        "        image_data = image_data.to(device)\n",
        "        label_onehot = label_onehot.to(device)\n",
        "        print('Label :', label_onehot)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tr_output= model(image_data)\n",
        "        print('predicted :', tr_output)\n",
        "\n",
        "        lsf = nn.LogSoftmax(dim=1)\n",
        "        tr_output_lsf = lsf(tr_output)\n",
        "        print('predicted(logsoftmax) :', tr_output_lsf)\n",
        "\n",
        "        loss = loss_fn(tr_output_lsf, label_onehot) \n",
        "        print(f'train batch{batch_idx+1} loss(kl_div) :', loss)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx + 1 == len(sample_loader2):\n",
        "            avg_loss = running_loss / len(sample_loader2)\n",
        "            print('Loss/train :', avg_loss)\n",
        "            running_loss = 0.\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # #evaluate\n",
        "    model.eval()\n",
        "    TP_TN_FP_FN = {}\n",
        "    for batch_idx, val_data in enumerate(sample_loader2): # training data로 우선 evaluation해보기\n",
        "        print(f'------ valid batch{batch_idx+1} ------')\n",
        "        v_image, v_label, v_label_onehot = val_data\n",
        "        v_image = v_image.to(device)\n",
        "        v_label_onehot = v_label_onehot.to(device)\n",
        "        val_output = model(v_image)\n",
        "        val_output_lsf = lsf(val_output)\n",
        "        val_output_idx = torch.argmax(val_output, dim=1)\n",
        "        print(f'predicted : {val_output_idx}, actual : {v_label}')\n",
        "\n",
        "        for i, class_idx in enumerate(range(class_num)):\n",
        "                TP, TN, FP, FN = calculate_conf_matrix(val_output_idx, v_label, class_idx) # val_output_idx, valid_label 모두 벡터형태로 입력해야 한다.\n",
        "                if batch_idx == 0:\n",
        "                    TP_TN_FP_FN[class_idx] = np.array([TP, TN, FP, FN])\n",
        "                else:\n",
        "                    TP_TN_FP_FN[class_idx] += np.array([TP, TN, FP, FN])\n",
        "\n",
        "        print(f'TP_TN_FP_FN : {TP_TN_FP_FN}')\n",
        "\n",
        "        vloss = loss_fn(val_output_lsf, v_label_onehot)\n",
        "        print(f'valid loss :', vloss)\n",
        "        running_vloss += vloss.item()\n",
        "        \n",
        "        if batch_idx + 1 == len(sample_loader2):\n",
        "            avg_vloss = running_vloss / len(sample_loader2)\n",
        "            calculate_acc_rec_pre(TP_TN_FP_FN)\n",
        "            print('Loss/valid :', avg_vloss)\n",
        "            # print('Accuracy :', accuracy_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            # print('Recall :', recall_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            # print('Precision :', precision_score(valid_label.cpu().detach().numpy(), val_output_idx.cpu().detach().numpy()))\n",
        "            running_vloss = 0.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3W_OrbWDJCl",
        "outputId": "1e46a8d4-dc3f-4583-c46e-aa5d781edde4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8439,  1.0333],\n",
            "        [-0.1857,  0.4360],\n",
            "        [-0.9439,  1.1068],\n",
            "        [ 0.0049,  0.1774]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0196, -0.1424],\n",
            "        [-1.0515, -0.4299],\n",
            "        [-2.1718, -0.1210],\n",
            "        [-0.7831, -0.6106]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.3691, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6364, -0.4945],\n",
            "        [ 0.8429, -0.8181],\n",
            "        [-0.9649,  1.1520],\n",
            "        [ 0.3250, -0.1016]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2797, -1.4106],\n",
            "        [-0.1739, -1.8349],\n",
            "        [-2.2306, -0.1137],\n",
            "        [-0.5024, -0.9291]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2674, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0587, -0.9089],\n",
            "        [-0.0356,  0.1437],\n",
            "        [ 0.2102, -0.1344],\n",
            "        [ 1.5723, -1.3929]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1308, -2.0985],\n",
            "        [-0.7868, -0.6075],\n",
            "        [-0.5356, -0.8802],\n",
            "        [-0.0503, -3.0155]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.4172, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.4358, -0.3018],\n",
            "        [-0.4594,  0.5532],\n",
            "        [ 0.8146, -0.5222],\n",
            "        [-0.9877,  1.0657]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3909, -1.1284],\n",
            "        [-1.3225, -0.3099],\n",
            "        [-0.2333, -1.5700],\n",
            "        [-2.1741, -0.1207]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2637, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3299, -1.2172],\n",
            "        [-0.5914,  0.7309],\n",
            "        [ 0.2528, -0.0949],\n",
            "        [ 1.1870, -1.0980]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0754, -2.6225],\n",
            "        [-1.5585, -0.2363],\n",
            "        [-0.5343, -0.8820],\n",
            "        [-0.0969, -2.3819]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2357, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.3499, -1.2717],\n",
            "        [-0.6773,  0.8331],\n",
            "        [-0.1396,  0.2473],\n",
            "        [-0.3775,  0.5663]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0702, -2.6918],\n",
            "        [-1.7100, -0.1995],\n",
            "        [-0.9051, -0.5183],\n",
            "        [-1.2725, -0.3287]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2792, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1715,  1.3946],\n",
            "        [ 0.9159, -0.7919],\n",
            "        [-1.2468,  1.4299],\n",
            "        [ 0.4963, -0.3610]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6401, -0.0740],\n",
            "        [-0.1666, -1.8743],\n",
            "        [-2.7432, -0.0665],\n",
            "        [-0.3537, -1.2109]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1652, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4465,  0.6221],\n",
            "        [-0.9658,  1.0643],\n",
            "        [ 0.8010, -0.6371],\n",
            "        [-1.1418,  1.3774]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3639, -0.2953],\n",
            "        [-2.1536, -0.1234],\n",
            "        [-0.2130, -1.6511],\n",
            "        [-2.5967, -0.0774]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1773, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8953,  1.1095],\n",
            "        [-1.0790,  1.2011],\n",
            "        [-0.5373,  0.6491],\n",
            "        [ 0.0052,  0.1814]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1312, -0.1264],\n",
            "        [-2.3774, -0.0974],\n",
            "        [-1.4528, -0.2665],\n",
            "        [-0.7851, -0.6090]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.3188, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.2083,  0.4670],\n",
            "        [ 0.8891, -0.7677],\n",
            "        [ 1.0817, -0.9992],\n",
            "        [-0.9034,  1.0233]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0868, -0.4114],\n",
            "        [-0.1746, -1.8315],\n",
            "        [-0.1176, -2.1985],\n",
            "        [-2.0626, -0.1360]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.3787, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6830, -1.5059],\n",
            "        [ 0.6091, -0.3379],\n",
            "        [-0.3900,  0.5062],\n",
            "        [ 0.1081, -0.0201]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0404, -3.2293],\n",
            "        [-0.3278, -1.2748],\n",
            "        [-1.2385, -0.3422],\n",
            "        [-0.6311, -0.7593]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.3674, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.1346,  0.2333],\n",
            "        [ 1.1019, -0.9807],\n",
            "        [ 0.5705, -0.4306],\n",
            "        [-0.3153,  0.5183]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.8939, -0.5260],\n",
            "        [-0.1174, -2.2000],\n",
            "        [-0.3130, -1.3141],\n",
            "        [-1.1944, -0.3608]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.3293, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.4587,  0.7325],\n",
            "        [ 1.7806, -1.7226],\n",
            "        [ 1.3418, -1.1924],\n",
            "        [ 1.2631, -1.0546]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4565, -0.2653],\n",
            "        [-0.0297, -3.5328],\n",
            "        [-0.0763, -2.6105],\n",
            "        [-0.0939, -2.4116]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1163, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.7575, -0.5701],\n",
            "        [-0.7048,  0.9459],\n",
            "        [ 0.5473, -0.4294],\n",
            "        [-0.4519,  0.5570]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2352, -1.5627],\n",
            "        [-1.8263, -0.1756],\n",
            "        [-0.3196, -1.2963],\n",
            "        [-1.3198, -0.3109]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2603, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5082,  0.7276],\n",
            "        [-0.1173,  0.1935],\n",
            "        [ 0.7993, -0.6378],\n",
            "        [ 0.3765, -0.2879]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4909, -0.2551],\n",
            "        [-0.8606, -0.5498],\n",
            "        [-0.2132, -1.6503],\n",
            "        [-0.4151, -1.0795]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.3583, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2869323879480362\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.2324, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.2297, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.3382, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  5,  0,  0]), 1: array([ 5, 11,  0,  0])}\n",
            "valid loss : tensor(0.2052, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([12,  7,  0,  1]), 1: array([ 7, 12,  1,  0])}\n",
            "valid loss : tensor(0.4362, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13,  9,  0,  2]), 1: array([ 9, 13,  2,  0])}\n",
            "valid loss : tensor(0.4592, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 11,  0,  2]), 1: array([11, 15,  2,  0])}\n",
            "valid loss : tensor(0.3311, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 14,  0,  3]), 1: array([14, 15,  3,  0])}\n",
            "valid loss : tensor(0.2995, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 17,  0,  3]), 1: array([17, 16,  3,  0])}\n",
            "valid loss : tensor(0.1566, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 19,  0,  3]), 1: array([19, 18,  3,  0])}\n",
            "valid loss : tensor(0.1622, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 21,  0,  3]), 1: array([21, 20,  3,  0])}\n",
            "valid loss : tensor(0.1408, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 24,  0,  3]), 1: array([24, 21,  3,  0])}\n",
            "valid loss : tensor(0.1187, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 25,  0,  3]), 1: array([25, 24,  3,  0])}\n",
            "valid loss : tensor(0.1592, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 28,  0,  3]), 1: array([28, 25,  3,  0])}\n",
            "valid loss : tensor(0.1512, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 30,  0,  3]), 1: array([30, 27,  3,  0])}\n",
            "valid loss : tensor(0.1387, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.95, recall : 0.9, precision : 1.0\n",
            "class1 >>> accuracy : 0.95, recall : 1.0, precision : 0.9090909090909091\n",
            "Loss/valid : 0.237265711526076\n",
            "\n",
            "========= Epoch14 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9207,  1.1581],\n",
            "        [-1.0370,  1.2876],\n",
            "        [-0.7778,  0.9954],\n",
            "        [ 0.1831,  0.0853]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1966, -0.1179],\n",
            "        [-2.4179, -0.0933],\n",
            "        [-1.9300, -0.1568],\n",
            "        [-0.6454, -0.7432]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.2534, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.8544, -1.7936],\n",
            "        [ 1.2090, -1.0051],\n",
            "        [-0.8303,  1.0504],\n",
            "        [-1.0154,  1.1894]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0257, -3.6738],\n",
            "        [-0.1037, -2.3178],\n",
            "        [-2.0226, -0.1419],\n",
            "        [-2.3094, -0.1046]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0940, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0753,  1.2958],\n",
            "        [ 1.1030, -0.9934],\n",
            "        [-0.3871,  0.4675],\n",
            "        [-0.5321,  0.6434]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4604, -0.0893],\n",
            "        [-0.1159, -2.2124],\n",
            "        [-1.2091, -0.3545],\n",
            "        [-1.4445, -0.2690]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.2072, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5982, -0.5624],\n",
            "        [-1.0822,  1.3111],\n",
            "        [ 0.4996, -0.3248],\n",
            "        [ 0.9234, -0.7242]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2726, -1.4331],\n",
            "        [-2.4808, -0.0874],\n",
            "        [-0.3636, -1.1880],\n",
            "        [-0.1761, -1.8236]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2249, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3813, -1.2453],\n",
            "        [ 0.0485,  0.1418],\n",
            "        [-0.5417,  0.6554],\n",
            "        [ 1.1758, -0.9459]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0698, -2.6964],\n",
            "        [-0.7409, -0.6476],\n",
            "        [-1.4610, -0.2640],\n",
            "        [-0.1132, -2.2349]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2970, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.9689, -0.7691],\n",
            "        [-0.4964,  0.7516],\n",
            "        [-1.0392,  1.1374],\n",
            "        [-0.1573,  0.3892]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1620, -1.9000],\n",
            "        [-1.5004, -0.2524],\n",
            "        [-2.2841, -0.1074],\n",
            "        [-1.0032, -0.4568]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2446, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1732, -1.0088],\n",
            "        [-0.9274,  1.0690],\n",
            "        [ 0.2808, -0.0101],\n",
            "        [-1.2961,  1.5037]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1069, -2.2889],\n",
            "        [-2.1237, -0.1274],\n",
            "        [-0.5582, -0.8491],\n",
            "        [-2.8589, -0.0590]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.2129, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.5309,  0.6583],\n",
            "        [ 1.5797, -1.4368],\n",
            "        [ 1.1741, -1.0410],\n",
            "        [-0.0404,  0.3421]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4550, -0.2658],\n",
            "        [-0.0478, -3.0643],\n",
            "        [-0.1036, -2.3187],\n",
            "        [-0.9026, -0.5201]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2343, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4284,  0.7439],\n",
            "        [ 0.3526, -0.2203],\n",
            "        [-0.4197,  0.5237],\n",
            "        [-0.6053,  0.7428]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4421, -0.2698],\n",
            "        [-0.4472, -1.0201],\n",
            "        [-1.2722, -0.3288],\n",
            "        [-1.5790, -0.2309]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.6122, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.2616, -1.1080],\n",
            "        [ 0.2860, -0.1820],\n",
            "        [-0.2396,  0.3647],\n",
            "        [ 1.1537, -0.9646]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0894, -2.4590],\n",
            "        [-0.4863, -0.9543],\n",
            "        [-1.0403, -0.4360],\n",
            "        [-0.1135, -2.2319]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.3983, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.1956,  0.4188],\n",
            "        [-0.7160,  0.8508],\n",
            "        [-0.7871,  0.9524],\n",
            "        [ 1.0673, -0.9708]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0469, -0.4324],\n",
            "        [-1.7564, -0.1895],\n",
            "        [-1.9012, -0.1618],\n",
            "        [-0.1225, -2.1606]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.3802, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1585, -0.9779],\n",
            "        [ 0.1957,  0.0023],\n",
            "        [ 1.4625, -1.2903],\n",
            "        [-0.8356,  1.0589]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1116, -2.2480],\n",
            "        [-0.6011, -0.7945],\n",
            "        [-0.0618, -2.8146],\n",
            "        [-2.0345, -0.1401]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2770, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.4392, -0.2278],\n",
            "        [ 1.1524, -1.0065],\n",
            "        [-1.0822,  1.2002],\n",
            "        [-0.6548,  0.7711]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4143, -1.0812],\n",
            "        [-0.1093, -2.2681],\n",
            "        [-2.3796, -0.0972],\n",
            "        [-1.6413, -0.2153]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2090, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2853, -0.9322],\n",
            "        [-0.1912,  0.3224],\n",
            "        [ 0.5680, -0.4683],\n",
            "        [-0.5179,  0.8229]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1034, -2.3208],\n",
            "        [-0.9826, -0.4689],\n",
            "        [-0.3036, -1.3400],\n",
            "        [-1.5732, -0.2324]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2771, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.2616, -0.0861],\n",
            "        [ 0.0829,  0.0810],\n",
            "        [-1.3944,  1.5899],\n",
            "        [ 1.4809, -1.3800]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.5343, -0.8820],\n",
            "        [-0.6922, -0.6941],\n",
            "        [-3.0337, -0.0493],\n",
            "        [-0.0556, -2.9166]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.3329, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2836599588394165\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.3000, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.3267, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.1714, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 9, 0, 0]), 1: array([9, 7, 0, 0])}\n",
            "valid loss : tensor(0.3656, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 8, 11,  1,  0]), 1: array([11,  8,  0,  1])}\n",
            "valid loss : tensor(0.5174, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 13,  2,  0]), 1: array([13,  9,  0,  2])}\n",
            "valid loss : tensor(0.5052, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([10, 15,  3,  0]), 1: array([15, 10,  0,  3])}\n",
            "valid loss : tensor(0.4320, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([12, 16,  4,  0]), 1: array([16, 12,  0,  4])}\n",
            "valid loss : tensor(0.4154, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 16,  4,  0]), 1: array([16, 16,  0,  4])}\n",
            "valid loss : tensor(0.1100, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 17,  4,  0]), 1: array([17, 19,  0,  4])}\n",
            "valid loss : tensor(0.1588, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 18,  5,  0]), 1: array([18, 21,  0,  5])}\n",
            "valid loss : tensor(0.3896, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 18,  6,  0]), 1: array([18, 24,  0,  6])}\n",
            "valid loss : tensor(0.3668, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 21,  6,  0]), 1: array([21, 25,  0,  6])}\n",
            "valid loss : tensor(0.3627, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 23,  6,  0]), 1: array([23, 27,  0,  6])}\n",
            "valid loss : tensor(0.2179, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 24,  6,  0]), 1: array([24, 30,  0,  6])}\n",
            "valid loss : tensor(0.1098, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9, recall : 1.0, precision : 0.8333333333333334\n",
            "class1 >>> accuracy : 0.9, recall : 0.8, precision : 1.0\n",
            "Loss/valid : 0.31660868376493456\n",
            "\n",
            "========= Epoch15 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.0095,  0.1523],\n",
            "        [ 1.3509, -1.2241],\n",
            "        [ 0.4084, -0.1076],\n",
            "        [ 0.2199, -0.0931]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.7671, -0.6243],\n",
            "        [-0.0734, -2.6484],\n",
            "        [-0.4681, -0.9840],\n",
            "        [-0.5489, -0.8618]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.6359, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6725, -0.5082],\n",
            "        [-1.7142,  1.9398],\n",
            "        [ 0.1073,  0.1025],\n",
            "        [-1.6209,  1.7517]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2678, -1.4484],\n",
            "        [-3.6795, -0.0256],\n",
            "        [-0.6908, -0.6955],\n",
            "        [-3.4063, -0.0337]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2545, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.6047, -1.5142],\n",
            "        [ 0.6761, -0.5145],\n",
            "        [-0.1958,  0.4958],\n",
            "        [ 0.4677, -0.3174]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0433, -3.1622],\n",
            "        [-0.2655, -1.4561],\n",
            "        [-1.0976, -0.4060],\n",
            "        [-0.3758, -1.1608]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.4455, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7082, -1.5305],\n",
            "        [ 2.3427, -2.1530],\n",
            "        [-0.0820,  0.2620],\n",
            "        [ 0.2792, -0.0720]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0385, -3.2772],\n",
            "        [-0.0111, -4.5068],\n",
            "        [-0.8799, -0.5358],\n",
            "        [-0.5329, -0.8841]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.3674, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.5247,  0.8008],\n",
            "        [ 0.6625, -0.5174],\n",
            "        [ 1.7778, -1.5619],\n",
            "        [-0.4580,  0.6703]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5611, -0.2356],\n",
            "        [-0.2680, -1.4479],\n",
            "        [-0.0348, -3.3745],\n",
            "        [-1.4086, -0.2804]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2047, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.1519,  0.2266],\n",
            "        [-0.4424,  0.5503],\n",
            "        [ 1.6851, -1.5528],\n",
            "        [-0.8682,  0.9736]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.9002, -0.5217],\n",
            "        [-1.3080, -0.3152],\n",
            "        [-0.0385, -3.2764],\n",
            "        [-1.9890, -0.1471]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2556, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.2736,  0.5502],\n",
            "        [-0.8958,  1.0249],\n",
            "        [-1.2981,  1.5215],\n",
            "        [ 0.0943,  0.1691]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1876, -0.3638],\n",
            "        [-2.0574, -0.1367],\n",
            "        [-2.8775, -0.0579],\n",
            "        [-0.7312, -0.6565]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.5284, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5404,  0.6640],\n",
            "        [ 1.0198, -0.9364],\n",
            "        [ 0.0478,  0.0454],\n",
            "        [ 0.4079, -0.2328]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4667, -0.2623],\n",
            "        [-0.1323, -2.0884],\n",
            "        [-0.6919, -0.6944],\n",
            "        [-0.4232, -1.0640]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.3780, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.3412, -0.1072],\n",
            "        [-0.3814,  0.4820],\n",
            "        [-0.0324,  0.2174],\n",
            "        [ 2.4655, -2.4089]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4939, -0.9423],\n",
            "        [-1.2153, -0.3519],\n",
            "        [-0.8258, -0.5760],\n",
            "        [-0.0076, -4.8821]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.4695, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.1703,  2.3688],\n",
            "        [ 0.2267, -0.1945],\n",
            "        [ 0.4254, -0.2957],\n",
            "        [ 0.2622, -0.1538]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.5497, -0.0106],\n",
            "        [-0.5046, -0.9257],\n",
            "        [-0.3962, -1.1173],\n",
            "        [-0.5066, -0.9226]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.3545, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0612, -0.9383],\n",
            "        [ 0.5894, -0.4284],\n",
            "        [-1.1459,  1.3894],\n",
            "        [ 0.3227, -0.2342]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1270, -2.1264],\n",
            "        [-0.3085, -1.3263],\n",
            "        [-2.6116, -0.0763],\n",
            "        [-0.4530, -1.0099]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2412, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.2484,  0.3348],\n",
            "        [-0.2805,  0.4009],\n",
            "        [ 1.1011, -0.7735],\n",
            "        [-0.7330,  0.8340]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0267, -0.4435],\n",
            "        [-1.0908, -0.4094],\n",
            "        [-0.1427, -2.0173],\n",
            "        [-1.7565, -0.1895]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2963, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8945, -0.6880],\n",
            "        [-1.3306,  1.5532],\n",
            "        [ 1.0024, -0.8692],\n",
            "        [ 0.1336,  0.0311]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1869, -1.7694],\n",
            "        [-2.9383, -0.0544],\n",
            "        [-0.1431, -2.0148],\n",
            "        [-0.6432, -0.7457]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2569, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9606,  1.1340],\n",
            "        [-1.1788,  1.2706],\n",
            "        [-1.0137,  1.2267],\n",
            "        [-0.0723,  0.2827]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2107, -0.1161],\n",
            "        [-2.5322, -0.0828],\n",
            "        [-2.3416, -0.1011],\n",
            "        [-0.8863, -0.5313]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2966, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1855, -1.0133],\n",
            "        [ 1.1724, -0.9801],\n",
            "        [-0.3741,  0.6248],\n",
            "        [-0.2193,  0.3380]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1052, -2.3040],\n",
            "        [-0.1099, -2.2624],\n",
            "        [-1.3125, -0.3135],\n",
            "        [-1.0101, -0.4528]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.2454, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.3486847718556722\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.1945, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.1235, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.1343, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  5,  0,  0]), 1: array([ 5, 11,  0,  0])}\n",
            "valid loss : tensor(0.1257, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([12,  8,  0,  0]), 1: array([ 8, 12,  0,  0])}\n",
            "valid loss : tensor(0.2451, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 10,  0,  0]), 1: array([10, 14,  0,  0])}\n",
            "valid loss : tensor(0.2391, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 11,  0,  0]), 1: array([11, 17,  0,  0])}\n",
            "valid loss : tensor(0.1007, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 13,  0,  0]), 1: array([13, 19,  0,  0])}\n",
            "valid loss : tensor(0.2228, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 15,  0,  0]), 1: array([15, 21,  0,  0])}\n",
            "valid loss : tensor(0.2245, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 17,  0,  0]), 1: array([17, 23,  0,  0])}\n",
            "valid loss : tensor(0.1126, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 20,  0,  0]), 1: array([20, 24,  0,  0])}\n",
            "valid loss : tensor(0.2027, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 22,  0,  1]), 1: array([22, 25,  1,  0])}\n",
            "valid loss : tensor(0.2917, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 24,  0,  1]), 1: array([24, 27,  1,  0])}\n",
            "valid loss : tensor(0.3205, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([28, 27,  0,  1]), 1: array([27, 28,  1,  0])}\n",
            "valid loss : tensor(0.1537, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([29, 30,  0,  1]), 1: array([30, 29,  1,  0])}\n",
            "valid loss : tensor(0.1946, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "Loss/valid : 0.19240362495183944\n",
            "\n",
            "========= Epoch16 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6785, -0.5862],\n",
            "        [-0.3659,  0.4601],\n",
            "        [-0.9279,  1.1529],\n",
            "        [ 1.3858, -1.2481]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2487, -1.5134],\n",
            "        [-1.1891, -0.3631],\n",
            "        [-2.1985, -0.1176],\n",
            "        [-0.0693, -2.7032]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1997, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7699,  0.9959],\n",
            "        [ 1.2931, -1.2000],\n",
            "        [-0.8301,  0.9673],\n",
            "        [ 1.3386, -1.1986]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9237, -0.1579],\n",
            "        [-0.0794, -2.5726],\n",
            "        [-1.9508, -0.1533],\n",
            "        [-0.0761, -2.6133]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1167, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.4284,  1.6802],\n",
            "        [-0.4148,  0.7199],\n",
            "        [-1.4387,  1.6312],\n",
            "        [ 1.3683, -1.1531]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1523, -0.0437],\n",
            "        [-1.4135, -0.2788],\n",
            "        [-3.1153, -0.0454],\n",
            "        [-0.0773, -2.5987]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1113, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9818,  1.0905],\n",
            "        [ 1.1439, -1.0045],\n",
            "        [-1.7547,  1.9886],\n",
            "        [-0.8141,  0.9459]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1909, -0.1186],\n",
            "        [-0.1104, -2.2587],\n",
            "        [-3.7667, -0.0234],\n",
            "        [-1.9187, -0.1588]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1028, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6823,  1.9976],\n",
            "        [ 0.6099, -0.4280],\n",
            "        [ 0.4505, -0.2665],\n",
            "        [-0.8895,  1.1227]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7048, -0.0249],\n",
            "        [-0.3032, -1.3411],\n",
            "        [-0.3976, -1.1145],\n",
            "        [-2.1377, -0.1255]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.7159, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.4632, -1.2814],\n",
            "        [ 2.1290, -1.9034],\n",
            "        [-0.2023,  0.3254],\n",
            "        [-0.2026,  0.3248]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0623, -2.8069],\n",
            "        [-0.0176, -4.0500],\n",
            "        [-0.9914, -0.4637],\n",
            "        [-0.9912, -0.4638]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2518, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.6615,  0.8677],\n",
            "        [ 1.1164, -0.9652],\n",
            "        [-0.8852,  0.9844],\n",
            "        [ 0.4892, -0.1838]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.7254, -0.1961],\n",
            "        [-0.1175, -2.1992],\n",
            "        [-2.0130, -0.1434],\n",
            "        [-0.4122, -1.0852]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.2173, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.1388, -0.9326],\n",
            "        [-0.1139,  0.2310],\n",
            "        [ 2.2125, -2.0125],\n",
            "        [ 0.9979, -0.8299]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1187, -2.1900],\n",
            "        [-0.8804, -0.5355],\n",
            "        [-0.0145, -4.2395],\n",
            "        [-0.1491, -1.9769]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2045, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9506, -1.8550],\n",
            "        [-0.1528,  0.2377],\n",
            "        [ 1.7881, -1.6227],\n",
            "        [ 2.0199, -1.9569]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0220, -3.8276],\n",
            "        [-0.9073, -0.5168],\n",
            "        [-0.0325, -3.4433],\n",
            "        [-0.0186, -3.9954]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1475, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7913,  0.9286],\n",
            "        [ 0.5702, -0.4098],\n",
            "        [-0.3779,  0.4947],\n",
            "        [ 1.0533, -1.0176]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8847, -0.1647],\n",
            "        [-0.3187, -1.2987],\n",
            "        [-1.2217, -0.3492],\n",
            "        [-0.1187, -2.1896]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.2378, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.5916, -0.3001],\n",
            "        [ 1.5442, -1.4258],\n",
            "        [ 0.8576, -0.7247],\n",
            "        [ 1.0754, -0.7979]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3436, -1.2352],\n",
            "        [-0.0500, -3.0201],\n",
            "        [-0.1869, -1.7692],\n",
            "        [-0.1429, -2.0162]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1808, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6762, -1.4911],\n",
            "        [ 0.3275, -0.1251],\n",
            "        [ 0.4234, -0.1816],\n",
            "        [ 0.0247,  0.0929]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0413, -3.2085],\n",
            "        [-0.4922, -0.9449],\n",
            "        [-0.4357, -1.0407],\n",
            "        [-0.7278, -0.6596]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.6716, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6751,  1.8636],\n",
            "        [-1.4512,  1.6851],\n",
            "        [-1.0001,  1.1380],\n",
            "        [-1.7877,  2.0135]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5673, -0.0286],\n",
            "        [-3.1788, -0.0425],\n",
            "        [-2.2495, -0.1114],\n",
            "        [-3.8232, -0.0221]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0512, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8020,  2.0642],\n",
            "        [-1.9334,  2.1074],\n",
            "        [-0.6499,  0.8479],\n",
            "        [ 0.6213, -0.4825]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.8869, -0.0207],\n",
            "        [-4.0582, -0.0174],\n",
            "        [-1.6996, -0.2018],\n",
            "        [-0.2864, -1.3902]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.5060, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2638,  1.3721],\n",
            "        [-1.6286,  1.8865],\n",
            "        [ 1.1601, -0.8194],\n",
            "        [ 0.0546,  0.1433]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7051, -0.0692],\n",
            "        [-3.5443, -0.0293],\n",
            "        [-0.1294, -2.1089],\n",
            "        [-0.7385, -0.6498]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.2416, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2637654088437557\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0816, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 1, 0, 0]), 1: array([1, 7, 0, 0])}\n",
            "valid loss : tensor(0.0615, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([10,  2,  0,  0]), 1: array([ 2, 10,  0,  0])}\n",
            "valid loss : tensor(0.2285, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11,  5,  0,  0]), 1: array([ 5, 11,  0,  0])}\n",
            "valid loss : tensor(0.1797, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([14,  6,  0,  0]), 1: array([ 6, 14,  0,  0])}\n",
            "valid loss : tensor(0.1699, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17,  7,  0,  0]), 1: array([ 7, 17,  0,  0])}\n",
            "valid loss : tensor(0.1178, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19,  9,  0,  0]), 1: array([ 9, 19,  0,  0])}\n",
            "valid loss : tensor(0.1936, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 12,  0,  0]), 1: array([12, 20,  0,  0])}\n",
            "valid loss : tensor(0.3866, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 14,  0,  0]), 1: array([14, 22,  0,  0])}\n",
            "valid loss : tensor(0.1405, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 17,  0,  0]), 1: array([17, 23,  0,  0])}\n",
            "valid loss : tensor(0.1916, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 20,  0,  0]), 1: array([20, 24,  0,  0])}\n",
            "valid loss : tensor(0.1693, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 21,  0,  0]), 1: array([21, 27,  0,  0])}\n",
            "valid loss : tensor(0.0822, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([29, 23,  0,  0]), 1: array([23, 29,  0,  0])}\n",
            "valid loss : tensor(0.2742, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 26,  0,  0]), 1: array([26, 30,  0,  0])}\n",
            "valid loss : tensor(0.1924, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.3286, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.18653736611207325\n",
            "\n",
            "========= Epoch17 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9989,  1.1184],\n",
            "        [-1.0851,  1.1779],\n",
            "        [ 0.8116, -0.6645],\n",
            "        [-1.0246,  1.2085]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2309, -0.1137],\n",
            "        [-2.3620, -0.0990],\n",
            "        [-0.2058, -1.6819],\n",
            "        [-2.3349, -0.1018]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1301, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.4183, -1.1921],\n",
            "        [ 1.0676, -1.0364],\n",
            "        [ 1.0071, -0.8080],\n",
            "        [ 1.5353, -1.3985]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0709, -2.6813],\n",
            "        [-0.1151, -2.2191],\n",
            "        [-0.1508, -1.9660],\n",
            "        [-0.0518, -2.9856]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0972, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4278,  0.6504],\n",
            "        [-0.2147,  0.3257],\n",
            "        [-0.8376,  1.0552],\n",
            "        [-0.2358,  0.3351]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3710, -0.2928],\n",
            "        [-0.9994, -0.4590],\n",
            "        [-2.0331, -0.1403],\n",
            "        [-1.0188, -0.4479]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.3350, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4177,  1.6258],\n",
            "        [-1.3547,  1.5222],\n",
            "        [ 1.0513, -0.9571],\n",
            "        [-1.7128,  1.9406]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.0900, -0.0466],\n",
            "        [-2.9317, -0.0548],\n",
            "        [-0.1259, -2.1343],\n",
            "        [-3.6789, -0.0256]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0632, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.7887,  0.9173],\n",
            "        [ 1.0648, -0.9499],\n",
            "        [-0.9063,  1.0072],\n",
            "        [-0.3682,  0.6824]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8729, -0.1669],\n",
            "        [-0.1252, -2.1399],\n",
            "        [-2.0511, -0.1376],\n",
            "        [-1.3505, -0.2999]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.4451, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.3144,  0.5095],\n",
            "        [-0.6692,  0.8508],\n",
            "        [-0.6246,  0.7504],\n",
            "        [-0.3701,  0.6258]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1877, -0.3637],\n",
            "        [-1.7178, -0.1978],\n",
            "        [-1.6004, -0.2254],\n",
            "        [-1.3103, -0.3144]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2753, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.6086,  0.8358],\n",
            "        [ 0.7283, -0.5520],\n",
            "        [ 0.8902, -0.5349],\n",
            "        [ 0.2294, -0.1310]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6562, -0.2118],\n",
            "        [-0.2453, -1.5256],\n",
            "        [-0.2155, -1.6406],\n",
            "        [-0.5291, -0.8895]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.6615, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.3214, -2.1968],\n",
            "        [ 1.6694, -1.5488],\n",
            "        [ 0.2543, -0.0378],\n",
            "        [ 2.2521, -2.0948]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0108, -4.5290],\n",
            "        [-0.0392, -3.2574],\n",
            "        [-0.5577, -0.8498],\n",
            "        [-0.0129, -4.3598]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2282, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.2404,  0.3472],\n",
            "        [-0.1688,  0.2785],\n",
            "        [ 2.1890, -1.9977],\n",
            "        [-0.0322,  0.2707]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0295, -0.4419],\n",
            "        [-0.9416, -0.4943],\n",
            "        [-0.0151, -4.2018],\n",
            "        [-0.8560, -0.5531]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.3761, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.0112,  0.1747],\n",
            "        [-1.0190,  1.1529],\n",
            "        [-1.5073,  1.7346],\n",
            "        [ 0.1006,  0.0985]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.7905, -0.6045],\n",
            "        [-2.2799, -0.1079],\n",
            "        [-3.2803, -0.0383],\n",
            "        [-0.6921, -0.6942]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.4072, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5344, -1.3734],\n",
            "        [-0.9028,  1.0028],\n",
            "        [ 1.1535, -0.9842],\n",
            "        [ 1.4602, -1.3366]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0532, -2.9610],\n",
            "        [-2.0443, -0.1387],\n",
            "        [-0.1115, -2.2492],\n",
            "        [-0.0592, -2.8560]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0906, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.4403, -0.2606],\n",
            "        [ 0.9062, -0.6392],\n",
            "        [-0.2902,  0.5107],\n",
            "        [ 2.0521, -1.9954]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4029, -1.1038],\n",
            "        [-0.1933, -1.7387],\n",
            "        [-1.1717, -0.3708],\n",
            "        [-0.0173, -4.0648]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2461, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2684, -1.1210],\n",
            "        [-0.0450,  0.3319],\n",
            "        [ 1.3036, -1.0481],\n",
            "        [-0.6175,  0.8687]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0877, -2.4772],\n",
            "        [-0.8993, -0.5223],\n",
            "        [-0.0909, -2.4426],\n",
            "        [-1.6902, -0.2039]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2262, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7687, -1.6805],\n",
            "        [-0.1830,  0.4779],\n",
            "        [-0.8554,  0.9783],\n",
            "        [ 1.0305, -0.8501]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0313, -3.4805],\n",
            "        [-1.0772, -0.4163],\n",
            "        [-1.9820, -0.1483],\n",
            "        [-0.1419, -2.0226]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1845, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0512, -0.9138],\n",
            "        [ 1.2216, -1.0092],\n",
            "        [-0.9209,  1.0015],\n",
            "        [-0.6538,  0.7841]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1312, -2.0961],\n",
            "        [-0.1021, -2.3328],\n",
            "        [-2.0589, -0.1365],\n",
            "        [-1.6509, -0.2130]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1457, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.260797821978728\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.2190, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 2, 0, 1]), 1: array([2, 5, 1, 0])}\n",
            "valid loss : tensor(0.3722, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 3, 0, 1]), 1: array([3, 8, 1, 0])}\n",
            "valid loss : tensor(0.1610, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11,  4,  0,  1]), 1: array([ 4, 11,  1,  0])}\n",
            "valid loss : tensor(0.2391, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13,  6,  0,  1]), 1: array([ 6, 13,  1,  0])}\n",
            "valid loss : tensor(0.1963, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 10,  0,  1]), 1: array([10, 13,  1,  0])}\n",
            "valid loss : tensor(0.0833, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 12,  0,  1]), 1: array([12, 15,  1,  0])}\n",
            "valid loss : tensor(0.1674, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 14,  0,  1]), 1: array([14, 17,  1,  0])}\n",
            "valid loss : tensor(0.0858, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 15,  0,  2]), 1: array([15, 19,  2,  0])}\n",
            "valid loss : tensor(0.4042, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 16,  0,  3]), 1: array([16, 21,  3,  0])}\n",
            "valid loss : tensor(0.4550, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 18,  0,  4]), 1: array([18, 22,  4,  0])}\n",
            "valid loss : tensor(0.4767, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 20,  0,  4]), 1: array([20, 24,  4,  0])}\n",
            "valid loss : tensor(0.1230, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  4]), 1: array([23, 25,  4,  0])}\n",
            "valid loss : tensor(0.0578, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 27,  0,  4]), 1: array([27, 25,  4,  0])}\n",
            "valid loss : tensor(0.0712, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 30,  0,  4]), 1: array([30, 26,  4,  0])}\n",
            "valid loss : tensor(0.1307, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9333333333333333, recall : 0.8666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9333333333333333, recall : 1.0, precision : 0.8823529411764706\n",
            "Loss/valid : 0.21619083285331725\n",
            "\n",
            "========= Epoch18 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6036, -0.4229],\n",
            "        [-1.4360,  1.6199],\n",
            "        [ 1.0799, -0.8803],\n",
            "        [-0.7689,  0.8833]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3062, -1.3327],\n",
            "        [-3.1019, -0.0460],\n",
            "        [-0.1318, -2.0920],\n",
            "        [-1.8275, -0.1753]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1648, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.3049, -1.1914],\n",
            "        [ 1.1756, -1.0417],\n",
            "        [-1.4001,  1.6252],\n",
            "        [-1.3699,  1.4988]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0792, -2.5754],\n",
            "        [-0.1034, -2.3207],\n",
            "        [-3.0727, -0.0474],\n",
            "        [-2.9240, -0.0552]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0713, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3645,  1.5766],\n",
            "        [ 1.2243, -0.8673],\n",
            "        [ 1.4599, -1.3158],\n",
            "        [-1.4916,  1.6061]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9925, -0.0515],\n",
            "        [-0.1164, -2.2080],\n",
            "        [-0.0604, -2.8362],\n",
            "        [-3.1418, -0.0442]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0681, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1153,  1.2519],\n",
            "        [-1.4421,  1.6748],\n",
            "        [ 0.4549, -0.2956],\n",
            "        [ 0.1865,  0.1035]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4568, -0.0896],\n",
            "        [-3.1603, -0.0433],\n",
            "        [-0.3867, -1.1372],\n",
            "        [-0.6525, -0.7355]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2930, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.3829, -1.2268],\n",
            "        [ 1.1533, -0.8413],\n",
            "        [-0.1546,  0.2873],\n",
            "        [ 0.0448,  0.0520]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0710, -2.6807],\n",
            "        [-0.1276, -2.1221],\n",
            "        [-0.9383, -0.4964],\n",
            "        [-0.6967, -0.6896]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.3461, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.1374,  0.0912],\n",
            "        [ 0.8631, -0.6903],\n",
            "        [-0.7677,  1.0033],\n",
            "        [-1.1470,  1.3413]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.6703, -0.7165],\n",
            "        [-0.1919, -1.7452],\n",
            "        [-1.9281, -0.1571],\n",
            "        [-2.5681, -0.0798]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.2748, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5682, -1.2930],\n",
            "        [ 0.0070,  0.0771],\n",
            "        [ 1.5937, -1.3815],\n",
            "        [ 2.1934, -1.9724]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0556, -2.9168],\n",
            "        [-0.7288, -0.6587],\n",
            "        [-0.0498, -3.0250],\n",
            "        [-0.0154, -4.1811]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1949, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.3593,  0.6013],\n",
            "        [-0.5701,  0.6861],\n",
            "        [ 1.9367, -1.7991],\n",
            "        [-0.9657,  1.2381]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.2846, -0.3240],\n",
            "        [-1.5068, -0.2505],\n",
            "        [-0.0236, -3.7594],\n",
            "        [-2.3086, -0.1047]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1757, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9780,  1.1246],\n",
            "        [-0.6492,  0.7835],\n",
            "        [ 1.2486, -1.0994],\n",
            "        [ 0.6503, -0.5488]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2178, -0.1152],\n",
            "        [-1.6468, -0.2140],\n",
            "        [-0.0913, -2.4392],\n",
            "        [-0.2635, -1.4626]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1710, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0066, -0.8065],\n",
            "        [ 1.1349, -0.9487],\n",
            "        [-0.8424,  0.9546],\n",
            "        [ 0.4346, -0.2197]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1511, -1.9642],\n",
            "        [-0.1173, -2.2009],\n",
            "        [-1.9503, -0.1534],\n",
            "        [-0.4186, -1.0729]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.2101, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5867, -1.4148],\n",
            "        [-0.6370,  0.8436],\n",
            "        [ 2.2446, -2.1432],\n",
            "        [ 1.7049, -1.4565]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0485, -3.0499],\n",
            "        [-1.6856, -0.2050],\n",
            "        [-0.0124, -4.4002],\n",
            "        [-0.0415, -3.2029]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0768, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.6603,  0.8362],\n",
            "        [ 1.0295, -0.9939],\n",
            "        [-1.1999,  1.4633],\n",
            "        [-0.9346,  1.1794]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6986, -0.2020],\n",
            "        [-0.1242, -2.1476],\n",
            "        [-2.7306, -0.0674],\n",
            "        [-2.2280, -0.1140]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1269, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.8280, -1.7612],\n",
            "        [ 0.3840, -0.2452],\n",
            "        [-0.8570,  0.9837],\n",
            "        [-0.0978,  0.3070]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0272, -3.6165],\n",
            "        [-0.4273, -1.0564],\n",
            "        [-1.9879, -0.1473],\n",
            "        [-0.9159, -0.5111]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.3794, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1388,  1.2450],\n",
            "        [ 1.6256, -1.4414],\n",
            "        [-0.0947,  0.4157],\n",
            "        [ 0.3899, -0.0700]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4720, -0.0882],\n",
            "        [-0.0455, -3.1126],\n",
            "        [-0.9806, -0.4702],\n",
            "        [-0.4894, -0.9493]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.3883, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6905,  1.9330],\n",
            "        [-1.6675,  1.9388],\n",
            "        [ 0.7787, -0.6779],\n",
            "        [-2.0210,  2.1551]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.6498, -0.0263],\n",
            "        [-3.6331, -0.0268],\n",
            "        [-0.2095, -1.6661],\n",
            "        [-4.1913, -0.0152]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0695, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.20072059879700344\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.2594, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 2, 0, 0]), 1: array([2, 6, 0, 0])}\n",
            "valid loss : tensor(0.1824, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0636, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 7, 0, 1]), 1: array([7, 8, 1, 0])}\n",
            "valid loss : tensor(0.3144, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([10,  9,  0,  1]), 1: array([ 9, 10,  1,  0])}\n",
            "valid loss : tensor(0.2126, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 10,  0,  2]), 1: array([10, 12,  2,  0])}\n",
            "valid loss : tensor(0.6225, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 13,  0,  2]), 1: array([13, 13,  2,  0])}\n",
            "valid loss : tensor(0.1054, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 15,  0,  2]), 1: array([15, 15,  2,  0])}\n",
            "valid loss : tensor(0.3339, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 17,  0,  3]), 1: array([17, 16,  3,  0])}\n",
            "valid loss : tensor(0.6065, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 20,  0,  3]), 1: array([20, 17,  3,  0])}\n",
            "valid loss : tensor(0.0632, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 22,  0,  3]), 1: array([22, 19,  3,  0])}\n",
            "valid loss : tensor(0.1218, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  4]), 1: array([23, 21,  4,  0])}\n",
            "valid loss : tensor(0.5539, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 26,  0,  4]), 1: array([26, 22,  4,  0])}\n",
            "valid loss : tensor(0.1790, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 28,  0,  4]), 1: array([28, 24,  4,  0])}\n",
            "valid loss : tensor(0.1062, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 30,  0,  4]), 1: array([30, 26,  4,  0])}\n",
            "valid loss : tensor(0.1065, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9333333333333333, recall : 0.8666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9333333333333333, recall : 1.0, precision : 0.8823529411764706\n",
            "Loss/valid : 0.2554226885239283\n",
            "\n",
            "========= Epoch19 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.1466,  0.1406],\n",
            "        [-2.0429,  2.1653],\n",
            "        [ 1.4760, -1.3717],\n",
            "        [-1.2758,  1.4008]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.6902, -0.6961],\n",
            "        [-4.2230, -0.0148],\n",
            "        [-0.0564, -2.9040],\n",
            "        [-2.7431, -0.0665]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.2070, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9153, -1.8517],\n",
            "        [-1.1940,  1.3678],\n",
            "        [ 1.3217, -1.1847],\n",
            "        [ 1.3714, -1.1988]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0229, -3.7898],\n",
            "        [-2.6361, -0.0743],\n",
            "        [-0.0784, -2.5848],\n",
            "        [-0.0737, -2.6439]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0623, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.1857,  1.5061],\n",
            "        [-1.1014,  1.3478],\n",
            "        [-1.2991,  1.5426],\n",
            "        [-1.2489,  1.4421]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7573, -0.0656],\n",
            "        [-2.5320, -0.0828],\n",
            "        [-2.8984, -0.0567],\n",
            "        [-2.7566, -0.0656]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0677, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8874,  1.0308],\n",
            "        [ 1.2207, -0.8438],\n",
            "        [-1.8226,  2.0886],\n",
            "        [-0.2507,  0.5825]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0553, -0.1370],\n",
            "        [-0.1195, -2.1839],\n",
            "        [-3.9310, -0.0198],\n",
            "        [-1.1941, -0.3609]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.3676, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.9096, -0.7068],\n",
            "        [-0.0848,  0.2054],\n",
            "        [ 1.6631, -1.4452],\n",
            "        [-0.5681,  0.7130]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1812, -1.7975],\n",
            "        [-0.8487, -0.5586],\n",
            "        [-0.0437, -3.1520],\n",
            "        [-1.5262, -0.2451]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.2571, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9059, -1.7614],\n",
            "        [-0.6808,  0.9241],\n",
            "        [-0.5581,  0.8016],\n",
            "        [-0.9478,  1.1855]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0252, -3.6925],\n",
            "        [-1.7879, -0.1831],\n",
            "        [-1.5882, -0.2285],\n",
            "        [-2.2452, -0.1119]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1372, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.9529, -0.7859],\n",
            "        [ 1.4830, -1.3786],\n",
            "        [ 0.7845, -0.6154],\n",
            "        [-0.7812,  1.0304]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1619, -1.9006],\n",
            "        [-0.0556, -2.9172],\n",
            "        [-0.2204, -1.6204],\n",
            "        [-1.9630, -0.1513]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1473, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9668,  1.1945],\n",
            "        [-0.7466,  0.8710],\n",
            "        [-0.2716,  0.3883],\n",
            "        [ 1.8590, -1.7328]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2702, -0.1090],\n",
            "        [-1.7986, -0.1810],\n",
            "        [-1.0766, -0.4167],\n",
            "        [-0.0272, -3.6189]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1835, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2810,  1.5632],\n",
            "        [-0.5202,  0.7681],\n",
            "        [ 0.4107, -0.2192],\n",
            "        [ 1.3847, -1.2245]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9007, -0.0566],\n",
            "        [-1.5318, -0.2435],\n",
            "        [-0.4270, -1.0569],\n",
            "        [-0.0710, -2.6802]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.5216, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.0554,  0.0764],\n",
            "        [ 0.3602, -0.2239],\n",
            "        [ 0.4019, -0.1941],\n",
            "        [-0.1916,  0.3237]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.7037, -0.6827],\n",
            "        [-0.4432, -1.0272],\n",
            "        [-0.4389, -1.0349],\n",
            "        [-0.9836, -0.4683]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.8033, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.6446,  2.9265],\n",
            "        [-2.0565,  2.1655],\n",
            "        [-0.5578,  0.7742],\n",
            "        [-2.1020,  2.2493]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.5749e+00, -3.7990e-03],\n",
            "        [-4.2366e+00, -1.4562e-02],\n",
            "        [-1.5662e+00, -2.3424e-01],\n",
            "        [-4.3641e+00, -1.2808e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.3993, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3962, -0.3030],\n",
            "        [-1.0951,  1.1769],\n",
            "        [-0.9243,  1.0212],\n",
            "        [-1.9183,  2.1071]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4035, -1.1026],\n",
            "        [-2.3702, -0.0981],\n",
            "        [-2.0791, -0.1336],\n",
            "        [-4.0431, -0.0177]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1632, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8390, -0.8054],\n",
            "        [ 1.1122, -0.9119],\n",
            "        [-0.9834,  1.2904],\n",
            "        [ 0.8058, -0.6176]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1766, -1.8210],\n",
            "        [-0.1241, -2.1482],\n",
            "        [-2.3717, -0.0980],\n",
            "        [-0.2158, -1.6393]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1536, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8230, -0.6475],\n",
            "        [ 0.1871,  0.0972],\n",
            "        [ 1.3626, -1.2220],\n",
            "        [ 0.5160, -0.3889]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2069, -1.6773],\n",
            "        [-0.6492, -0.7391],\n",
            "        [-0.0727, -2.6573],\n",
            "        [-0.3397, -1.2446]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.3171, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.3673, -2.1514],\n",
            "        [ 1.7624, -1.5930],\n",
            "        [ 2.2175, -1.9864],\n",
            "        [ 1.5486, -1.3717]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0108, -4.5295],\n",
            "        [-0.0343, -3.3897],\n",
            "        [-0.0148, -4.2187],\n",
            "        [-0.0525, -2.9728]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0281, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.2544005005309979\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1805, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.1010, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.1815, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 6, 1, 0]), 1: array([6, 9, 0, 1])}\n",
            "valid loss : tensor(0.4126, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11,  8,  1,  0]), 1: array([ 8, 11,  0,  1])}\n",
            "valid loss : tensor(0.2984, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 10,  1,  0]), 1: array([10, 13,  0,  1])}\n",
            "valid loss : tensor(0.2688, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 11,  2,  0]), 1: array([11, 15,  0,  2])}\n",
            "valid loss : tensor(0.3359, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 11,  2,  0]), 1: array([11, 19,  0,  2])}\n",
            "valid loss : tensor(0.0292, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 12,  2,  0]), 1: array([12, 22,  0,  2])}\n",
            "valid loss : tensor(0.1341, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 15,  2,  0]), 1: array([15, 23,  0,  2])}\n",
            "valid loss : tensor(0.2865, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 17,  3,  0]), 1: array([17, 24,  0,  3])}\n",
            "valid loss : tensor(0.4866, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 20,  3,  0]), 1: array([20, 25,  0,  3])}\n",
            "valid loss : tensor(0.3963, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 23,  3,  0]), 1: array([23, 26,  0,  3])}\n",
            "valid loss : tensor(0.3723, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([28, 24,  4,  0]), 1: array([24, 28,  0,  4])}\n",
            "valid loss : tensor(0.2685, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 25,  5,  0]), 1: array([25, 30,  0,  5])}\n",
            "valid loss : tensor(0.3202, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9166666666666666, recall : 1.0, precision : 0.8571428571428571\n",
            "class1 >>> accuracy : 0.9166666666666666, recall : 0.8333333333333334, precision : 1.0\n",
            "Loss/valid : 0.2714898124337196\n",
            "\n",
            "========= Epoch20 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.0189,  0.1190],\n",
            "        [ 1.5834, -1.2774],\n",
            "        [ 0.9249, -0.7070],\n",
            "        [ 0.0629,  0.1660]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.7645, -0.6266],\n",
            "        [-0.0556, -2.9164],\n",
            "        [-0.1786, -1.8105],\n",
            "        [-0.7460, -0.6429]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.3759, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3209, -0.1314],\n",
            "        [ 1.7239, -1.6318],\n",
            "        [-0.3014,  0.4144],\n",
            "        [-0.5134,  0.6255]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4923, -0.9447],\n",
            "        [-0.0343, -3.3900],\n",
            "        [-1.1138, -0.3979],\n",
            "        [-1.4167, -0.2777]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.3006, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7095,  0.9383],\n",
            "        [ 1.6642, -1.5225],\n",
            "        [ 1.2415, -0.9699],\n",
            "        [-0.7695,  0.9069]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8239, -0.1760],\n",
            "        [-0.0405, -3.2271],\n",
            "        [-0.1040, -2.3153],\n",
            "        [-1.8478, -0.1715]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1230, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0096,  1.2643],\n",
            "        [-1.1568,  1.2858],\n",
            "        [ 1.0925, -0.8849],\n",
            "        [-1.0721,  1.3431]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3719, -0.0979],\n",
            "        [-2.5260, -0.0834],\n",
            "        [-0.1296, -2.1070],\n",
            "        [-2.5007, -0.0856]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0991, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5535,  1.7493],\n",
            "        [ 0.7563, -0.5814],\n",
            "        [ 1.5145, -1.3674],\n",
            "        [ 1.2873, -1.1510]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3389, -0.0361],\n",
            "        [-0.2331, -1.5707],\n",
            "        [-0.0545, -2.9364],\n",
            "        [-0.0837, -2.5220]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1019, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9529, -0.8218],\n",
            "        [-0.9984,  1.2142],\n",
            "        [ 1.0092, -0.8492],\n",
            "        [ 1.6217, -1.3754]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1566, -1.9313],\n",
            "        [-2.3165, -0.1038],\n",
            "        [-0.1449, -2.0033],\n",
            "        [-0.0487, -3.0458]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1135, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9743, -0.6799],\n",
            "        [-0.4224,  0.5540],\n",
            "        [-0.6067,  0.8743],\n",
            "        [ 1.1712, -1.0127]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1750, -1.8292],\n",
            "        [-1.2961, -0.3197],\n",
            "        [-1.6859, -0.2049],\n",
            "        [-0.1067, -2.2906]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.2016, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7230, -1.5495],\n",
            "        [-0.7549,  1.0755],\n",
            "        [-0.3398,  0.4437],\n",
            "        [ 1.9118, -1.5388]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0372, -3.3097],\n",
            "        [-1.9791, -0.1487],\n",
            "        [-1.1597, -0.3763],\n",
            "        [-0.0312, -3.4818]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1484, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.6760,  0.8186],\n",
            "        [-1.3871,  1.5832],\n",
            "        [ 1.5528, -1.3220],\n",
            "        [-1.2680,  1.4774]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6971, -0.2024],\n",
            "        [-3.0203, -0.0500],\n",
            "        [-0.0549, -2.9297],\n",
            "        [-2.8076, -0.0622]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0924, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.8622, -1.7963],\n",
            "        [ 0.6723, -0.4778],\n",
            "        [-1.0556,  1.1455],\n",
            "        [ 1.2454, -1.1210]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0254, -3.6840],\n",
            "        [-0.2751, -1.4252],\n",
            "        [-2.3061, -0.1050],\n",
            "        [-0.0897, -2.4561]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1238, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1707, -0.9804],\n",
            "        [-1.6639,  1.9122],\n",
            "        [-1.2873,  1.5292],\n",
            "        [-1.1606,  1.4111]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1101, -2.2612],\n",
            "        [-3.6037, -0.0276],\n",
            "        [-2.8746, -0.0581],\n",
            "        [-2.6453, -0.0736]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0673, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0340, -0.8248],\n",
            "        [-1.3880,  1.5717],\n",
            "        [-1.1332,  1.2831],\n",
            "        [ 1.6106, -1.4622]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1449, -2.0036],\n",
            "        [-3.0103, -0.0505],\n",
            "        [-2.5018, -0.0855],\n",
            "        [-0.0453, -3.1181]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0815, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.8832, -1.7741],\n",
            "        [ 0.7583, -0.5254],\n",
            "        [-1.7820,  1.9207],\n",
            "        [ 1.0147, -0.9763]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0255, -3.6828],\n",
            "        [-0.2445, -1.5282],\n",
            "        [-3.7270, -0.0244],\n",
            "        [-0.1280, -2.1190]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1056, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9344,  1.0589],\n",
            "        [-1.5528,  1.6782],\n",
            "        [ 1.2997, -1.1092],\n",
            "        [-1.6103,  1.7210]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1210, -0.1277],\n",
            "        [-3.2697, -0.0388],\n",
            "        [-0.0861, -2.4950],\n",
            "        [-3.3664, -0.0351]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0719, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.8660, -0.7599],\n",
            "        [-0.5275,  0.8630],\n",
            "        [-1.2641,  1.5206],\n",
            "        [ 1.6362, -1.4142]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1796, -1.8055],\n",
            "        [-1.6128, -0.2223],\n",
            "        [-2.8446, -0.0599],\n",
            "        [-0.0463, -3.0966]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1270, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.14223472227652867\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 1, 0, 1]), 1: array([1, 2, 1, 0])}\n",
            "valid loss : tensor(0.4588, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 2, 0, 2]), 1: array([2, 4, 2, 0])}\n",
            "valid loss : tensor(0.3528, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 5, 0, 2]), 1: array([5, 5, 2, 0])}\n",
            "valid loss : tensor(0.1684, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 7, 0, 2]), 1: array([7, 7, 2, 0])}\n",
            "valid loss : tensor(0.0763, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 8, 10,  0,  2]), 1: array([10,  8,  2,  0])}\n",
            "valid loss : tensor(0.1059, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 13,  0,  2]), 1: array([13,  9,  2,  0])}\n",
            "valid loss : tensor(0.0671, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([12, 14,  0,  2]), 1: array([14, 12,  2,  0])}\n",
            "valid loss : tensor(0.0487, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 16,  0,  2]), 1: array([16, 14,  2,  0])}\n",
            "valid loss : tensor(0.0826, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 18,  0,  2]), 1: array([18, 16,  2,  0])}\n",
            "valid loss : tensor(0.1121, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 21,  0,  3]), 1: array([21, 16,  3,  0])}\n",
            "valid loss : tensor(0.2230, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 22,  0,  3]), 1: array([22, 19,  3,  0])}\n",
            "valid loss : tensor(0.0435, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 24,  0,  3]), 1: array([24, 21,  3,  0])}\n",
            "valid loss : tensor(0.0676, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 27,  0,  3]), 1: array([27, 22,  3,  0])}\n",
            "valid loss : tensor(0.0858, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 28,  0,  3]), 1: array([28, 25,  3,  0])}\n",
            "valid loss : tensor(0.0922, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 30,  0,  3]), 1: array([30, 27,  3,  0])}\n",
            "valid loss : tensor(0.1031, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.95, recall : 0.9, precision : 1.0\n",
            "class1 >>> accuracy : 0.95, recall : 1.0, precision : 0.9090909090909091\n",
            "Loss/valid : 0.13919427394866943\n",
            "\n",
            "========= Epoch21 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7286, -1.4882],\n",
            "        [ 1.5891, -1.1956],\n",
            "        [-0.7545,  0.8845],\n",
            "        [-1.1009,  1.2596]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0393, -3.2562],\n",
            "        [-0.0599, -2.8446],\n",
            "        [-1.8165, -0.1774],\n",
            "        [-2.4507, -0.0902]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0917, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8617,  0.9801],\n",
            "        [ 1.4372, -1.3047],\n",
            "        [-1.8117,  1.9282],\n",
            "        [ 1.1242, -0.9064]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9890, -0.1471],\n",
            "        [-0.0625, -2.8044],\n",
            "        [-3.7634, -0.0235],\n",
            "        [-0.1233, -2.1540]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0891, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2559, -0.9809],\n",
            "        [ 1.4146, -1.3045],\n",
            "        [-0.0089,  0.3628],\n",
            "        [-1.8843,  2.0963]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1015, -2.3383],\n",
            "        [-0.0639, -2.7829],\n",
            "        [-0.8961, -0.5245],\n",
            "        [-3.9991, -0.0185]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.2700, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.0665, -1.9211],\n",
            "        [ 1.7548, -1.4393],\n",
            "        [-0.0642,  0.3258],\n",
            "        [-0.1035,  0.2529]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0184, -4.0060],\n",
            "        [-0.0402, -3.2343],\n",
            "        [-0.9070, -0.5171],\n",
            "        [-0.8871, -0.5308]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.2766, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7513, -1.5883],\n",
            "        [-0.9743,  1.2321],\n",
            "        [-1.3154,  1.5739],\n",
            "        [ 1.5069, -1.3091]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0348, -3.3744],\n",
            "        [-2.3109, -0.1044],\n",
            "        [-2.9434, -0.0541],\n",
            "        [-0.0581, -2.8741]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0629, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.1082, -0.8017],\n",
            "        [-1.5838,  1.7167],\n",
            "        [ 1.1212, -0.8790],\n",
            "        [ 1.1862, -1.1460]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1381, -2.0480],\n",
            "        [-3.3367, -0.0362],\n",
            "        [-0.1269, -2.1272],\n",
            "        [-0.0926, -2.4249]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0985, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7513,  0.9406],\n",
            "        [-0.6737,  0.8084],\n",
            "        [ 0.2136,  0.0399],\n",
            "        [-0.7709,  0.9941]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8609, -0.1690],\n",
            "        [-1.6869, -0.2047],\n",
            "        [-0.6101, -0.7838],\n",
            "        [-1.9230, -0.1580]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.2855, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4887,  0.6138],\n",
            "        [-0.9841,  1.1291],\n",
            "        [-0.5761,  0.7365],\n",
            "        [-0.6448,  0.9092]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3892, -0.2867],\n",
            "        [-2.2273, -0.1141],\n",
            "        [-1.5510, -0.2383],\n",
            "        [-1.7458, -0.1918]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.2077, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6263, -0.4472],\n",
            "        [-1.7613,  2.0076],\n",
            "        [ 0.6043, -0.3659],\n",
            "        [-1.6652,  1.8825]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2940, -1.3676],\n",
            "        [-3.7917, -0.0228],\n",
            "        [-0.3214, -1.2915],\n",
            "        [-3.5762, -0.0284]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1666, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.8160, -1.6410],\n",
            "        [ 2.2701, -2.1994],\n",
            "        [-1.2419,  1.5125],\n",
            "        [ 1.4394, -1.2368]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0310, -3.4881],\n",
            "        [-0.0114, -4.4809],\n",
            "        [-2.8161, -0.0617],\n",
            "        [-0.0666, -2.7427]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0427, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8985,  1.1946],\n",
            "        [ 2.2141, -2.0939],\n",
            "        [ 1.2365, -1.0289],\n",
            "        [ 1.0500, -0.8681]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2094, -0.1163],\n",
            "        [-0.0134, -4.3213],\n",
            "        [-0.0988, -2.3641],\n",
            "        [-0.1371, -2.0551]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0914, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9750,  1.2670],\n",
            "        [-0.5080,  0.6064],\n",
            "        [-0.2922,  0.6474],\n",
            "        [-0.7291,  1.0883]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3430, -0.1010],\n",
            "        [-1.3981, -0.2838],\n",
            "        [-1.2695, -0.3299],\n",
            "        [-1.9679, -0.1505]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.2163, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7144, -0.5167],\n",
            "        [-1.7475,  1.8864],\n",
            "        [ 0.0156,  0.1964],\n",
            "        [ 0.7934, -0.5550]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2562, -1.4872],\n",
            "        [-3.6600, -0.0261],\n",
            "        [-0.7876, -0.6068],\n",
            "        [-0.2308, -1.5792]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.3252, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8209,  1.1264],\n",
            "        [ 1.1883, -1.0778],\n",
            "        [ 1.1760, -1.0293],\n",
            "        [-0.9648,  1.1099]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0807, -0.1333],\n",
            "        [-0.0987, -2.3648],\n",
            "        [-0.1046, -2.3099],\n",
            "        [-2.1930, -0.1183]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1137, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9341,  1.1918],\n",
            "        [ 0.3821, -0.1533],\n",
            "        [-0.6376,  0.7945],\n",
            "        [ 2.1783, -2.0182]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.2386, -0.1127],\n",
            "        [-0.4609, -0.9962],\n",
            "        [-1.6463, -0.2141],\n",
            "        [-0.0149, -4.2115]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.2007, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.16922959784666697\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1198, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.0513, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 6, 0, 0]), 1: array([6, 6, 0, 0])}\n",
            "valid loss : tensor(0.1774, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.0766, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.1370, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([11, 13,  0,  0]), 1: array([13, 11,  0,  0])}\n",
            "valid loss : tensor(0.1019, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 15,  0,  0]), 1: array([15, 13,  0,  0])}\n",
            "valid loss : tensor(0.1143, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 16,  0,  0]), 1: array([16, 16,  0,  0])}\n",
            "valid loss : tensor(0.1041, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 19,  0,  0]), 1: array([19, 17,  0,  0])}\n",
            "valid loss : tensor(0.1314, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 20,  0,  0]), 1: array([20, 20,  0,  0])}\n",
            "valid loss : tensor(0.0593, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 22,  0,  0]), 1: array([22, 22,  0,  0])}\n",
            "valid loss : tensor(0.1322, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.1962, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 25,  0,  0]), 1: array([25, 27,  0,  0])}\n",
            "valid loss : tensor(0.0924, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 26,  0,  0]), 1: array([26, 30,  0,  0])}\n",
            "valid loss : tensor(0.0726, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1674, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.11559626087546349\n",
            "\n",
            "========= Epoch22 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6193, -1.4142],\n",
            "        [-1.1760,  1.3232],\n",
            "        [-0.9067,  1.0756],\n",
            "        [-1.0354,  1.3466]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0470, -3.0806],\n",
            "        [-2.5781, -0.0790],\n",
            "        [-2.1113, -0.1291],\n",
            "        [-2.4703, -0.0883]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0858, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.8312, -1.6497],\n",
            "        [-1.4006,  1.5522],\n",
            "        [ 1.0960, -0.8557],\n",
            "        [ 1.1686, -0.9585]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0303, -3.5112],\n",
            "        [-3.0036, -0.0509],\n",
            "        [-0.1328, -2.0845],\n",
            "        [-0.1126, -2.2397]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0817, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.1970, -2.0327],\n",
            "        [-0.8237,  1.1251],\n",
            "        [-1.2110,  1.4845],\n",
            "        [ 1.7501, -1.5946]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0145, -4.2442],\n",
            "        [-2.0819, -0.1332],\n",
            "        [-2.7608, -0.0653],\n",
            "        [-0.0347, -3.3794]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0619, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2881, -1.2451],\n",
            "        [ 1.1681, -1.0508],\n",
            "        [-1.1210,  1.4881],\n",
            "        [-0.8069,  0.9708]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0764, -2.6097],\n",
            "        [-0.1032, -2.3221],\n",
            "        [-2.6801, -0.0710],\n",
            "        [-1.9338, -0.1562]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1017, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.3286,  1.6124],\n",
            "        [ 2.1429, -1.8854],\n",
            "        [-1.4928,  1.7926],\n",
            "        [-1.3712,  1.5731]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.9925, -0.0515],\n",
            "        [-0.0176, -4.0460],\n",
            "        [-3.3221, -0.0367],\n",
            "        [-2.9956, -0.0513]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0393, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.8591, -0.5964],\n",
            "        [ 1.3181, -1.0201],\n",
            "        [ 1.5524, -1.3739],\n",
            "        [-1.9328,  2.0603]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2097, -1.6652],\n",
            "        [-0.0921, -2.4304],\n",
            "        [-0.0522, -2.9785],\n",
            "        [-4.0113, -0.0183]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0931, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7968, -1.6774],\n",
            "        [ 0.6538, -0.2784],\n",
            "        [-0.7886,  0.9223],\n",
            "        [ 2.5566, -2.4836]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0305, -3.5047],\n",
            "        [-0.3320, -1.2641],\n",
            "        [-1.8770, -0.1661],\n",
            "        [-0.0065, -5.0466]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1338, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9246, -1.7112],\n",
            "        [ 1.6953, -1.4582],\n",
            "        [-0.6559,  0.9328],\n",
            "        [-0.6613,  0.9391]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0260, -3.6618],\n",
            "        [-0.0418, -3.1953],\n",
            "        [-1.7745, -0.1858],\n",
            "        [-1.7842, -0.1838]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1094, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.6576,  0.8224],\n",
            "        [-1.5669,  1.7920],\n",
            "        [ 0.0251,  0.2477],\n",
            "        [-0.7727,  0.9466]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6851, -0.2051],\n",
            "        [-3.3931, -0.0342],\n",
            "        [-0.8106, -0.5880],\n",
            "        [-1.8841, -0.1648]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.3037, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.1637, -1.9512],\n",
            "        [ 2.8144, -2.6859],\n",
            "        [ 1.4840, -1.1386],\n",
            "        [-0.8092,  1.0944]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6196e-02, -4.1311e+00],\n",
            "        [-4.0773e-03, -5.5043e+00],\n",
            "        [-7.0093e-02, -2.6928e+00],\n",
            "        [-2.0426e+00, -1.3891e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0573, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.3999,  0.6440],\n",
            "        [-1.1656,  1.3131],\n",
            "        [-0.6573,  0.9172],\n",
            "        [-0.1073,  0.2420]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3455, -0.3016],\n",
            "        [-2.5593, -0.0805],\n",
            "        [-1.7627, -0.1882],\n",
            "        [-0.8830, -0.5337]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2760, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3468, -1.0965],\n",
            "        [ 0.1294,  0.0700],\n",
            "        [ 1.2285, -0.7907],\n",
            "        [ 0.2236, -0.0606]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0833, -2.5266],\n",
            "        [-0.6639, -0.7232],\n",
            "        [-0.1247, -2.1439],\n",
            "        [-0.5611, -0.8453]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.3583, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.4290,  0.6966],\n",
            "        [-0.0663,  0.2088],\n",
            "        [-0.4148,  0.5551],\n",
            "        [ 2.4436, -2.2992]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.4066, -0.2810],\n",
            "        [-0.8401, -0.5650],\n",
            "        [-1.2914, -0.3214],\n",
            "        [-0.0087, -4.7515]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2940, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.4459,  1.6743],\n",
            "        [ 0.2485, -0.0057],\n",
            "        [ 1.0138, -0.6811],\n",
            "        [ 1.1292, -0.9413]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1634, -0.0432],\n",
            "        [-0.5741, -0.8283],\n",
            "        [-0.1686, -1.8635],\n",
            "        [-0.1188, -2.1892]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2262, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.3806,  0.5288],\n",
            "        [ 0.2929,  0.0784],\n",
            "        [ 2.6040, -2.4012],\n",
            "        [ 0.0507,  0.0520]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.2479, -0.3384],\n",
            "        [-0.5916, -0.8062],\n",
            "        [-0.0067, -5.0120],\n",
            "        [-0.6938, -0.6925]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.4610, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.17886753380298615\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([0, 3, 0, 1]), 1: array([3, 0, 1, 0])}\n",
            "valid loss : tensor(0.1905, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 6, 0, 1]), 1: array([6, 1, 1, 0])}\n",
            "valid loss : tensor(0.0549, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 9, 0, 1]), 1: array([9, 2, 1, 0])}\n",
            "valid loss : tensor(0.0225, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 4, 11,  0,  1]), 1: array([11,  4,  1,  0])}\n",
            "valid loss : tensor(0.1361, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 12,  0,  2]), 1: array([12,  6,  2,  0])}\n",
            "valid loss : tensor(0.4567, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 13,  0,  2]), 1: array([13,  9,  2,  0])}\n",
            "valid loss : tensor(0.1585, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([12, 14,  0,  2]), 1: array([14, 12,  2,  0])}\n",
            "valid loss : tensor(0.3002, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 17,  0,  2]), 1: array([17, 13,  2,  0])}\n",
            "valid loss : tensor(0.1757, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([16, 18,  0,  2]), 1: array([18, 16,  2,  0])}\n",
            "valid loss : tensor(0.2496, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 21,  0,  2]), 1: array([21, 17,  2,  0])}\n",
            "valid loss : tensor(0.1883, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 23,  0,  2]), 1: array([23, 19,  2,  0])}\n",
            "valid loss : tensor(0.2265, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 23,  0,  3]), 1: array([23, 22,  3,  0])}\n",
            "valid loss : tensor(0.6843, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 24,  0,  5]), 1: array([24, 23,  5,  0])}\n",
            "valid loss : tensor(1.3496, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 27,  0,  5]), 1: array([27, 24,  5,  0])}\n",
            "valid loss : tensor(0.0423, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 30,  0,  6]), 1: array([30, 24,  6,  0])}\n",
            "valid loss : tensor(0.2090, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9, recall : 0.8, precision : 1.0\n",
            "class1 >>> accuracy : 0.9, recall : 1.0, precision : 0.8333333333333334\n",
            "Loss/valid : 0.29631576339403787\n",
            "\n",
            "========= Epoch23 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.8028,  3.0287],\n",
            "        [ 1.2403, -1.0480],\n",
            "        [-2.3851,  2.6327],\n",
            "        [ 0.9859, -0.8040]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.8345e+00, -2.9292e-03],\n",
            "        [-9.6613e-02, -2.3850e+00],\n",
            "        [-5.0244e+00, -6.5972e-03],\n",
            "        [-1.5442e-01, -1.9443e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0651, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.4865,  2.7940],\n",
            "        [-1.7032,  1.8452],\n",
            "        [ 1.2329, -1.0604],\n",
            "        [ 0.4625, -0.1306]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.2856e+00, -5.0766e-03],\n",
            "        [-3.5768e+00, -2.8364e-02],\n",
            "        [-9.6159e-02, -2.3895e+00],\n",
            "        [-4.3994e-01, -1.0330e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1424, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.7841,  1.9399],\n",
            "        [ 1.9819, -1.7387],\n",
            "        [-1.6523,  1.9084],\n",
            "        [ 0.9644, -0.7751]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7478, -0.0239],\n",
            "        [-0.0239, -3.7446],\n",
            "        [-3.5887, -0.0280],\n",
            "        [-0.1618, -1.9013]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0594, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4261,  1.7507],\n",
            "        [ 1.3582, -1.1435],\n",
            "        [ 1.2572, -0.9928],\n",
            "        [-1.5621,  1.7155]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2177, -0.0409],\n",
            "        [-0.0788, -2.5805],\n",
            "        [-0.1002, -2.3503],\n",
            "        [-3.3146, -0.0370]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0642, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7201, -0.3746],\n",
            "        [ 1.2502, -1.0338],\n",
            "        [-1.2039,  1.4843],\n",
            "        [ 1.7643, -1.6436]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2887, -1.3834],\n",
            "        [-0.0970, -2.3810],\n",
            "        [-2.7540, -0.0658],\n",
            "        [-0.0326, -3.4405]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1210, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9993, -1.8411],\n",
            "        [ 1.8042, -1.5946],\n",
            "        [ 1.7347, -1.5791],\n",
            "        [ 1.3845, -1.1450]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0213, -3.8617],\n",
            "        [-0.0329, -3.4317],\n",
            "        [-0.0357, -3.3495],\n",
            "        [-0.0767, -2.6062]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0416, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.9152, -2.4834],\n",
            "        [-1.4438,  1.5688],\n",
            "        [-0.1335,  0.5035],\n",
            "        [-0.7687,  0.9458]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.5125e-03, -5.4032e+00],\n",
            "        [-3.0606e+00, -4.7992e-02],\n",
            "        [-1.0615e+00, -4.2455e-01],\n",
            "        [-1.8800e+00, -1.6557e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1607, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.4081, -2.2781],\n",
            "        [ 1.3399, -1.2967],\n",
            "        [-0.6979,  0.8342],\n",
            "        [-1.0279,  1.3085]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0092, -4.6954],\n",
            "        [-0.0692, -2.7057],\n",
            "        [-1.7277, -0.1956],\n",
            "        [-2.4287, -0.0923]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0916, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6416,  1.9146],\n",
            "        [ 1.6302, -1.4818],\n",
            "        [ 1.5867, -1.2789],\n",
            "        [-1.3360,  1.6500]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5844, -0.0281],\n",
            "        [-0.0435, -3.1556],\n",
            "        [-0.0554, -2.9209],\n",
            "        [-3.0353, -0.0493]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0441, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.8579,  2.1457],\n",
            "        [-1.0400,  1.2057],\n",
            "        [-0.6419,  0.7783],\n",
            "        [-1.7661,  1.9934]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.0216, -0.0181],\n",
            "        [-2.3463, -0.1006],\n",
            "        [-1.6367, -0.2164],\n",
            "        [-3.7825, -0.0230]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0895, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.6214, -0.4953],\n",
            "        [-2.0854,  2.3824],\n",
            "        [-1.3453,  1.4492],\n",
            "        [-1.5733,  1.7551]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2832, -1.3999],\n",
            "        [-4.4792, -0.0114],\n",
            "        [-2.8538, -0.0593],\n",
            "        [-3.3636, -0.0352]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0973, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.0334,  0.4266],\n",
            "        [-1.1229,  1.2720],\n",
            "        [ 0.6697, -0.4465],\n",
            "        [-1.9976,  2.1499]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.9494, -0.4893],\n",
            "        [-2.4822, -0.0873],\n",
            "        [-0.2833, -1.3995],\n",
            "        [-4.1631, -0.0157]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.3339, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.2780,  0.1032],\n",
            "        [ 3.1810, -3.1065],\n",
            "        [-0.2338,  0.4406],\n",
            "        [-0.4054,  0.5536]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-6.0958e-01, -7.8434e-01],\n",
            "        [-1.8578e-03, -6.2893e+00],\n",
            "        [-1.0861e+00, -4.1176e-01],\n",
            "        [-1.2835e+00, -3.2445e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.3806, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.3340,  0.5817],\n",
            "        [ 1.7476, -1.4787],\n",
            "        [ 0.1546,  0.1008],\n",
            "        [-1.6792,  1.8548]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.2524, -0.3366],\n",
            "        [-0.0389, -3.2653],\n",
            "        [-0.6666, -0.7204],\n",
            "        [-3.5627, -0.0288]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.4967, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6037, -0.3368],\n",
            "        [-0.9477,  1.2266],\n",
            "        [ 1.6761, -1.4920],\n",
            "        [ 2.5452, -2.3477]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3296, -1.2701],\n",
            "        [-2.2820, -0.1077],\n",
            "        [-0.0412, -3.2094],\n",
            "        [-0.0075, -4.9004]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1215, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.15397418662905693\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([1, 2, 1, 0]), 1: array([2, 1, 0, 1])}\n",
            "valid loss : tensor(0.5828, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 6, 1, 0]), 1: array([6, 1, 0, 1])}\n",
            "valid loss : tensor(0.3247, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 8, 1, 0]), 1: array([8, 3, 0, 1])}\n",
            "valid loss : tensor(0.1671, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([6, 8, 2, 0]), 1: array([8, 6, 0, 2])}\n",
            "valid loss : tensor(0.2517, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 8, 10,  2,  0]), 1: array([10,  8,  0,  2])}\n",
            "valid loss : tensor(0.2163, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 13,  2,  0]), 1: array([13,  9,  0,  2])}\n",
            "valid loss : tensor(0.2500, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([12, 14,  2,  0]), 1: array([14, 12,  0,  2])}\n",
            "valid loss : tensor(0.1538, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 15,  2,  0]), 1: array([15, 15,  0,  2])}\n",
            "valid loss : tensor(0.0421, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 17,  2,  0]), 1: array([17, 17,  0,  2])}\n",
            "valid loss : tensor(0.0596, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 18,  3,  0]), 1: array([18, 19,  0,  3])}\n",
            "valid loss : tensor(0.2740, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 21,  3,  0]), 1: array([21, 20,  0,  3])}\n",
            "valid loss : tensor(0.3675, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 22,  3,  0]), 1: array([22, 23,  0,  3])}\n",
            "valid loss : tensor(0.0646, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([24, 24,  4,  0]), 1: array([24, 24,  0,  4])}\n",
            "valid loss : tensor(0.4227, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([28, 24,  4,  0]), 1: array([24, 28,  0,  4])}\n",
            "valid loss : tensor(0.0189, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 26,  4,  0]), 1: array([26, 30,  0,  4])}\n",
            "valid loss : tensor(0.1646, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9333333333333333, recall : 1.0, precision : 0.8823529411764706\n",
            "class1 >>> accuracy : 0.9333333333333333, recall : 0.8666666666666667, precision : 1.0\n",
            "Loss/valid : 0.22402798694868883\n",
            "\n",
            "========= Epoch24 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.3394,  0.4805],\n",
            "        [ 2.1766, -2.0240],\n",
            "        [-1.4424,  1.5652],\n",
            "        [ 1.1031, -0.8344]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.1849, -0.3650],\n",
            "        [-0.0149, -4.2155],\n",
            "        [-3.0558, -0.0482],\n",
            "        [-0.1346, -2.0721]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1407, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5731,  0.8534],\n",
            "        [ 2.2947, -1.9664],\n",
            "        [ 1.9041, -1.6897],\n",
            "        [ 1.8514, -1.6648]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6418, -0.2152],\n",
            "        [-0.0140, -4.2751],\n",
            "        [-0.0271, -3.6209],\n",
            "        [-0.0293, -3.5454]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0714, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.2044,  0.3393],\n",
            "        [-0.9853,  1.1629],\n",
            "        [-0.7010,  1.0723],\n",
            "        [-0.9823,  1.2402]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.0015, -0.4578],\n",
            "        [-2.2585, -0.1104],\n",
            "        [-1.9301, -0.1568],\n",
            "        [-2.3254, -0.1029]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.2070, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.1380, -2.0637],\n",
            "        [ 1.5404, -1.3362],\n",
            "        [-1.9150,  2.0708],\n",
            "        [-1.8719,  2.0297]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0149, -4.2166],\n",
            "        [-0.0548, -2.9314],\n",
            "        [-4.0042, -0.0184],\n",
            "        [-3.9216, -0.0200]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0270, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9519, -0.6811],\n",
            "        [ 1.3822, -1.2591],\n",
            "        [-1.6132,  1.8601],\n",
            "        [ 1.0462, -0.8273]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1784, -1.8115],\n",
            "        [-0.0688, -2.7101],\n",
            "        [-3.5039, -0.0305],\n",
            "        [-0.1429, -2.0164]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.1052, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3100, -1.1205],\n",
            "        [ 0.9896, -0.7773],\n",
            "        [ 1.9082, -1.4702],\n",
            "        [ 2.1224, -1.8741]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0843, -2.5148],\n",
            "        [-0.1577, -1.9246],\n",
            "        [-0.0335, -3.4120],\n",
            "        [-0.0182, -4.0147]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0735, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7737,  1.1029],\n",
            "        [-0.8621,  1.0033],\n",
            "        [-0.7097,  1.0238],\n",
            "        [-0.8295,  1.1187]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0191, -0.1425],\n",
            "        [-2.0094, -0.1440],\n",
            "        [-1.8962, -0.1627],\n",
            "        [-2.0815, -0.1332]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1456, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.2464,  2.4762],\n",
            "        [ 1.8339, -1.6491],\n",
            "        [-1.1911,  1.3596],\n",
            "        [-1.1842,  1.3598]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.7315, -0.0089],\n",
            "        [-0.0303, -3.5132],\n",
            "        [-2.6259, -0.0751],\n",
            "        [-2.6196, -0.0756]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0475, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.0361,  0.3222],\n",
            "        [ 0.7321, -0.5086],\n",
            "        [ 1.7133, -1.5138],\n",
            "        [ 2.0021, -1.7300]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.8464, -0.5603],\n",
            "        [-0.2540, -1.4947],\n",
            "        [-0.0389, -3.2660],\n",
            "        [-0.0237, -3.7557]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.2907, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7873,  1.0594],\n",
            "        [ 3.0206, -2.8912],\n",
            "        [-0.4152,  0.6986],\n",
            "        [-0.7756,  1.0004]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9931e+00, -1.4649e-01],\n",
            "        [-2.7036e-03, -5.9145e+00],\n",
            "        [-1.3977e+00, -2.8392e-01],\n",
            "        [-1.9324e+00, -1.5642e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1474, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.6942, -0.4476],\n",
            "        [-0.3329,  0.4694],\n",
            "        [-0.5628,  0.6646],\n",
            "        [ 2.2916, -2.1165]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2770, -1.4189],\n",
            "        [-1.1727, -0.3704],\n",
            "        [-1.4844, -0.2570],\n",
            "        [-0.0121, -4.4202]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.2291, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.7713, -1.6196],\n",
            "        [-1.6029,  1.9918],\n",
            "        [-1.8894,  2.1856],\n",
            "        [ 1.1661, -0.9095]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0331, -3.4241],\n",
            "        [-3.6219, -0.0271],\n",
            "        [-4.0918, -0.0169],\n",
            "        [-0.1182, -2.1938]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0488, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1074,  1.2885],\n",
            "        [-1.7913,  2.0025],\n",
            "        [-1.0145,  1.1640],\n",
            "        [ 1.3592, -1.3139]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4831, -0.0872],\n",
            "        [-3.8161, -0.0223],\n",
            "        [-2.2857, -0.1073],\n",
            "        [-0.0668, -2.7399]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0709, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.4712,  1.6246],\n",
            "        [ 0.9100, -0.6582],\n",
            "        [ 1.5155, -1.3494],\n",
            "        [ 0.3175,  0.0805]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1400, -0.0442],\n",
            "        [-0.1893, -1.7575],\n",
            "        [-0.0554, -2.9204],\n",
            "        [-0.5816, -0.8187]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.2177, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.3680, -0.0541],\n",
            "        [ 2.7727, -2.4566],\n",
            "        [ 1.7836, -1.6604],\n",
            "        [-0.5144,  0.8021]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.5042, -0.9263],\n",
            "        [-0.0053, -5.2346],\n",
            "        [-0.0314, -3.4754],\n",
            "        [-1.5540, -0.2375]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.3001, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.14149985052645206\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1711, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.0959, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 7, 0, 1]), 1: array([7, 4, 1, 0])}\n",
            "valid loss : tensor(0.4749, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([6, 9, 0, 1]), 1: array([9, 6, 1, 0])}\n",
            "valid loss : tensor(0.2217, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 7, 12,  0,  1]), 1: array([12,  7,  1,  0])}\n",
            "valid loss : tensor(0.0716, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 13,  0,  2]), 1: array([13,  9,  2,  0])}\n",
            "valid loss : tensor(0.7761, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 15,  0,  2]), 1: array([15, 11,  2,  0])}\n",
            "valid loss : tensor(0.0764, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 18,  0,  3]), 1: array([18, 11,  3,  0])}\n",
            "valid loss : tensor(0.3753, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 20,  0,  3]), 1: array([20, 13,  3,  0])}\n",
            "valid loss : tensor(0.0535, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 23,  0,  3]), 1: array([23, 14,  3,  0])}\n",
            "valid loss : tensor(0.0282, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 23,  0,  3]), 1: array([23, 18,  3,  0])}\n",
            "valid loss : tensor(0.3297, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 1, 0]), actual : tensor([1., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 25,  0,  3]), 1: array([25, 20,  3,  0])}\n",
            "valid loss : tensor(0.1709, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 27,  0,  3]), 1: array([27, 22,  3,  0])}\n",
            "valid loss : tensor(0.1187, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 28,  0,  5]), 1: array([28, 23,  5,  0])}\n",
            "valid loss : tensor(0.5305, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 30,  0,  5]), 1: array([30, 25,  5,  0])}\n",
            "valid loss : tensor(0.2026, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9166666666666666, recall : 0.8333333333333334, precision : 1.0\n",
            "class1 >>> accuracy : 0.9166666666666666, recall : 1.0, precision : 0.8571428571428571\n",
            "Loss/valid : 0.24647439308464528\n",
            "\n",
            "========= Epoch25 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0161, -0.8058],\n",
            "        [ 0.7435, -0.5188],\n",
            "        [-2.2401,  2.5331],\n",
            "        [ 1.2010, -1.0131]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1499, -1.9718],\n",
            "        [-0.2492, -1.5115],\n",
            "        [-4.7817, -0.0084],\n",
            "        [-0.1037, -2.3178]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1278, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7814, -1.5094],\n",
            "        [ 2.0552, -1.9202],\n",
            "        [ 0.6915, -0.5678],\n",
            "        [-2.4228,  2.7371]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0366, -3.3273],\n",
            "        [-0.0186, -3.9940],\n",
            "        [-0.2499, -1.5092],\n",
            "        [-5.1656, -0.0057]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0777, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2572, -0.9814],\n",
            "        [ 1.9427, -1.4963],\n",
            "        [-1.1030,  1.2434],\n",
            "        [-1.7208,  1.9316]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1013, -2.3399],\n",
            "        [-0.0316, -3.4706],\n",
            "        [-2.4379, -0.0914],\n",
            "        [-3.6780, -0.0256]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0625, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9203,  1.0965],\n",
            "        [ 1.1006, -0.9396],\n",
            "        [-1.6751,  1.9711],\n",
            "        [-1.8661,  2.0972]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1418, -0.1249],\n",
            "        [-0.1222, -2.1624],\n",
            "        [-3.6720, -0.0258],\n",
            "        [-3.9821, -0.0188]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0729, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.0986, -1.8435],\n",
            "        [-1.2421,  1.3881],\n",
            "        [ 1.1045, -0.8775],\n",
            "        [-1.4880,  1.8794]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0192, -3.9613],\n",
            "        [-2.6998, -0.0696],\n",
            "        [-0.1291, -2.1111],\n",
            "        [-3.4013, -0.0339]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0629, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2760,  1.5281],\n",
            "        [-1.0332,  1.2150],\n",
            "        [-0.9101,  1.0499],\n",
            "        [-2.0407,  2.1928]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8629, -0.0588],\n",
            "        [-2.3486, -0.1004],\n",
            "        [-2.0918, -0.1318],\n",
            "        [-4.2479, -0.0144]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0763, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.3094, -2.2326],\n",
            "        [-0.6915,  0.9793],\n",
            "        [-1.9186,  2.2209],\n",
            "        [ 0.0276,  0.3737]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0106, -4.5527],\n",
            "        [-1.8431, -0.1724],\n",
            "        [-4.1553, -0.0158],\n",
            "        [-0.8811, -0.5350]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.6877, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.9356, -0.6094],\n",
            "        [ 2.6070, -2.3649],\n",
            "        [-0.7406,  0.8650],\n",
            "        [ 0.4908, -0.3463]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1934, -1.7384],\n",
            "        [-0.0069, -4.9789],\n",
            "        [-1.7886, -0.1830],\n",
            "        [-0.3597, -1.1969]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.7813, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.9161,  1.0624],\n",
            "        [ 1.2864, -1.0370],\n",
            "        [-1.1525,  1.3293],\n",
            "        [-1.7188,  1.9874]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1081, -0.1295],\n",
            "        [-0.0934, -2.4169],\n",
            "        [-2.5620, -0.0803],\n",
            "        [-3.7305, -0.0243]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0819, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.8470, -1.5452],\n",
            "        [-0.9589,  1.1256],\n",
            "        [-1.3388,  1.5986],\n",
            "        [-1.4137,  1.5739]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0331, -3.4253],\n",
            "        [-2.2017, -0.1172],\n",
            "        [-2.9890, -0.0516],\n",
            "        [-3.0368, -0.0492]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0628, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.5052, -1.3178],\n",
            "        [ 1.8343, -1.6150],\n",
            "        [ 1.0720, -0.7367],\n",
            "        [ 1.5532, -1.3905]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0577, -2.8808],\n",
            "        [-0.0313, -3.4806],\n",
            "        [-0.1517, -1.9605],\n",
            "        [-0.0513, -2.9951]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0730, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.4505, -2.3031],\n",
            "        [-0.2822,  0.6596],\n",
            "        [-0.5485,  0.6513],\n",
            "        [-1.0220,  1.3335]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0086, -4.7622],\n",
            "        [-1.2710, -0.3293],\n",
            "        [-1.4631, -0.2633],\n",
            "        [-2.4461, -0.0906]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.1729, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6730,  1.9627],\n",
            "        [ 1.7601, -1.5661],\n",
            "        [ 1.7605, -1.6402],\n",
            "        [-0.0384,  0.3953]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.6618, -0.0260],\n",
            "        [-0.0353, -3.3615],\n",
            "        [-0.0328, -3.4335],\n",
            "        [-0.9333, -0.4997]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.2569, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8887,  1.1153],\n",
            "        [ 1.4348, -1.2442],\n",
            "        [ 2.5767, -2.4080],\n",
            "        [-1.0850,  1.2386]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1305, -0.1264],\n",
            "        [-0.0664, -2.7453],\n",
            "        [-0.0068, -4.9915],\n",
            "        [-2.4170, -0.0934]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0733, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0769, -0.8346],\n",
            "        [-0.9760,  1.2609],\n",
            "        [ 1.8641, -1.6453],\n",
            "        [ 1.6388, -1.5937]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1379, -2.0494],\n",
            "        [-2.3384, -0.1015],\n",
            "        [-0.0295, -3.5388],\n",
            "        [-0.0387, -3.2712]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0769, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.18311516294876734\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.0587, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 6, 0, 0]), 1: array([6, 2, 0, 0])}\n",
            "valid loss : tensor(0.0984, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([4, 8, 0, 0]), 1: array([8, 4, 0, 0])}\n",
            "valid loss : tensor(0.0800, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.0742, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([10, 10,  0,  0]), 1: array([10, 10,  0,  0])}\n",
            "valid loss : tensor(0.0218, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 1, 0, 1]), actual : tensor([1., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 13,  0,  0]), 1: array([13, 11,  0,  0])}\n",
            "valid loss : tensor(0.1077, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([11, 17,  0,  0]), 1: array([17, 11,  0,  0])}\n",
            "valid loss : tensor(0.2496, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 19,  0,  0]), 1: array([19, 13,  0,  0])}\n",
            "valid loss : tensor(0.0576, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 21,  0,  0]), 1: array([21, 15,  0,  0])}\n",
            "valid loss : tensor(0.0774, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 23,  0,  0]), 1: array([23, 17,  0,  0])}\n",
            "valid loss : tensor(0.1035, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([18, 26,  0,  0]), 1: array([26, 18,  0,  0])}\n",
            "valid loss : tensor(0.1909, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 27,  0,  0]), 1: array([27, 21,  0,  0])}\n",
            "valid loss : tensor(0.0490, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([24, 28,  0,  0]), 1: array([28, 24,  0,  0])}\n",
            "valid loss : tensor(0.0694, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 30,  0,  0]), 1: array([30, 26,  0,  0])}\n",
            "valid loss : tensor(0.0781, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1053, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.09475620028873284\n",
            "\n",
            "========= Epoch26 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.3574, -1.1640],\n",
            "        [-1.1582,  1.3045],\n",
            "        [-1.3003,  1.4560],\n",
            "        [ 2.3020, -2.1689]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0773, -2.5987],\n",
            "        [-2.5445, -0.0818],\n",
            "        [-2.8179, -0.0616],\n",
            "        [-0.0114, -4.4823]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0580, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.7531, -0.4724],\n",
            "        [ 2.4712, -2.3231],\n",
            "        [-0.9066,  1.2188],\n",
            "        [-1.0499,  1.3126]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2574, -1.4829],\n",
            "        [-0.0082, -4.8025],\n",
            "        [-2.2382, -0.1128],\n",
            "        [-2.4526, -0.0900]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.1171, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.8294,  2.1645],\n",
            "        [-0.9454,  1.2610],\n",
            "        [ 2.3267, -2.1193],\n",
            "        [ 1.7206, -1.3812]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.0122, -0.0183],\n",
            "        [-2.3108, -0.1045],\n",
            "        [-0.0117, -4.4576],\n",
            "        [-0.0440, -3.1458]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0446, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.5767,  0.7162],\n",
            "        [ 1.6528, -1.6054],\n",
            "        [ 0.9745, -0.7289],\n",
            "        [-1.3412,  1.5699]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5354, -0.2425],\n",
            "        [-0.0377, -3.2959],\n",
            "        [-0.1673, -1.8706],\n",
            "        [-2.9641, -0.0530]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1251, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.0563, -1.8702],\n",
            "        [ 2.0891, -1.6427],\n",
            "        [ 2.7719, -2.5219],\n",
            "        [ 2.2334, -2.1570]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.9520e-02, -3.9461e+00],\n",
            "        [-2.3666e-02, -3.7555e+00],\n",
            "        [-5.0101e-03, -5.2988e+00],\n",
            "        [-1.2319e-02, -4.4028e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0151, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.4963,  0.8822],\n",
            "        [-1.1461,  1.3300],\n",
            "        [ 1.4577, -1.2947],\n",
            "        [ 2.1932, -1.9710]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6032, -0.2247],\n",
            "        [-2.5569, -0.0807],\n",
            "        [-0.0618, -2.8143],\n",
            "        [-0.0154, -4.1797]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0957, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.2185,  1.4750],\n",
            "        [-1.1329,  1.3051],\n",
            "        [-1.0529,  1.1578],\n",
            "        [-1.3584,  1.6525]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7589, -0.0655],\n",
            "        [-2.5218, -0.0837],\n",
            "        [-2.3148, -0.1040],\n",
            "        [-3.0589, -0.0481]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0753, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6727,  1.8636],\n",
            "        [ 0.6486, -0.3839],\n",
            "        [ 1.2761, -1.0476],\n",
            "        [-1.4500,  1.5980]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5649, -0.0287],\n",
            "        [-0.3046, -1.3371],\n",
            "        [-0.0934, -2.4172],\n",
            "        [-3.0944, -0.0464]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.1183, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.9532,  2.2560],\n",
            "        [-1.7282,  1.8953],\n",
            "        [-1.2532,  1.5588],\n",
            "        [ 0.6791, -0.3141]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.2239, -0.0147],\n",
            "        [-3.6499, -0.0263],\n",
            "        [-2.8704, -0.0583],\n",
            "        [-0.3151, -1.3084]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1036, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.6437,  0.8234],\n",
            "        [ 0.8745, -0.6213],\n",
            "        [ 2.0343, -1.7147],\n",
            "        [ 1.2612, -0.8599]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.6745, -0.2075],\n",
            "        [-0.2022, -1.6980],\n",
            "        [-0.0233, -3.7723],\n",
            "        [-0.1132, -2.2343]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1365, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.4439,  0.5876],\n",
            "        [-1.0512,  1.2893],\n",
            "        [ 1.9217, -1.7263],\n",
            "        [-1.0172,  1.3124]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.3363, -0.3049],\n",
            "        [-2.4324, -0.0919],\n",
            "        [-0.0257, -3.6737],\n",
            "        [-2.4225, -0.0929]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.1289, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.3542, -2.1709],\n",
            "        [-2.2205,  2.3775],\n",
            "        [ 0.9832, -0.8524],\n",
            "        [ 2.2711, -2.1429]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0108, -4.5358],\n",
            "        [-4.6080, -0.0100],\n",
            "        [-0.1480, -1.9837],\n",
            "        [-0.0120, -4.4261]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0452, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.4429, -2.1635],\n",
            "        [-1.4755,  1.7778],\n",
            "        [ 1.6840, -1.4549],\n",
            "        [ 1.7602, -1.5906]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0099, -4.6163],\n",
            "        [-3.2912, -0.0379],\n",
            "        [-0.0424, -3.1813],\n",
            "        [-0.0345, -3.3852]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0312, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5816,  1.8021],\n",
            "        [-1.8067,  2.0926],\n",
            "        [-1.0408,  1.1973],\n",
            "        [ 2.1695, -1.9641]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.4171, -0.0334],\n",
            "        [-3.9193, -0.0201],\n",
            "        [-2.3394, -0.1013],\n",
            "        [-0.0159, -4.1494]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0427, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.3924,  2.5312],\n",
            "        [-1.0931,  1.4986],\n",
            "        [ 0.8185, -0.5810],\n",
            "        [ 0.9819, -0.6918]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.9309, -0.0072],\n",
            "        [-2.6639, -0.0722],\n",
            "        [-0.2205, -1.6200],\n",
            "        [-0.1719, -1.8456]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1180, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.08368568209310373\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([0, 4, 0, 0]), 1: array([4, 0, 0, 0])}\n",
            "valid loss : tensor(0.0440, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.0468, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([8, 4, 0, 0]), 1: array([4, 8, 0, 0])}\n",
            "valid loss : tensor(0.0779, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([10,  6,  0,  0]), 1: array([ 6, 10,  0,  0])}\n",
            "valid loss : tensor(0.0606, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([12,  8,  0,  0]), 1: array([ 8, 12,  0,  0])}\n",
            "valid loss : tensor(0.1140, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([14, 10,  0,  0]), 1: array([10, 14,  0,  0])}\n",
            "valid loss : tensor(0.0503, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 11,  0,  0]), 1: array([11, 17,  0,  0])}\n",
            "valid loss : tensor(0.0943, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([20, 12,  0,  0]), 1: array([12, 20,  0,  0])}\n",
            "valid loss : tensor(0.0184, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 14,  0,  0]), 1: array([14, 22,  0,  0])}\n",
            "valid loss : tensor(0.0680, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([23, 17,  0,  0]), 1: array([17, 23,  0,  0])}\n",
            "valid loss : tensor(0.1470, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 19,  0,  0]), 1: array([19, 25,  0,  0])}\n",
            "valid loss : tensor(0.0928, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([25, 23,  0,  0]), 1: array([23, 25,  0,  0])}\n",
            "valid loss : tensor(0.1371, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([27, 25,  0,  0]), 1: array([25, 27,  0,  0])}\n",
            "valid loss : tensor(0.0729, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 26,  0,  0]), 1: array([26, 30,  0,  0])}\n",
            "valid loss : tensor(0.0234, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.0844, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.07546659968793393\n",
            "\n",
            "========= Epoch27 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.6514, -2.3704],\n",
            "        [ 2.3091, -2.1035],\n",
            "        [-0.8766,  1.0658],\n",
            "        [-0.6333,  1.0357]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0066, -5.0284],\n",
            "        [-0.0121, -4.4246],\n",
            "        [-2.0764, -0.1340],\n",
            "        [-1.8417, -0.1726]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0813, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4236,  1.7261],\n",
            "        [ 2.0049, -1.8109],\n",
            "        [-0.0214,  0.3164],\n",
            "        [-1.7038,  2.0040]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1916, -0.0420],\n",
            "        [-0.0218, -3.8376],\n",
            "        [-0.8763, -0.5384],\n",
            "        [-3.7320, -0.0242]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.2411, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 3.0194, -2.8906],\n",
            "        [ 1.6190, -1.3624],\n",
            "        [ 2.1307, -1.7623],\n",
            "        [ 3.1555, -2.9727]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7085e-03, -5.9127e+00],\n",
            "        [-4.9477e-02, -3.0309e+00],\n",
            "        [-2.0178e-02, -3.9132e+00],\n",
            "        [-2.1781e-03, -6.1304e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0186, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.0760, -1.8427],\n",
            "        [ 1.9607, -1.8315],\n",
            "        [-1.0715,  1.4185],\n",
            "        [-0.0385,  0.3634]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0197, -3.9384],\n",
            "        [-0.0223, -3.8145],\n",
            "        [-2.5696, -0.0797],\n",
            "        [-0.9142, -0.5122]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.1585, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0866,  1.2724],\n",
            "        [-1.9321,  2.1691],\n",
            "        [ 0.8350, -0.4801],\n",
            "        [-1.9051,  2.2104]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4493, -0.0903],\n",
            "        [-4.1177, -0.0164],\n",
            "        [-0.2378, -1.5529],\n",
            "        [-4.1318, -0.0162]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0902, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1194,  1.2676],\n",
            "        [ 0.6821, -0.4218],\n",
            "        [ 1.8720, -1.5427],\n",
            "        [ 1.2303, -1.0614]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4749, -0.0879],\n",
            "        [-0.2864, -1.3903],\n",
            "        [-0.0324, -3.4470],\n",
            "        [-0.0963, -2.3880]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.1257, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9504, -1.7183],\n",
            "        [ 1.7143, -1.4458],\n",
            "        [-0.7533,  0.9305],\n",
            "        [-0.6140,  0.8778]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0252, -3.6938],\n",
            "        [-0.0415, -3.2016],\n",
            "        [-1.8542, -0.1703],\n",
            "        [-1.6947, -0.2029]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.1100, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0725,  1.2253],\n",
            "        [-1.1686,  1.3302],\n",
            "        [-1.7309,  1.9033],\n",
            "        [-2.1500,  2.3111]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3936, -0.0957],\n",
            "        [-2.5778, -0.0790],\n",
            "        [-3.6603, -0.0261],\n",
            "        [-4.4725, -0.0115]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0531, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5583, -0.1401],\n",
            "        [-2.0833,  2.3989],\n",
            "        [-2.2892,  2.4588],\n",
            "        [-1.6235,  1.7823]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.4037, -1.1021],\n",
            "        [-4.4934, -0.0112],\n",
            "        [-4.7566, -0.0086],\n",
            "        [-3.4384, -0.0326]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.1141, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.5706,  0.6803],\n",
            "        [ 2.6030, -2.5227],\n",
            "        [-0.5234,  0.8399],\n",
            "        [ 1.4384, -1.2293]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5027, -0.2517],\n",
            "        [-0.0059, -5.1316],\n",
            "        [-1.5911, -0.2278],\n",
            "        [-0.0671, -2.7348]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.1381, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.0274, -1.5541],\n",
            "        [-1.4923,  1.6927],\n",
            "        [ 1.8414, -1.6063],\n",
            "        [ 1.3850, -1.1817]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0275, -3.6090],\n",
            "        [-3.2255, -0.0405],\n",
            "        [-0.0313, -3.4790],\n",
            "        [-0.0740, -2.6407]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0433, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.7703,  1.9994],\n",
            "        [ 1.7587, -1.5821],\n",
            "        [-2.0047,  2.2998],\n",
            "        [ 1.0410, -0.7413]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.7924, -0.0228],\n",
            "        [-0.0348, -3.3757],\n",
            "        [-4.3179, -0.0134],\n",
            "        [-0.1555, -1.9379]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0566, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.8571,  1.0101],\n",
            "        [-2.2751,  2.4184],\n",
            "        [-1.3277,  1.6130],\n",
            "        [ 2.2025, -1.9793]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.0109, -0.1437],\n",
            "        [-4.7026, -0.0091],\n",
            "        [-2.9921, -0.0515],\n",
            "        [-0.0152, -4.1970]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0549, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.8255,  1.2367],\n",
            "        [ 2.5901, -2.3216],\n",
            "        [ 1.6520, -1.4138],\n",
            "        [-1.5493,  1.8005]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1819, -0.1197],\n",
            "        [-0.0073, -4.9190],\n",
            "        [-0.0456, -3.1114],\n",
            "        [-3.3843, -0.0345]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0518, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6449,  1.9856],\n",
            "        [ 2.1530, -1.9983],\n",
            "        [ 1.3121, -1.2587],\n",
            "        [ 1.9790, -1.8123]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.6566, -0.0262],\n",
            "        [-0.0156, -4.1669],\n",
            "        [-0.0737, -2.6444],\n",
            "        [-0.0223, -3.8135]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0344, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.09144436866044998\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([3, 1, 0, 0]), 1: array([1, 3, 0, 0])}\n",
            "valid loss : tensor(0.0831, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([5, 3, 0, 0]), 1: array([3, 5, 0, 0])}\n",
            "valid loss : tensor(0.0648, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 4, 0, 1]), 1: array([4, 7, 1, 0])}\n",
            "valid loss : tensor(0.2578, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([8, 7, 0, 1]), 1: array([7, 8, 1, 0])}\n",
            "valid loss : tensor(0.0628, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([10,  9,  0,  1]), 1: array([ 9, 10,  1,  0])}\n",
            "valid loss : tensor(0.0760, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 10,  0,  1]), 1: array([10, 13,  1,  0])}\n",
            "valid loss : tensor(0.0916, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 12,  0,  1]), 1: array([12, 15,  1,  0])}\n",
            "valid loss : tensor(0.0272, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 15,  0,  1]), 1: array([15, 16,  1,  0])}\n",
            "valid loss : tensor(0.0993, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([17, 18,  0,  1]), 1: array([18, 17,  1,  0])}\n",
            "valid loss : tensor(0.0517, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 21,  0,  1]), 1: array([21, 18,  1,  0])}\n",
            "valid loss : tensor(0.0229, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([20, 23,  0,  1]), 1: array([23, 20,  1,  0])}\n",
            "valid loss : tensor(0.1045, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([22, 25,  0,  1]), 1: array([25, 22,  1,  0])}\n",
            "valid loss : tensor(0.0238, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 26,  0,  1]), 1: array([26, 25,  1,  0])}\n",
            "valid loss : tensor(0.0913, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 1, 1]), actual : tensor([1., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([26, 29,  0,  1]), 1: array([29, 26,  1,  0])}\n",
            "valid loss : tensor(0.0531, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([29, 30,  0,  1]), 1: array([30, 29,  1,  0])}\n",
            "valid loss : tensor(0.0418, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "Loss/valid : 0.07677416950464248\n",
            "\n",
            "========= Epoch28 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.7972, -1.5599],\n",
            "        [-2.5684,  2.7136],\n",
            "        [ 0.7533, -0.3275],\n",
            "        [-1.1313,  1.3273]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.4244e-02, -3.3913e+00],\n",
            "        [-5.2871e+00, -5.0691e-03],\n",
            "        [-2.9217e-01, -1.3730e+00],\n",
            "        [-2.5407e+00, -8.2094e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1034, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-0.9322,  1.0890],\n",
            "        [ 2.0168, -1.7773],\n",
            "        [ 2.8561, -2.5638],\n",
            "        [ 2.6024, -2.4482]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.1456e+00, -1.2443e-01],\n",
            "        [-2.2254e-02, -3.8163e+00],\n",
            "        [-4.4180e-03, -5.4243e+00],\n",
            "        [-6.3850e-03, -5.0570e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0394, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 0.5976, -0.2900],\n",
            "        [-0.5284,  0.9429],\n",
            "        [-0.8550,  1.0122],\n",
            "        [-1.5002,  1.6776]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.3448, -1.2323],\n",
            "        [-1.6780, -0.2067],\n",
            "        [-2.0109, -0.1437],\n",
            "        [-3.2186, -0.0408]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.1840, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2904, -1.0740],\n",
            "        [-1.7120,  1.8870],\n",
            "        [-1.5367,  1.7836],\n",
            "        [-2.0873,  2.4547]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0898, -2.4543],\n",
            "        [-3.6260, -0.0270],\n",
            "        [-3.3558, -0.0355],\n",
            "        [-4.5526, -0.0106]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0407, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.6060, -2.5228],\n",
            "        [ 2.9897, -2.7166],\n",
            "        [-1.2136,  1.4197],\n",
            "        [-1.7889,  2.0899]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.9064e-03, -5.1347e+00],\n",
            "        [-3.3192e-03, -5.7097e+00],\n",
            "        [-2.7026e+00, -6.9380e-02],\n",
            "        [-3.8993e+00, -2.0465e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0248, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6530,  1.8871],\n",
            "        [-1.1558,  1.3236],\n",
            "        [ 1.4646, -1.0914],\n",
            "        [ 1.4163, -1.2378]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5687, -0.0286],\n",
            "        [-2.5599, -0.0805],\n",
            "        [-0.0747, -2.6308],\n",
            "        [-0.0680, -2.7221]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0630, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0843,  1.4050],\n",
            "        [ 2.5320, -2.0437],\n",
            "        [ 1.3972, -1.2542],\n",
            "        [-1.6723,  2.0205]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5690, -0.0797],\n",
            "        [-0.0102, -4.5859],\n",
            "        [-0.0682, -2.7196],\n",
            "        [-3.7173, -0.0246]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0457, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.2071, -1.9985],\n",
            "        [ 1.6991, -1.4883],\n",
            "        [ 1.8991, -1.7167],\n",
            "        [-0.9852,  1.1457]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0148, -4.2204],\n",
            "        [-0.0404, -3.2279],\n",
            "        [-0.0265, -3.6423],\n",
            "        [-2.2431, -0.1122]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0485, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1544,  1.4869],\n",
            "        [ 0.8086, -0.5306],\n",
            "        [ 1.3880, -1.1023],\n",
            "        [ 2.5286, -2.3277]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.7102, -0.0688],\n",
            "        [-0.2327, -1.5719],\n",
            "        [-0.0796, -2.5699],\n",
            "        [-0.0077, -4.8640]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0972, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.9585, -1.7113],\n",
            "        [-1.1749,  1.4957],\n",
            "        [-1.2149,  1.4740],\n",
            "        [ 2.7653, -2.6228]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.5163e-02, -3.6949e+00],\n",
            "        [-2.7375e+00, -6.6924e-02],\n",
            "        [-2.7547e+00, -6.5745e-02],\n",
            "        [-4.5604e-03, -5.3926e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0406, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4999,  1.8280],\n",
            "        [-2.0164,  2.1900],\n",
            "        [ 2.2703, -2.0377],\n",
            "        [-1.3745,  1.6721]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3631, -0.0352],\n",
            "        [-4.2212, -0.0148],\n",
            "        [-0.0134, -4.3213],\n",
            "        [-3.0931, -0.0464]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0275, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.0922,  1.2085],\n",
            "        [ 1.1206, -0.8399],\n",
            "        [-1.4353,  1.6478],\n",
            "        [-1.3529,  1.7050]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.3961, -0.0955],\n",
            "        [-0.1317, -2.0923],\n",
            "        [-3.1279, -0.0448],\n",
            "        [-3.1038, -0.0459]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0795, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0846, -0.7671],\n",
            "        [ 2.0923, -1.8629],\n",
            "        [-1.9555,  2.2906],\n",
            "        [ 1.4475, -1.3908]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1458, -1.9975],\n",
            "        [-0.0190, -3.9742],\n",
            "        [-4.2604, -0.0142],\n",
            "        [-0.0569, -2.8952]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0590, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.3967, -2.2220],\n",
            "        [-0.9676,  1.1590],\n",
            "        [-1.2314,  1.3998],\n",
            "        [-0.9278,  1.3643]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0098, -4.6285],\n",
            "        [-2.2392, -0.1127],\n",
            "        [-2.7008, -0.0695],\n",
            "        [-2.3884, -0.0963]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0721, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.7257, -0.4633],\n",
            "        [ 0.7403, -0.3363],\n",
            "        [-1.5831,  1.8719],\n",
            "        [ 1.7483, -1.3862]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.2658, -1.4548],\n",
            "        [-0.2932, -1.3698],\n",
            "        [-3.4860, -0.0311],\n",
            "        [-0.0426, -3.1772]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1582, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.07222678897281488\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.1359, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 4, 0, 0]), 1: array([4, 4, 0, 0])}\n",
            "valid loss : tensor(0.0583, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0713, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([9, 7, 0, 0]), 1: array([7, 9, 0, 0])}\n",
            "valid loss : tensor(0.1477, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.1833, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([10, 14,  0,  0]), 1: array([14, 10,  0,  0])}\n",
            "valid loss : tensor(0.0627, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([13, 15,  0,  0]), 1: array([15, 13,  0,  0])}\n",
            "valid loss : tensor(0.0697, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([15, 17,  0,  0]), 1: array([17, 15,  0,  0])}\n",
            "valid loss : tensor(0.1191, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 18,  0,  0]), 1: array([18, 18,  0,  0])}\n",
            "valid loss : tensor(0.0657, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([21, 19,  0,  0]), 1: array([19, 21,  0,  0])}\n",
            "valid loss : tensor(0.0745, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 23,  0,  0]), 1: array([23, 21,  0,  0])}\n",
            "valid loss : tensor(0.1139, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([23, 25,  0,  0]), 1: array([25, 23,  0,  0])}\n",
            "valid loss : tensor(0.1641, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 26,  0,  0]), 1: array([26, 26,  0,  0])}\n",
            "valid loss : tensor(0.0953, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([28, 28,  0,  0]), 1: array([28, 28,  0,  0])}\n",
            "valid loss : tensor(0.0843, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([30, 30,  0,  0]), 1: array([30, 30,  0,  0])}\n",
            "valid loss : tensor(0.1175, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "class1 >>> accuracy : 1.0, recall : 1.0, precision : 1.0\n",
            "Loss/valid : 0.10421694194277127\n",
            "\n",
            "========= Epoch29 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-0.7069,  0.8777],\n",
            "        [-1.0662,  1.3192],\n",
            "        [-0.6110,  0.9634],\n",
            "        [-0.5559,  0.7454]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.7711, -0.1865],\n",
            "        [-2.4735, -0.0881],\n",
            "        [-1.7627, -0.1882],\n",
            "        [-1.5421, -0.2407]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.1759, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.3074, -2.0239],\n",
            "        [ 1.6514, -1.4645],\n",
            "        [-1.9006,  2.3431],\n",
            "        [ 1.9488, -1.7710]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0131, -4.3443],\n",
            "        [-0.0434, -3.1593],\n",
            "        [-4.2579, -0.0143],\n",
            "        [-0.0240, -3.7437]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.0237, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0778, -0.6915],\n",
            "        [-2.0721,  2.3761],\n",
            "        [-1.7434,  1.9597],\n",
            "        [ 1.9752, -1.7425]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1574, -1.9267],\n",
            "        [-4.4599, -0.0116],\n",
            "        [-3.7274, -0.0243],\n",
            "        [-0.0240, -3.7418]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.0543, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-2.3607,  2.7432],\n",
            "        [-1.0683,  1.2364],\n",
            "        [-1.0472,  1.4803],\n",
            "        [ 2.5741, -2.4081]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.1099, -0.0061],\n",
            "        [-2.3998, -0.0951],\n",
            "        [-2.6044, -0.0768],\n",
            "        [-0.0068, -4.9891]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0462, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.0961, -0.7729],\n",
            "        [-1.3397,  1.5513],\n",
            "        [-2.2264,  2.4698],\n",
            "        [ 1.3429, -1.0910]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1435, -2.0125],\n",
            "        [-2.9450, -0.0540],\n",
            "        [-4.7053, -0.0091],\n",
            "        [-0.0841, -2.5179]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0727, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 1.2309, -1.0818],\n",
            "        [ 2.0413, -1.6745],\n",
            "        [ 2.1077, -1.8694],\n",
            "        [ 2.2983, -1.7932]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0944, -2.4072],\n",
            "        [-0.0240, -3.7398],\n",
            "        [-0.0186, -3.9956],\n",
            "        [-0.0166, -4.1081]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0384, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.2323,  1.5645],\n",
            "        [-1.8723,  2.1853],\n",
            "        [ 1.3923, -1.1046],\n",
            "        [ 1.4982, -1.2731]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8560, -0.0592],\n",
            "        [-4.0747, -0.0171],\n",
            "        [-0.0791, -2.5760],\n",
            "        [-0.0607, -2.8320]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0540, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.5477,  1.8138],\n",
            "        [ 1.9832, -1.7286],\n",
            "        [ 0.8828, -0.5955],\n",
            "        [ 1.4481, -1.0402]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.3955, -0.0341],\n",
            "        [-0.0241, -3.7359],\n",
            "        [-0.2054, -1.6837],\n",
            "        [-0.0798, -2.5681]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0859, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.6419, -2.3940],\n",
            "        [-0.4454,  0.5638],\n",
            "        [ 2.0490, -1.8639],\n",
            "        [ 2.9358, -2.7200]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-6.4793e-03, -5.0424e+00],\n",
            "        [-1.3200e+00, -3.1078e-01],\n",
            "        [-1.9785e-02, -3.9327e+00],\n",
            "        [-3.4911e-03, -5.6593e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0851, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.0359,  1.3782],\n",
            "        [ 1.5044, -1.2374],\n",
            "        [ 1.7161, -1.4969],\n",
            "        [ 2.4366, -2.2871]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.4997, -0.0857],\n",
            "        [-0.0625, -2.8043],\n",
            "        [-0.0394, -3.2524],\n",
            "        [-0.0088, -4.7325]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0491, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.9253, -2.8384],\n",
            "        [-1.0069,  1.2998],\n",
            "        [-1.1043,  1.3240],\n",
            "        [ 3.0579, -2.7472]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1345e-03, -5.7669e+00],\n",
            "        [-2.4016e+00, -9.4945e-02],\n",
            "        [-2.5128e+00, -8.4517e-02],\n",
            "        [-3.0075e-03, -5.8081e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0464, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.0213, -0.5660],\n",
            "        [-1.5261,  1.8895],\n",
            "        [-2.6715,  2.8281],\n",
            "        [-2.0232,  2.3642]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.8605e-01, -1.7733e+00],\n",
            "        [-3.4479e+00, -3.2329e-02],\n",
            "        [-5.5037e+00, -4.0801e-03],\n",
            "        [-4.3997e+00, -1.2357e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0587, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.6207,  1.8100],\n",
            "        [-1.6949,  1.8828],\n",
            "        [ 1.6306, -1.5716],\n",
            "        [ 0.5721, -0.2439]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.4625, -0.0319],\n",
            "        [-3.6053, -0.0276],\n",
            "        [-0.0399, -3.2420],\n",
            "        [-0.3662, -1.1822]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.1164, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 3.3425, -3.1328],\n",
            "        [-0.2692,  0.4397],\n",
            "        [-0.6462,  0.9804],\n",
            "        [-1.2657,  1.4472]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5398e-03, -6.4769e+00],\n",
            "        [-1.1091e+00, -4.0024e-01],\n",
            "        [-1.8061e+00, -1.7949e-01],\n",
            "        [-2.7771e+00, -6.4235e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.1614, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4487,  1.6196],\n",
            "        [-1.7855,  2.1330],\n",
            "        [ 1.4107, -1.1123],\n",
            "        [-1.1934,  1.3690]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1137, -0.0455],\n",
            "        [-3.9381, -0.0197],\n",
            "        [-0.0772, -2.6001],\n",
            "        [-2.6367, -0.0743]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.0541, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.07481943778693675\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([1, 3, 0, 0]), 1: array([3, 1, 0, 0])}\n",
            "valid loss : tensor(0.0247, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.0441, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([4, 8, 0, 0]), 1: array([8, 4, 0, 0])}\n",
            "valid loss : tensor(0.0407, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([ 6, 10,  0,  0]), 1: array([10,  6,  0,  0])}\n",
            "valid loss : tensor(0.0341, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([ 9, 11,  0,  0]), 1: array([11,  9,  0,  0])}\n",
            "valid loss : tensor(0.0497, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([10, 13,  0,  1]), 1: array([13, 10,  1,  0])}\n",
            "valid loss : tensor(0.2020, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([14, 13,  0,  1]), 1: array([13, 14,  1,  0])}\n",
            "valid loss : tensor(0.0864, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([18, 13,  0,  1]), 1: array([13, 18,  1,  0])}\n",
            "valid loss : tensor(0.0524, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 16,  0,  1]), 1: array([16, 19,  1,  0])}\n",
            "valid loss : tensor(0.0501, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([19, 20,  0,  1]), 1: array([20, 19,  1,  0])}\n",
            "valid loss : tensor(0.0413, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([0, 1, 0, 1]), actual : tensor([0., 1., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 22,  0,  1]), 1: array([22, 21,  1,  0])}\n",
            "valid loss : tensor(0.0402, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 25,  0,  1]), 1: array([25, 22,  1,  0])}\n",
            "valid loss : tensor(0.0252, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 26,  0,  1]), 1: array([26, 25,  1,  0])}\n",
            "valid loss : tensor(0.1247, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([0, 0, 1, 1]), actual : tensor([0., 0., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([27, 28,  0,  1]), 1: array([28, 27,  1,  0])}\n",
            "valid loss : tensor(0.0519, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([1, 1, 0, 0]), actual : tensor([1., 1., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([29, 30,  0,  1]), 1: array([30, 29,  1,  0])}\n",
            "valid loss : tensor(0.0180, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "Loss/valid : 0.059030670672655106\n",
            "\n",
            "========= Epoch30 =========\n",
            "\n",
            "------ train batch1 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.7274, -2.6388],\n",
            "        [ 1.9322, -1.7401],\n",
            "        [ 2.7709, -2.5976],\n",
            "        [-1.3834,  1.5590]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.6611e-03, -5.3708e+00],\n",
            "        [-2.5100e-02, -3.6974e+00],\n",
            "        [-4.6503e-03, -5.3732e+00],\n",
            "        [-2.9938e+00, -5.1395e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch1 loss(kl_div) : tensor(0.0215, grad_fn=<DivBackward0>)\n",
            "------ train batch2 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 2.6588, -2.4457],\n",
            "        [ 0.4751, -0.1806],\n",
            "        [ 0.0564,  0.2757],\n",
            "        [ 1.3636, -1.3031]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0061, -5.1106],\n",
            "        [-0.4181, -1.0737],\n",
            "        [-0.8088, -0.5895],\n",
            "        [-0.0672, -2.7339]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch2 loss(kl_div) : tensor(0.3250, grad_fn=<DivBackward0>)\n",
            "------ train batch3 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 3.8501, -3.6087],\n",
            "        [ 3.1478, -2.8531],\n",
            "        [ 0.6214, -0.4075],\n",
            "        [ 3.5307, -3.2932]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-5.7621e-04, -7.4594e+00],\n",
            "        [-2.4734e-03, -6.0034e+00],\n",
            "        [-3.0557e-01, -1.3345e+00],\n",
            "        [-1.0868e-03, -6.8250e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch3 loss(kl_div) : tensor(0.3347, grad_fn=<DivBackward0>)\n",
            "------ train batch4 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[-1.1443,  1.4844],\n",
            "        [ 2.0729, -1.6849],\n",
            "        [-1.1985,  1.5586],\n",
            "        [ 2.2871, -2.0357]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.6984, -0.0697],\n",
            "        [-0.0231, -3.7809],\n",
            "        [-2.8187, -0.0615],\n",
            "        [-0.0132, -4.3359]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch4 loss(kl_div) : tensor(0.0419, grad_fn=<DivBackward0>)\n",
            "------ train batch5 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.6641, -1.3390],\n",
            "        [ 2.0697, -1.8140],\n",
            "        [ 2.6138, -2.3627],\n",
            "        [-0.8335,  1.1766]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0484, -3.0515],\n",
            "        [-0.0204, -3.9041],\n",
            "        [-0.0069, -4.9833],\n",
            "        [-2.1358, -0.1257]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch5 loss(kl_div) : tensor(0.0504, grad_fn=<DivBackward0>)\n",
            "------ train batch6 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4098,  1.7182],\n",
            "        [ 2.9890, -2.7020],\n",
            "        [-2.4351,  2.5910],\n",
            "        [-1.1739,  1.3424]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.1709e+00, -4.2873e-02],\n",
            "        [-3.3704e-03, -5.6944e+00],\n",
            "        [-5.0326e+00, -6.5429e-03],\n",
            "        [-2.5940e+00, -7.7660e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch6 loss(kl_div) : tensor(0.0326, grad_fn=<DivBackward0>)\n",
            "------ train batch7 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.1213, -0.8916],\n",
            "        [ 1.1534, -0.7409],\n",
            "        [-1.7461,  2.0801],\n",
            "        [-1.4118,  1.5812]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1254, -2.1383],\n",
            "        [-0.1401, -2.0344],\n",
            "        [-3.8478, -0.0216],\n",
            "        [-3.0419, -0.0489]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch7 loss(kl_div) : tensor(0.0840, grad_fn=<DivBackward0>)\n",
            "------ train batch8 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.6425,  1.8292],\n",
            "        [ 2.9271, -2.6157],\n",
            "        [-1.1373,  1.3577],\n",
            "        [-0.3704,  0.8105]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.5023e+00, -3.0591e-02],\n",
            "        [-3.9079e-03, -5.5467e+00],\n",
            "        [-2.5743e+00, -7.9270e-02],\n",
            "        [-1.4486e+00, -2.6775e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch8 loss(kl_div) : tensor(0.0954, grad_fn=<DivBackward0>)\n",
            "------ train batch9 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.2323, -1.0117],\n",
            "        [-1.7078,  2.0461],\n",
            "        [ 1.9509, -1.7686],\n",
            "        [-2.3205,  2.6625]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1008, -2.3447],\n",
            "        [-3.7771, -0.0232],\n",
            "        [-0.0240, -3.7434],\n",
            "        [-4.9898, -0.0068]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch9 loss(kl_div) : tensor(0.0387, grad_fn=<DivBackward0>)\n",
            "------ train batch10 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 1.9585, -1.5841],\n",
            "        [ 1.3053, -1.0123],\n",
            "        [ 2.0169, -1.7966],\n",
            "        [-2.5211,  2.9128]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-2.8528e-02, -3.5711e+00],\n",
            "        [-9.3954e-02, -2.4116e+00],\n",
            "        [-2.1830e-02, -3.8353e+00],\n",
            "        [-5.4383e+00, -4.3564e-03]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch10 loss(kl_div) : tensor(0.0372, grad_fn=<DivBackward0>)\n",
            "------ train batch11 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.6912, -2.1769],\n",
            "        [-1.2407,  1.4157],\n",
            "        [-1.6324,  1.9033],\n",
            "        [-1.3425,  1.6391]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.0077, -4.8758],\n",
            "        [-2.7243, -0.0678],\n",
            "        [-3.5645, -0.0287],\n",
            "        [-3.0310, -0.0495]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch11 loss(kl_div) : tensor(0.0384, grad_fn=<DivBackward0>)\n",
            "------ train batch12 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-1.4910,  1.6716],\n",
            "        [-1.6143,  1.9804],\n",
            "        [-2.1854,  2.4479],\n",
            "        [-2.0643,  2.2568]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-3.2040, -0.0414],\n",
            "        [-3.6218, -0.0271],\n",
            "        [-4.6429, -0.0097],\n",
            "        [-4.3343, -0.0132]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch12 loss(kl_div) : tensor(0.0229, grad_fn=<DivBackward0>)\n",
            "------ train batch13 ------\n",
            "Label : tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[-2.2054,  2.5266],\n",
            "        [ 1.0220, -0.8668],\n",
            "        [-1.4488,  1.6503],\n",
            "        [-2.1648,  2.4154]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-4.7408, -0.0088],\n",
            "        [-0.1408, -2.0297],\n",
            "        [-3.1432, -0.0441],\n",
            "        [-4.5904, -0.0102]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch13 loss(kl_div) : tensor(0.0510, grad_fn=<DivBackward0>)\n",
            "------ train batch14 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "predicted : tensor([[ 2.1585, -2.0057],\n",
            "        [-1.6239,  1.7466],\n",
            "        [-1.6279,  1.8534],\n",
            "        [-2.7940,  2.9793]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-1.5423e-02, -4.1796e+00],\n",
            "        [-3.4043e+00, -3.3794e-02],\n",
            "        [-3.5116e+00, -3.0304e-02],\n",
            "        [-5.7764e+00, -3.1046e-03]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch14 loss(kl_div) : tensor(0.0207, grad_fn=<DivBackward0>)\n",
            "------ train batch15 ------\n",
            "Label : tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "predicted : tensor([[ 0.9066, -0.6305],\n",
            "        [ 0.7150, -0.2511],\n",
            "        [-1.7900,  2.2501],\n",
            "        [ 0.9636, -0.7713]], grad_fn=<AddmmBackward0>)\n",
            "predicted(logsoftmax) : tensor([[-0.1947, -1.7319],\n",
            "        [-0.3225, -1.2887],\n",
            "        [-4.0575, -0.0174],\n",
            "        [-0.1625, -1.8974]], grad_fn=<LogSoftmaxBackward0>)\n",
            "train batch15 loss(kl_div) : tensor(0.1743, grad_fn=<DivBackward0>)\n",
            "Loss/train : 0.09122626980145772\n",
            "\n",
            "------ valid batch1 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([2, 2, 0, 0]), 1: array([2, 2, 0, 0])}\n",
            "valid loss : tensor(0.0907, grad_fn=<DivBackward0>)\n",
            "------ valid batch2 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([3, 5, 0, 0]), 1: array([5, 3, 0, 0])}\n",
            "valid loss : tensor(0.1109, grad_fn=<DivBackward0>)\n",
            "------ valid batch3 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([7, 5, 0, 0]), 1: array([5, 7, 0, 0])}\n",
            "valid loss : tensor(0.0043, grad_fn=<DivBackward0>)\n",
            "------ valid batch4 ------\n",
            "predicted : tensor([1, 1, 1, 1]), actual : tensor([1., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([7, 9, 0, 0]), 1: array([9, 7, 0, 0])}\n",
            "valid loss : tensor(0.2507, grad_fn=<DivBackward0>)\n",
            "------ valid batch5 ------\n",
            "predicted : tensor([0, 0, 0, 1]), actual : tensor([0., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([10, 10,  0,  0]), 1: array([10, 10,  0,  0])}\n",
            "valid loss : tensor(0.0690, grad_fn=<DivBackward0>)\n",
            "------ valid batch6 ------\n",
            "predicted : tensor([0, 0, 1, 0]), actual : tensor([0., 0., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([13, 11,  0,  0]), 1: array([11, 13,  0,  0])}\n",
            "valid loss : tensor(0.1194, grad_fn=<DivBackward0>)\n",
            "------ valid batch7 ------\n",
            "predicted : tensor([0, 1, 0, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([15, 12,  1,  0]), 1: array([12, 15,  0,  1])}\n",
            "valid loss : tensor(0.3424, grad_fn=<DivBackward0>)\n",
            "------ valid batch8 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([16, 15,  1,  0]), 1: array([15, 16,  0,  1])}\n",
            "valid loss : tensor(0.2308, grad_fn=<DivBackward0>)\n",
            "------ valid batch9 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([17, 18,  1,  0]), 1: array([18, 17,  0,  1])}\n",
            "valid loss : tensor(0.2350, grad_fn=<DivBackward0>)\n",
            "------ valid batch10 ------\n",
            "predicted : tensor([0, 1, 1, 0]), actual : tensor([0., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([19, 20,  1,  0]), 1: array([20, 19,  0,  1])}\n",
            "valid loss : tensor(0.1380, grad_fn=<DivBackward0>)\n",
            "------ valid batch11 ------\n",
            "predicted : tensor([1, 0, 0, 1]), actual : tensor([1., 0., 0., 1.])\n",
            "TP_TN_FP_FN : {0: array([21, 22,  1,  0]), 1: array([22, 21,  0,  1])}\n",
            "valid loss : tensor(0.0730, grad_fn=<DivBackward0>)\n",
            "------ valid batch12 ------\n",
            "predicted : tensor([0, 1, 1, 1]), actual : tensor([0., 1., 1., 1.])\n",
            "TP_TN_FP_FN : {0: array([22, 25,  1,  0]), 1: array([25, 22,  0,  1])}\n",
            "valid loss : tensor(0.1916, grad_fn=<DivBackward0>)\n",
            "------ valid batch13 ------\n",
            "predicted : tensor([1, 0, 0, 0]), actual : tensor([1., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([25, 26,  1,  0]), 1: array([26, 25,  0,  1])}\n",
            "valid loss : tensor(0.0908, grad_fn=<DivBackward0>)\n",
            "------ valid batch14 ------\n",
            "predicted : tensor([1, 1, 1, 0]), actual : tensor([1., 1., 1., 0.])\n",
            "TP_TN_FP_FN : {0: array([26, 29,  1,  0]), 1: array([29, 26,  0,  1])}\n",
            "valid loss : tensor(0.1347, grad_fn=<DivBackward0>)\n",
            "------ valid batch15 ------\n",
            "predicted : tensor([0, 0, 0, 0]), actual : tensor([0., 0., 0., 0.])\n",
            "TP_TN_FP_FN : {0: array([30, 29,  1,  0]), 1: array([29, 30,  0,  1])}\n",
            "valid loss : tensor(0.0062, grad_fn=<DivBackward0>)\n",
            "class0 >>> accuracy : 0.9833333333333333, recall : 1.0, precision : 0.967741935483871\n",
            "class1 >>> accuracy : 0.9833333333333333, recall : 0.9666666666666667, precision : 1.0\n",
            "Loss/valid : 0.13916497019430002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uyrCXFIonW0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}